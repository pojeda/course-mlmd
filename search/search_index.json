{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the course \u201cML/MD\u201d \u00b6 The aim of this ONLINE course is to give an intro to Machine Learning and classical Molecular Dynamics (MD) simulations.","title":"Home"},{"location":"#welcome__to__the__course__mlmd","text":"The aim of this ONLINE course is to give an intro to Machine Learning and classical Molecular Dynamics (MD) simulations.","title":"Welcome to the course &ldquo;ML/MD&rdquo;"},{"location":"day0/","text":"Day 0: Machine Learning and Deep Learning Fundamentals \u00b6 Welcome to the Course! \u00b6 Before diving into molecular and materials applications, this foundational day ensures everyone has a solid understanding of core machine learning concepts. Whether you\u2019re refreshing your knowledge or learning these concepts for the first time, this material will prepare you for the advanced topics in Days 1-5. Learning Objectives \u00b6 By the end of Day 0, you will: - Understand the fundamental concepts of machine learning - Distinguish between different types of learning problems - Recognize and address overfitting and underfitting - Apply proper validation and evaluation techniques - Understand the basics of neural networks - Know how to optimize and tune models 1. What is Machine Learning? \u00b6 1.1 Definition \u00b6 Machine learning is the science of programming computers to learn from data without being explicitly programmed. Instead of writing rules, we provide examples and let the algorithm discover patterns. Traditional Programming : Rules + Data \u2192 Output Machine Learning : Data + Output \u2192 Rules (Model) 1.2 Why Machine Learning for Science? \u00b6 In molecular and materials science, ML helps us: - Predict properties without expensive experiments or simulations - Discover patterns in complex datasets - Generate hypotheses for new molecules or materials - Accelerate discovery by orders of magnitude - Navigate high-dimensional spaces that are intractable by traditional methods 1.3 Types of Machine Learning \u00b6 Supervised Learning \u00b6 Learn from labeled examples (input-output pairs): - Regression : Predict continuous values (e.g., binding affinity, melting point) - Classification : Predict categories (e.g., toxic/non-toxic, active/inactive) # Example: Predicting molecular solubility (regression) X = molecular_features # Input: molecular descriptors y = solubility_values # Output: measured solubility model . fit ( X , y ) # Learn the relationship prediction = model . predict ( new_molecule ) # Predict for new molecule Unsupervised Learning \u00b6 Find patterns in unlabeled data: - Clustering : Group similar molecules together - Dimensionality reduction : Visualize high-dimensional chemical space - Anomaly detection : Find unusual molecules # Example: Clustering molecules by similarity from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 5 ) clusters = kmeans . fit_predict ( molecular_fingerprints ) # Molecules in same cluster are structurally similar Reinforcement Learning \u00b6 Learn through trial and error with rewards: - Molecular optimization : Generate molecules with desired properties - Synthesis planning : Find optimal reaction pathways - Experiment design : Choose most informative experiments # Example: RL agent learns to design molecules for episode in range ( num_episodes ): state = initial_molecule while not done : action = agent . select_action ( state ) # Modify molecule reward = evaluate_properties ( new_molecule ) agent . learn ( state , action , reward ) state = new_molecule 2. The Machine Learning Workflow \u00b6 2.1 Problem Definition \u00b6 Define the goal : What do you want to predict or discover? Choose the task type : Regression, classification, generation? Define success metrics : How will you measure performance? Example: \u201cPredict whether a molecule can cross the blood-brain barrier (binary classification) with >85% accuracy.\u201d 2.2 Data Collection and Preparation \u00b6 Data Collection \u00b6 Experimental measurements Computational simulations (DFT, MD) Public databases (PubChem, ChEMBL, Materials Project) Literature mining Data Quality Checks \u00b6 import pandas as pd import numpy as np # Load data data = pd . read_csv ( 'molecular_data.csv' ) # Check for missing values print ( data . isnull () . sum ()) # Check for duplicates print ( f \"Duplicates: { data . duplicated () . sum () } \" ) # Check distributions print ( data . describe ()) # Remove outliers (example: 3 sigma rule) z_scores = np . abs (( data [ 'property' ] - data [ 'property' ] . mean ()) / data [ 'property' ] . std ()) data_clean = data [ z_scores < 3 ] Feature Engineering \u00b6 Transform raw data into meaningful features: from rdkit import Chem from rdkit.Chem import Descriptors def calculate_features ( smiles ): mol = Chem . MolFromSmiles ( smiles ) features = { 'molecular_weight' : Descriptors . MolWt ( mol ), 'logP' : Descriptors . MolLogP ( mol ), 'num_h_donors' : Descriptors . NumHDonors ( mol ), 'num_h_acceptors' : Descriptors . NumHAcceptors ( mol ), 'tpsa' : Descriptors . TPSA ( mol ), 'num_rotatable_bonds' : Descriptors . NumRotatableBonds ( mol ), 'num_aromatic_rings' : Descriptors . NumAromaticRings ( mol ) } return features 2.3 Train-Test Split \u00b6 Critical principle : Never test on training data! from sklearn.model_selection import train_test_split # Basic split (80% train, 20% test) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Stratified split (maintains class distribution) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , stratify = y , random_state = 42 ) Important for molecules : Use scaffold-based splits to test generalization to new chemical structures: from sklearn.model_selection import GroupShuffleSplit # Split by molecular scaffold (core structure) scaffolds = [ get_scaffold ( mol ) for mol in molecules ] splitter = GroupShuffleSplit ( n_splits = 1 , test_size = 0.2 , random_state = 42 ) train_idx , test_idx = next ( splitter . split ( X , y , groups = scaffolds )) X_train , X_test = X [ train_idx ], X [ test_idx ] y_train , y_test = y [ train_idx ], y [ test_idx ] 3. Overfitting and Underfitting \u00b6 3.1 The Bias-Variance Tradeoff \u00b6 Underfitting (High Bias) : - Model is too simple - Poor performance on both training and test data - Doesn\u2019t capture underlying patterns Overfitting (High Variance) : - Model is too complex - Excellent on training data, poor on test data - Memorizes noise instead of learning patterns Sweet Spot : - Balanced complexity - Good performance on both training and test data - Generalizes to new examples import matplotlib.pyplot as plt import numpy as np # Generate example data X = np . linspace ( 0 , 10 , 50 ) y = 2 * X + 1 + np . random . normal ( 0 , 2 , 50 ) # Underfitting: degree 1 polynomial (too simple) underfit_model = np . poly1d ( np . polyfit ( X , y , 1 )) # Good fit: degree 2 polynomial good_model = np . poly1d ( np . polyfit ( X , y , 2 )) # Overfitting: degree 15 polynomial (too complex) overfit_model = np . poly1d ( np . polyfit ( X , y , 15 )) # Visualize X_plot = np . linspace ( 0 , 10 , 200 ) plt . scatter ( X , y , alpha = 0.5 , label = 'Data' ) plt . plot ( X_plot , underfit_model ( X_plot ), label = 'Underfitting' , linestyle = '--' ) plt . plot ( X_plot , good_model ( X_plot ), label = 'Good Fit' ) plt . plot ( X_plot , overfit_model ( X_plot ), label = 'Overfitting' , linestyle = ':' ) plt . legend () plt . show () 3.2 Detecting Overfitting \u00b6 Learning Curves : Plot training and validation performance vs. training set size from sklearn.model_selection import learning_curve train_sizes , train_scores , val_scores = learning_curve ( model , X , y , train_sizes = np . linspace ( 0.1 , 1.0 , 10 ), cv = 5 , scoring = 'neg_mean_squared_error' ) # Plot plt . figure ( figsize = ( 10 , 6 )) plt . plot ( train_sizes , - train_scores . mean ( axis = 1 ), label = 'Training Error' ) plt . plot ( train_sizes , - val_scores . mean ( axis = 1 ), label = 'Validation Error' ) plt . xlabel ( 'Training Set Size' ) plt . ylabel ( 'Mean Squared Error' ) plt . legend () plt . title ( 'Learning Curves' ) plt . show () # Signs of overfitting: # - Large gap between training and validation curves # - Training error much lower than validation error # - Validation error increases or plateaus 3.3 Preventing Overfitting \u00b6 1. Get More Data \u00b6 The most effective solution when possible: # Data augmentation for molecules def augment_molecule ( smiles ): mol = Chem . MolFromSmiles ( smiles ) # Generate different SMILES representations augmented = [] for _ in range ( 5 ): random_smiles = Chem . MolToSmiles ( mol , doRandom = True ) augmented . append ( random_smiles ) return augmented 2. Regularization \u00b6 Add penalty for model complexity: L1 Regularization (Lasso) : Encourages sparsity from sklearn.linear_model import Lasso model = Lasso ( alpha = 0.1 ) # alpha controls regularization strength model . fit ( X_train , y_train ) L2 Regularization (Ridge) : Penalizes large weights from sklearn.linear_model import Ridge model = Ridge ( alpha = 1.0 ) model . fit ( X_train , y_train ) Elastic Net : Combines L1 and L2 from sklearn.linear_model import ElasticNet model = ElasticNet ( alpha = 0.1 , l1_ratio = 0.5 ) model . fit ( X_train , y_train ) 3. Cross-Validation \u00b6 Use all data for both training and validation: from sklearn.model_selection import cross_val_score # 5-fold cross-validation scores = cross_val_score ( model , X , y , cv = 5 , scoring = 'r2' ) print ( f \"Cross-validation R\u00b2: { scores . mean () : .3f } \u00b1 { scores . std () : .3f } \" ) 4. Feature Selection \u00b6 Remove irrelevant or redundant features: from sklearn.feature_selection import SelectKBest , f_regression # Select top 10 features selector = SelectKBest ( score_func = f_regression , k = 10 ) X_selected = selector . fit_transform ( X_train , y_train ) # Get selected feature names selected_features = X . columns [ selector . get_support ()] print ( f \"Selected features: { selected_features . tolist () } \" ) 5. Early Stopping (for neural networks) \u00b6 Stop training when validation error starts increasing: from sklearn.neural_network import MLPRegressor model = MLPRegressor ( hidden_layer_sizes = ( 100 , 50 ), early_stopping = True , validation_fraction = 0.2 , n_iter_no_change = 10 # Stop if no improvement for 10 epochs ) 6. Dropout (for neural networks) \u00b6 Randomly deactivate neurons during training: import torch.nn as nn class MolecularNN ( nn . Module ): def __init__ ( self , input_dim , hidden_dim ): super () . __init__ () self . fc1 = nn . Linear ( input_dim , hidden_dim ) self . dropout1 = nn . Dropout ( 0.3 ) # Drop 30% of neurons self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) self . dropout2 = nn . Dropout ( 0.3 ) self . fc3 = nn . Linear ( hidden_dim , 1 ) def forward ( self , x ): x = torch . relu ( self . fc1 ( x )) x = self . dropout1 ( x ) x = torch . relu ( self . fc2 ( x )) x = self . dropout2 ( x ) return self . fc3 ( x ) 3.4 Addressing Underfitting \u00b6 Use a more complex model : Add more features, use deeper networks Remove regularization : Reduce alpha/lambda values Engineer better features : Domain knowledge can help Train longer : Increase number of epochs/iterations Check for errors : Ensure data is preprocessed correctly 4. Cross-Validation \u00b6 4.1 Why Cross-Validation? \u00b6 Makes efficient use of limited data Provides more reliable performance estimates Reduces sensitivity to train-test split Helps detect overfitting 4.2 K-Fold Cross-Validation \u00b6 Split data into K parts, train on K-1, test on 1, repeat K times: from sklearn.model_selection import KFold , cross_validate # 5-fold cross-validation kf = KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) # Get multiple metrics scoring = { 'r2' : 'r2' , 'rmse' : 'neg_mean_squared_error' , 'mae' : 'neg_mean_absolute_error' } results = cross_validate ( model , X , y , cv = kf , scoring = scoring , return_train_score = True ) print ( f \"Test R\u00b2: { results [ 'test_r2' ] . mean () : .3f } \u00b1 { results [ 'test_r2' ] . std () : .3f } \" ) print ( f \"Test RMSE: { np . sqrt ( - results [ 'test_rmse' ] . mean ()) : .3f } \" ) print ( f \"Test MAE: { - results [ 'test_mae' ] . mean () : .3f } \" ) 4.3 Stratified K-Fold \u00b6 Maintains class distribution in each fold (for classification): from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X , y )): print ( f \"Fold { fold + 1 } :\" ) print ( f \" Training class distribution: { np . bincount ( y [ train_idx ]) } \" ) print ( f \" Validation class distribution: { np . bincount ( y [ val_idx ]) } \" ) 4.4 Leave-One-Out Cross-Validation (LOOCV) \u00b6 Each sample is used once as test set: from sklearn.model_selection import LeaveOneOut loo = LeaveOneOut () scores = cross_val_score ( model , X , y , cv = loo , scoring = 'r2' ) print ( f \"LOOCV R\u00b2: { scores . mean () : .3f } \" ) # Note: Computationally expensive for large datasets! # Use only when data is very limited 4.5 Time Series Cross-Validation \u00b6 For temporal data (don\u2019t peek into the future!): from sklearn.model_selection import TimeSeriesSplit tscv = TimeSeriesSplit ( n_splits = 5 ) for fold , ( train_idx , test_idx ) in enumerate ( tscv . split ( X )): print ( f \"Fold { fold + 1 } :\" ) print ( f \" Train: { train_idx . min () } to { train_idx . max () } \" ) print ( f \" Test: { test_idx . min () } to { test_idx . max () } \" ) 5. Model Evaluation Metrics \u00b6 5.1 Regression Metrics \u00b6 Mean Absolute Error (MAE) \u00b6 Average absolute difference between predictions and true values: from sklearn.metrics import mean_absolute_error mae = mean_absolute_error ( y_true , y_pred ) print ( f \"MAE: { mae : .3f } \" ) # Interpretation: Average prediction error in original units # Lower is better # Robust to outliers Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) \u00b6 from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y_true , y_pred ) rmse = np . sqrt ( mse ) print ( f \"MSE: { mse : .3f } \" ) print ( f \"RMSE: { rmse : .3f } \" ) # Interpretation: Penalizes large errors more than MAE # RMSE in same units as target variable # Lower is better R\u00b2 (Coefficient of Determination) \u00b6 from sklearn.metrics import r2_score r2 = r2_score ( y_true , y_pred ) print ( f \"R\u00b2: { r2 : .3f } \" ) # Interpretation: # R\u00b2 = 1: Perfect predictions # R\u00b2 = 0: As good as predicting mean # R\u00b2 < 0: Worse than predicting mean # Range: (-\u221e, 1] Visualization \u00b6 import matplotlib.pyplot as plt plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_true , y_pred , alpha = 0.5 ) plt . plot ([ y_true . min (), y_true . max ()], [ y_true . min (), y_true . max ()], 'r--' , linewidth = 2 , label = 'Perfect Prediction' ) plt . xlabel ( 'True Values' ) plt . ylabel ( 'Predicted Values' ) plt . title ( f 'R\u00b2 = { r2 : .3f } , RMSE = { rmse : .3f } ' ) plt . legend () plt . axis ( 'equal' ) plt . show () 5.2 Classification Metrics \u00b6 Confusion Matrix \u00b6 from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay cm = confusion_matrix ( y_true , y_pred ) disp = ConfusionMatrixDisplay ( confusion_matrix = cm , display_labels = [ 'Inactive' , 'Active' ]) disp . plot () plt . show () # Predicted # Negative Positive # Actual Neg TN FP # Pos FN TP Accuracy, Precision, Recall, F1-Score \u00b6 from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score accuracy = accuracy_score ( y_true , y_pred ) precision = precision_score ( y_true , y_pred ) recall = recall_score ( y_true , y_pred ) f1 = f1_score ( y_true , y_pred ) print ( f \"Accuracy: { accuracy : .3f } - (TP + TN) / Total\" ) print ( f \"Precision: { precision : .3f } - TP / (TP + FP) - How many predicted positives are correct?\" ) print ( f \"Recall: { recall : .3f } - TP / (TP + FN) - How many actual positives did we find?\" ) print ( f \"F1-Score: { f1 : .3f } - Harmonic mean of precision and recall\" ) ROC Curve and AUC \u00b6 from sklearn.metrics import roc_curve , roc_auc_score # Get probability predictions y_prob = model . predict_proba ( X_test )[:, 1 ] # Calculate ROC curve fpr , tpr , thresholds = roc_curve ( y_test , y_prob ) auc = roc_auc_score ( y_test , y_prob ) # Plot plt . figure ( figsize = ( 8 , 6 )) plt . plot ( fpr , tpr , label = f 'ROC Curve (AUC = { auc : .3f } )' ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' , label = 'Random Classifier' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver Operating Characteristic (ROC) Curve' ) plt . legend () plt . show () # AUC interpretation: # 1.0: Perfect classifier # 0.5: Random guessing # < 0.5: Worse than random (flip predictions!) Precision-Recall Curve \u00b6 Better than ROC for imbalanced datasets: from sklearn.metrics import precision_recall_curve , average_precision_score precision , recall , thresholds = precision_recall_curve ( y_test , y_prob ) ap = average_precision_score ( y_test , y_prob ) plt . figure ( figsize = ( 8 , 6 )) plt . plot ( recall , precision , label = f 'PR Curve (AP = { ap : .3f } )' ) plt . xlabel ( 'Recall' ) plt . ylabel ( 'Precision' ) plt . title ( 'Precision-Recall Curve' ) plt . legend () plt . show () 5.3 Choosing the Right Metric \u00b6 For Regression : - MAE: Easy to interpret, robust to outliers - RMSE: Penalizes large errors - R\u00b2: Normalized, easy to compare across datasets For Classification : - Balanced classes: Accuracy, F1-score - Imbalanced classes: Precision, Recall, AUC, Average Precision - Cost-sensitive: Define custom metric based on FP/FN costs For Molecular Applications : - Drug discovery: Prioritize recall (find all active compounds) - Toxicity prediction: Prioritize precision (avoid false negatives) - Property prediction: RMSE or MAE depending on outlier sensitivity 6. Hyperparameter Tuning \u00b6 6.1 What are Hyperparameters? \u00b6 Parameters set before training (not learned from data): - Learning rate - Number of layers/neurons - Regularization strength - Number of trees in random forest - Kernel parameters in SVM 6.2 Grid Search \u00b6 Try all combinations in a grid: from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestRegressor # Define parameter grid param_grid = { 'n_estimators' : [ 100 , 200 , 500 ], 'max_depth' : [ 10 , 20 , 30 , None ], 'min_samples_split' : [ 2 , 5 , 10 ], 'min_samples_leaf' : [ 1 , 2 , 4 ] } # Grid search with cross-validation grid_search = GridSearchCV ( RandomForestRegressor ( random_state = 42 ), param_grid , cv = 5 , scoring = 'r2' , n_jobs =- 1 , # Use all CPUs verbose = 2 ) grid_search . fit ( X_train , y_train ) print ( f \"Best parameters: { grid_search . best_params_ } \" ) print ( f \"Best CV score: { grid_search . best_score_ : .3f } \" ) # Use best model best_model = grid_search . best_estimator_ 6.3 Random Search \u00b6 Sample random combinations (more efficient): from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint , uniform # Define parameter distributions param_distributions = { 'n_estimators' : randint ( 100 , 1000 ), 'max_depth' : randint ( 10 , 50 ), 'min_samples_split' : randint ( 2 , 20 ), 'min_samples_leaf' : randint ( 1 , 10 ), 'max_features' : uniform ( 0.1 , 0.9 ) } random_search = RandomizedSearchCV ( RandomForestRegressor ( random_state = 42 ), param_distributions , n_iter = 50 , # Number of random combinations to try cv = 5 , scoring = 'r2' , n_jobs =- 1 , random_state = 42 , verbose = 2 ) random_search . fit ( X_train , y_train ) print ( f \"Best parameters: { random_search . best_params_ } \" ) print ( f \"Best CV score: { random_search . best_score_ : .3f } \" ) 6.4 Bayesian Optimization \u00b6 Intelligent search using previous results: from skopt import BayesSearchCV from skopt.space import Real , Integer # Define search space search_spaces = { 'n_estimators' : Integer ( 100 , 1000 ), 'max_depth' : Integer ( 10 , 50 ), 'min_samples_split' : Integer ( 2 , 20 ), 'min_samples_leaf' : Integer ( 1 , 10 ), 'max_features' : Real ( 0.1 , 1.0 ) } bayes_search = BayesSearchCV ( RandomForestRegressor ( random_state = 42 ), search_spaces , n_iter = 50 , cv = 5 , scoring = 'r2' , n_jobs =- 1 , random_state = 42 ) bayes_search . fit ( X_train , y_train ) print ( f \"Best parameters: { bayes_search . best_params_ } \" ) print ( f \"Best CV score: { bayes_search . best_score_ : .3f } \" ) 7. Neural Networks Basics \u00b6 7.1 Architecture Components \u00b6 Neuron (Perceptron) \u00b6 Basic building block: output = activation(w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b) import numpy as np class Neuron : def __init__ ( self , input_dim ): self . weights = np . random . randn ( input_dim ) self . bias = np . random . randn () def forward ( self , x ): return np . dot ( self . weights , x ) + self . bias def activate ( self , z ): # ReLU activation return np . maximum ( 0 , z ) Layers \u00b6 import torch.nn as nn # Feedforward network model = nn . Sequential ( nn . Linear ( input_dim , 128 ), # Input layer nn . ReLU (), # Activation nn . Dropout ( 0.2 ), # Regularization nn . Linear ( 128 , 64 ), # Hidden layer nn . ReLU (), nn . Dropout ( 0.2 ), nn . Linear ( 64 , output_dim ) # Output layer ) 7.2 Activation Functions \u00b6 Common Activations \u00b6 import torch import torch.nn.functional as F x = torch . linspace ( - 3 , 3 , 100 ) # ReLU: max(0, x) relu = F . relu ( x ) # Sigmoid: 1 / (1 + e^-x) sigmoid = torch . sigmoid ( x ) # Tanh: (e^x - e^-x) / (e^x + e^-x) tanh = torch . tanh ( x ) # Leaky ReLU: max(0.01x, x) leaky_relu = F . leaky_relu ( x , negative_slope = 0.01 ) # Softmax (for multiple classes) logits = torch . tensor ([[ 1.0 , 2.0 , 3.0 ]]) softmax = F . softmax ( logits , dim = 1 ) print ( f \"Softmax output: { softmax } \" ) # Sums to 1 When to use : - ReLU : Default choice for hidden layers - Sigmoid : Binary classification output - Tanh : When you want outputs in [-1, 1] - Softmax : Multi-class classification output - Leaky ReLU : When dealing with dying ReLU problem 7.3 Loss Functions \u00b6 Regression \u00b6 import torch.nn as nn # Mean Squared Error mse_loss = nn . MSELoss () loss = mse_loss ( predictions , targets ) # Mean Absolute Error mae_loss = nn . L1Loss () loss = mae_loss ( predictions , targets ) # Huber Loss (robust to outliers) huber_loss = nn . SmoothL1Loss () loss = huber_loss ( predictions , targets ) Classification \u00b6 # Binary Cross-Entropy bce_loss = nn . BCEWithLogitsLoss () # Includes sigmoid loss = bce_loss ( logits , targets ) # Multi-class Cross-Entropy ce_loss = nn . CrossEntropyLoss () # Includes softmax loss = ce_loss ( logits , targets ) 7.4 Optimization \u00b6 Gradient Descent \u00b6 # Basic gradient descent learning_rate = 0.01 for epoch in range ( num_epochs ): # Forward pass predictions = model ( X ) loss = loss_function ( predictions , y ) # Backward pass loss . backward () # Compute gradients # Update weights with torch . no_grad (): for param in model . parameters (): param -= learning_rate * param . grad # Zero gradients model . zero_grad () Common Optimizers \u00b6 import torch.optim as optim # Stochastic Gradient Descent (SGD) optimizer = optim . SGD ( model . parameters (), lr = 0.01 , momentum = 0.9 ) # Adam (Adaptive Moment Estimation) - most common optimizer = optim . Adam ( model . parameters (), lr = 0.001 , betas = ( 0.9 , 0.999 )) # AdamW (Adam with weight decay) optimizer = optim . AdamW ( model . parameters (), lr = 0.001 , weight_decay = 0.01 ) # RMSprop optimizer = optim . RMSprop ( model . parameters (), lr = 0.001 ) # Training loop for epoch in range ( num_epochs ): optimizer . zero_grad () predictions = model ( X ) loss = loss_function ( predictions , y ) loss . backward () optimizer . step () 7.5 Batch Training \u00b6 from torch.utils.data import DataLoader , TensorDataset # Create dataset and dataloader dataset = TensorDataset ( X_tensor , y_tensor ) dataloader = DataLoader ( dataset , batch_size = 32 , shuffle = True ) # Training with batches for epoch in range ( num_epochs ): epoch_loss = 0 for batch_X , batch_y in dataloader : # Forward pass predictions = model ( batch_X ) loss = loss_function ( predictions , batch_y ) # Backward pass optimizer . zero_grad () loss . backward () optimizer . step () epoch_loss += loss . item () avg_loss = epoch_loss / len ( dataloader ) print ( f \"Epoch { epoch + 1 } , Loss: { avg_loss : .4f } \" ) 7.6 Learning Rate Scheduling \u00b6 from torch.optim.lr_scheduler import StepLR , ReduceLROnPlateau , CosineAnnealingLR # Step decay: reduce LR every N epochs scheduler = StepLR ( optimizer , step_size = 30 , gamma = 0.1 ) # Reduce on plateau: reduce when metric stops improving scheduler = ReduceLROnPlateau ( optimizer , mode = 'min' , factor = 0.1 , patience = 10 ) # Cosine annealing scheduler = CosineAnnealingLR ( optimizer , T_max = 50 , eta_min = 1e-6 ) # Training with scheduler for epoch in range ( num_epochs ): train_loss = train_one_epoch ( model , dataloader , optimizer ) val_loss = validate ( model , val_dataloader ) # Update learning rate scheduler . step () # For StepLR and CosineAnnealingLR # scheduler.step(val_loss) # For ReduceLROnPlateau print ( f \"Epoch { epoch + 1 } , LR: { optimizer . param_groups [ 0 ][ 'lr' ] : .6f } \" ) 8. Common Pitfalls and Best Practices \u00b6 8.1 Data Leakage \u00b6 Problem : Information from test set influences training Common mistakes : # WRONG: Standardize before splitting from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_scaled = scaler . fit_transform ( X ) # Uses info from entire dataset! X_train , X_test , y_train , y_test = train_test_split ( X_scaled , y ) # CORRECT: Fit on training, transform both X_train , X_test , y_train , y_test = train_test_split ( X , y ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # Fit only on training X_test_scaled = scaler . transform ( X_test ) # Transform using training stats # WRONG: Feature selection on entire dataset selector = SelectKBest ( k = 10 ) X_selected = selector . fit_transform ( X , y ) # Leakage! X_train , X_test = train_test_split ( X_selected ) # CORRECT: Feature selection in each CV fold from sklearn.pipeline import Pipeline pipeline = Pipeline ([ ( 'scaler' , StandardScaler ()), ( 'selector' , SelectKBest ( k = 10 )), ( 'model' , RandomForestRegressor ()) ]) scores = cross_val_score ( pipeline , X , y , cv = 5 ) 8.2 Not Using Validation Set \u00b6 Problem : Tuning hyperparameters on test set # WRONG: Tune on test set best_accuracy = 0 best_params = None for params in parameter_grid : model . set_params ( ** params ) model . fit ( X_train , y_train ) accuracy = model . score ( X_test , y_test ) # Leakage! if accuracy > best_accuracy : best_accuracy = accuracy best_params = params # CORRECT: Use separate validation set or cross-validation X_train , X_temp , y_train , y_temp = train_test_split ( X , y , test_size = 0.3 ) X_val , X_test , y_val , y_test = train_test_split ( X_temp , y_temp , test_size = 0.5 ) best_accuracy = 0 best_params = None for params in parameter_grid : model . set_params ( ** params ) model . fit ( X_train , y_train ) accuracy = model . score ( X_val , y_val ) # Tune on validation if accuracy > best_accuracy : best_accuracy = accuracy best_params = params # Final evaluation on test set model . set_params ( ** best_params ) model . fit ( X_train , y_train ) test_accuracy = model . score ( X_test , y_test ) 8.3 Ignoring Class Imbalance \u00b6 Problem : Poor performance on minority class Solutions : # 1. Class weights from sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight ( 'balanced' , classes = np . unique ( y_train ), y = y_train ) class_weight_dict = dict ( enumerate ( class_weights )) model = RandomForestClassifier ( class_weight = class_weight_dict ) # 2. Resampling from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler # Oversample minority class smote = SMOTE ( random_state = 42 ) X_resampled , y_resampled = smote . fit_resample ( X_train , y_train ) # Undersample majority class undersampler = RandomUnderSampler ( random_state = 42 ) X_resampled , y_resampled = undersampler . fit_resample ( X_train , y_train ) # 3. Use appropriate metrics # Don't use accuracy! Use precision, recall, F1, or AUC from sklearn.metrics import classification_report y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred )) 8.4 Not Checking for Errors \u00b6 # Always validate your preprocessing def validate_data ( X , y ): \"\"\"Comprehensive data validation\"\"\" # Check for NaN assert not np . isnan ( X ) . any (), \"Features contain NaN values\" assert not np . isnan ( y ) . any (), \"Targets contain NaN values\" # Check for infinite values assert not np . isinf ( X ) . any (), \"Features contain infinite values\" # Check shapes assert X . shape [ 0 ] == y . shape [ 0 ], \"X and y have different number of samples\" # Check for constant features constant_features = ( X . std ( axis = 0 ) == 0 ) . sum () if constant_features > 0 : print ( f \"Warning: { constant_features } constant features detected\" ) # Check target distribution print ( f \"Target distribution: mean= { y . mean () : .3f } , std= { y . std () : .3f } \" ) print ( f \"Target range: [ { y . min () : .3f } , { y . max () : .3f } ]\" ) return True validate_data ( X_train , y_train ) 8.5 Forgetting to Set Random Seeds \u00b6 # For reproducibility, set all random seeds import random import numpy as np import torch def set_seed ( seed = 42 ): random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False set_seed ( 42 ) 9. Practical Exercise: Complete ML Pipeline \u00b6 Task \u00b6 Build a complete machine learning pipeline to predict molecular solubility. The dataset can be downloaded from: J. Chem. Inf. Comput. Sci. 2004, 44, 3, 1000\u20131005 (https://pubs.acs.org/doi/10.1021/ci034243x) Dataset \u00b6 import pandas as pd from rdkit import Chem from rdkit.Chem import Descriptors from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split , cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error import numpy as np # Load data - adjust column name for target variable data = pd . read_csv ( 'esol.csv' ) print ( f \"Dataset size: { len ( data ) } \" ) print ( data . head ()) print ( f \"Column names: { data . columns . tolist () } \" ) Step 1: Feature Engineering \u00b6 def calculate_molecular_features ( smiles ): \"\"\"Calculate molecular descriptors from SMILES\"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None features = { 'MolWt' : Descriptors . MolWt ( mol ), 'LogP' : Descriptors . MolLogP ( mol ), 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'NumHeteroatoms' : Descriptors . NumHeteroatoms ( mol ), 'NumRings' : Descriptors . RingCount ( mol ), 'NumSaturatedRings' : Descriptors . NumSaturatedRings ( mol ) } return features # Calculate features for all molecules features_list = [] valid_indices = [] for idx , smiles in enumerate ( data [ 'SMILES' ]): features = calculate_molecular_features ( smiles ) if features is not None : features_list . append ( features ) valid_indices . append ( idx ) # Create feature DataFrame X = pd . DataFrame ( features_list ) # Use the correct column name for solubility y = data . loc [ valid_indices , 'measured log(solubility:mol/L)' ] . values print ( f \" \\n Features shape: { X . shape } \" ) print ( f \"Targets shape: { y . shape } \" ) print ( f \"Valid molecules: { len ( valid_indices ) } / { len ( data ) } \" ) Step 2: Data Validation and Exploration \u00b6 # Check for missing values print ( \" \\n Missing values:\" ) print ( X . isnull () . sum ()) # Check distributions print ( \" \\n Feature statistics:\" ) print ( X . describe ()) print ( \" \\n Target statistics:\" ) print ( f \"Mean: { y . mean () : .3f } \" ) print ( f \"Std: { y . std () : .3f } \" ) print ( f \"Min: { y . min () : .3f } \" ) print ( f \"Max: { y . max () : .3f } \" ) # Visualize correlations import matplotlib.pyplot as plt import seaborn as sns plt . figure ( figsize = ( 10 , 8 )) correlations = X . corrwith ( pd . Series ( y , index = X . index )) correlations . sort_values () . plot ( kind = 'barh' ) plt . xlabel ( 'Correlation with Solubility' ) plt . title ( 'Feature Correlations' ) plt . tight_layout () #plt.show() plt . savefig ( 'feature_importance.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () Step 3: Train-Test Split \u00b6 # Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( f \" \\n Training set size: { len ( X_train ) } \" ) print ( f \"Test set size: { len ( X_test ) } \" ) Step 4: Preprocessing \u00b6 # Standardize features scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) Step 5: Model Training with Cross-Validation \u00b6 # Try different models from sklearn.linear_model import Ridge , Lasso from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from sklearn.svm import SVR models = { 'Ridge' : Ridge ( alpha = 1.0 ), 'Lasso' : Lasso ( alpha = 0.1 ), 'Random Forest' : RandomForestRegressor ( n_estimators = 100 , random_state = 42 ), 'Gradient Boosting' : GradientBoostingRegressor ( n_estimators = 100 , random_state = 42 ), 'SVR' : SVR ( kernel = 'rbf' , C = 1.0 ) } # Cross-validation print ( \" \\n Cross-validation results:\" ) cv_results = {} for name , model in models . items (): scores = cross_val_score ( model , X_train_scaled , y_train , cv = 5 , scoring = 'r2' , n_jobs =- 1 ) cv_results [ name ] = scores print ( f \" { name : 20s } : R\u00b2 = { scores . mean () : .3f } \u00b1 { scores . std () : .3f } \" ) # Select best model best_model_name = max ( cv_results , key = lambda k : cv_results [ k ] . mean ()) best_model = models [ best_model_name ] print ( f \" \\n Best model: { best_model_name } \" ) Step 6: Hyperparameter Tuning \u00b6 from sklearn.model_selection import RandomizedSearchCV # Tune the best model (Random Forest in this example) if best_model_name == 'Random Forest' : param_distributions = { 'n_estimators' : [ 100 , 200 , 500 ], 'max_depth' : [ 10 , 20 , 30 , None ], 'min_samples_split' : [ 2 , 5 , 10 ], 'min_samples_leaf' : [ 1 , 2 , 4 ], 'max_features' : [ 'sqrt' , 'log2' , 0.5 , 1.0 , None ] # Changed: removed 'auto', added valid options } random_search = RandomizedSearchCV ( RandomForestRegressor ( random_state = 42 ), param_distributions , n_iter = 20 , cv = 5 , scoring = 'r2' , n_jobs =- 1 , random_state = 42 , verbose = 1 ) random_search . fit ( X_train_scaled , y_train ) print ( f \" \\n Best parameters: { random_search . best_params_ } \" ) print ( f \"Best CV R\u00b2: { random_search . best_score_ : .3f } \" ) best_model = random_search . best_estimator_ Step 7: Final Evaluation \u00b6 # Train on full training set best_model . fit ( X_train_scaled , y_train ) # Predict on test set y_pred_train = best_model . predict ( X_train_scaled ) y_pred_test = best_model . predict ( X_test_scaled ) # Calculate metrics train_r2 = r2_score ( y_train , y_pred_train ) train_rmse = np . sqrt ( mean_squared_error ( y_train , y_pred_train )) train_mae = mean_absolute_error ( y_train , y_pred_train ) test_r2 = r2_score ( y_test , y_pred_test ) test_rmse = np . sqrt ( mean_squared_error ( y_test , y_pred_test )) test_mae = mean_absolute_error ( y_test , y_pred_test ) print ( \" \\n Final Results:\" ) print ( f \"Training - R\u00b2: { train_r2 : .3f } , RMSE: { train_rmse : .3f } , MAE: { train_mae : .3f } \" ) print ( f \"Test - R\u00b2: { test_r2 : .3f } , RMSE: { test_rmse : .3f } , MAE: { test_mae : .3f } \" ) # Check for overfitting if train_r2 - test_r2 > 0.1 : print ( \" \\n Warning: Possible overfitting detected!\" ) else : print ( \" \\n Model generalizes well!\" ) Step 8: Visualization and Analysis \u00b6 # Prediction plots fig , axes = plt . subplots ( 1 , 2 , figsize = ( 15 , 6 )) # Training set axes [ 0 ] . scatter ( y_train , y_pred_train , alpha = 0.5 ) axes [ 0 ] . plot ([ y_train . min (), y_train . max ()], [ y_train . min (), y_train . max ()], 'r--' , lw = 2 ) axes [ 0 ] . set_xlabel ( 'True Solubility' ) axes [ 0 ] . set_ylabel ( 'Predicted Solubility' ) axes [ 0 ] . set_title ( f 'Training Set (R\u00b2 = { train_r2 : .3f } )' ) axes [ 0 ] . axis ( 'equal' ) # Test set axes [ 1 ] . scatter ( y_test , y_pred_test , alpha = 0.5 ) axes [ 1 ] . plot ([ y_test . min (), y_test . max ()], [ y_test . min (), y_test . max ()], 'r--' , lw = 2 ) axes [ 1 ] . set_xlabel ( 'True Solubility' ) axes [ 1 ] . set_ylabel ( 'Predicted Solubility' ) axes [ 1 ] . set_title ( f 'Test Set (R\u00b2 = { test_r2 : .3f } )' ) axes [ 1 ] . axis ( 'equal' ) plt . tight_layout () #plt.show() plt . savefig ( 'predicted_solubility.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () # Feature importance (for tree-based models) if hasattr ( best_model , 'feature_importances_' ): importances = pd . DataFrame ({ 'feature' : X . columns , 'importance' : best_model . feature_importances_ }) . sort_values ( 'importance' , ascending = False ) print ( \" \\n Top 5 Most Important Features:\" ) print ( importances . head ()) plt . figure ( figsize = ( 10 , 6 )) importances . plot ( x = 'feature' , y = 'importance' , kind = 'barh' ) plt . xlabel ( 'Importance' ) plt . title ( 'Feature Importance' ) plt . tight_layout () #plt.show() plt . savefig ( 'best_feature_importance.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () # Residual analysis residuals = y_test - y_pred_test plt . figure ( figsize = ( 12 , 4 )) plt . subplot ( 131 ) plt . scatter ( y_pred_test , residuals , alpha = 0.5 ) plt . axhline ( y = 0 , color = 'r' , linestyle = '--' ) plt . xlabel ( 'Predicted Values' ) plt . ylabel ( 'Residuals' ) plt . title ( 'Residual Plot' ) plt . subplot ( 132 ) plt . hist ( residuals , bins = 30 , edgecolor = 'black' ) plt . xlabel ( 'Residuals' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Residual Distribution' ) plt . subplot ( 133 ) from scipy import stats stats . probplot ( residuals , dist = \"norm\" , plot = plt ) plt . title ( 'Q-Q Plot' ) plt . tight_layout () #plt.show() plt . savefig ( 'residuals.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () Step 9: Model Persistence \u00b6 import joblib # Save model and scaler joblib . dump ( best_model , 'solubility_model.pkl' ) joblib . dump ( scaler , 'solubility_scaler.pkl' ) print ( \" \\n Model saved successfully!\" ) # Load and use model loaded_model = joblib . load ( 'solubility_model.pkl' ) loaded_scaler = joblib . load ( 'solubility_scaler.pkl' ) # Make prediction for new molecule new_smiles = \"CCO\" # Ethanol new_features = calculate_molecular_features ( new_smiles ) new_X = pd . DataFrame ([ new_features ]) new_X_scaled = loaded_scaler . transform ( new_X ) prediction = loaded_model . predict ( new_X_scaled ) print ( f \" \\n Prediction for { new_smiles } : { prediction [ 0 ] : .3f } \" ) 10. Key Takeaways \u00b6 Essential Concepts \u00b6 Always split your data before any preprocessing Use cross-validation for reliable performance estimates Watch for overfitting : Monitor both training and validation performance Choose appropriate metrics based on your problem Tune hyperparameters systematically Validate your data : Check for errors, outliers, and leakage Set random seeds for reproducibility Document everything : Parameters, preprocessing steps, results Machine Learning Workflow Summary \u00b6 1. Define Problem \u2192 2. Collect Data \u2192 3. Explore Data \u2192 4. Preprocess \u2192 5. Split Data \u2192 6. Train Models \u2192 7. Cross-Validate \u2192 8. Tune Hyperparameters \u2192 9. Evaluate on Test Set \u2192 10. Deploy/Iterate Red Flags \u00b6 Training accuracy >> Test accuracy \u2192 Overfitting Both accuracies low \u2192 Underfitting Test accuracy > Training accuracy \u2192 Data leakage Inconsistent CV scores \u2192 Data problems or small dataset Perfect scores \u2192 Check for data leakage! 11. Preparation for Days 1-5 \u00b6 Prerequisites Check \u00b6 You should now understand: - \u2713 Supervised vs unsupervised learning - \u2713 Training, validation, and test sets - \u2713 Overfitting and underfitting - \u2713 Cross-validation - \u2713 Common metrics (MSE, MAE, R\u00b2, accuracy, precision, recall, AUC) - \u2713 Hyperparameter tuning - \u2713 Basic neural network concepts What\u2019s Next? \u00b6 Day 1 : Apply these concepts to molecular representations Day 2 : Deep learning for molecular property prediction Day 3 : Graph neural networks for molecular structures Day 4 : Generative models for molecular design Day 5 : Advanced applications and deployment Recommended Practice \u00b6 Before Day 1, try: 1. Implement the solubility prediction exercise above 2. Experiment with different models and hyperparameters 3. Try other sklearn datasets (boston housing, wine quality) 4. Read sklearn documentation on your favorite algorithms 12. Additional Resources \u00b6 Books \u00b6 \u201cHands-On Machine Learning\u201d - Aur\u00e9lien G\u00e9ron \u201cPattern Recognition and Machine Learning\u201d - Christopher Bishop \u201cDeep Learning\u201d - Goodfellow, Bengio, and Courville Online Courses \u00b6 Andrew Ng\u2019s Machine Learning (Coursera) Fast.ai Practical Deep Learning Stanford CS229 (Machine Learning) Documentation \u00b6 Scikit-learn: https://scikit-learn.org/ PyTorch: https://pytorch.org/ TensorFlow: https://tensorflow.org/ Practice Platforms \u00b6 Kaggle: https://kaggle.com/ Google Colab: https://colab.research.google.com/ Papers with Code: https://paperswithcode.com/ Homework Assignment \u00b6 Complete the following before Day 1: Implement a basic ML pipeline : Use the solubility prediction example or choose your own dataset Compare 3 models : Try Random Forest, SVR, and a neural network Perform hyperparameter tuning : Use Grid Search or Random Search Analyze results : Create visualizations and interpret feature importance Document your findings : What worked? What didn\u2019t? Why? Bonus Challenges \u00b6 Implement k-fold cross-validation from scratch Build a simple neural network using only NumPy Create a function to detect and handle data leakage Visualize the decision boundary of a classifier","title":"Introduction"},{"location":"day0/#day__0__machine__learning__and__deep__learning__fundamentals","text":"","title":"Day 0: Machine Learning and Deep Learning Fundamentals"},{"location":"day0/#welcome__to__the__course","text":"Before diving into molecular and materials applications, this foundational day ensures everyone has a solid understanding of core machine learning concepts. Whether you\u2019re refreshing your knowledge or learning these concepts for the first time, this material will prepare you for the advanced topics in Days 1-5.","title":"Welcome to the Course!"},{"location":"day0/#learning__objectives","text":"By the end of Day 0, you will: - Understand the fundamental concepts of machine learning - Distinguish between different types of learning problems - Recognize and address overfitting and underfitting - Apply proper validation and evaluation techniques - Understand the basics of neural networks - Know how to optimize and tune models","title":"Learning Objectives"},{"location":"day0/#1__what__is__machine__learning","text":"","title":"1. What is Machine Learning?"},{"location":"day0/#11__definition","text":"Machine learning is the science of programming computers to learn from data without being explicitly programmed. Instead of writing rules, we provide examples and let the algorithm discover patterns. Traditional Programming : Rules + Data \u2192 Output Machine Learning : Data + Output \u2192 Rules (Model)","title":"1.1 Definition"},{"location":"day0/#12__why__machine__learning__for__science","text":"In molecular and materials science, ML helps us: - Predict properties without expensive experiments or simulations - Discover patterns in complex datasets - Generate hypotheses for new molecules or materials - Accelerate discovery by orders of magnitude - Navigate high-dimensional spaces that are intractable by traditional methods","title":"1.2 Why Machine Learning for Science?"},{"location":"day0/#13__types__of__machine__learning","text":"","title":"1.3 Types of Machine Learning"},{"location":"day0/#supervised__learning","text":"Learn from labeled examples (input-output pairs): - Regression : Predict continuous values (e.g., binding affinity, melting point) - Classification : Predict categories (e.g., toxic/non-toxic, active/inactive) # Example: Predicting molecular solubility (regression) X = molecular_features # Input: molecular descriptors y = solubility_values # Output: measured solubility model . fit ( X , y ) # Learn the relationship prediction = model . predict ( new_molecule ) # Predict for new molecule","title":"Supervised Learning"},{"location":"day0/#unsupervised__learning","text":"Find patterns in unlabeled data: - Clustering : Group similar molecules together - Dimensionality reduction : Visualize high-dimensional chemical space - Anomaly detection : Find unusual molecules # Example: Clustering molecules by similarity from sklearn.cluster import KMeans kmeans = KMeans ( n_clusters = 5 ) clusters = kmeans . fit_predict ( molecular_fingerprints ) # Molecules in same cluster are structurally similar","title":"Unsupervised Learning"},{"location":"day0/#reinforcement__learning","text":"Learn through trial and error with rewards: - Molecular optimization : Generate molecules with desired properties - Synthesis planning : Find optimal reaction pathways - Experiment design : Choose most informative experiments # Example: RL agent learns to design molecules for episode in range ( num_episodes ): state = initial_molecule while not done : action = agent . select_action ( state ) # Modify molecule reward = evaluate_properties ( new_molecule ) agent . learn ( state , action , reward ) state = new_molecule","title":"Reinforcement Learning"},{"location":"day0/#2__the__machine__learning__workflow","text":"","title":"2. The Machine Learning Workflow"},{"location":"day0/#21__problem__definition","text":"Define the goal : What do you want to predict or discover? Choose the task type : Regression, classification, generation? Define success metrics : How will you measure performance? Example: \u201cPredict whether a molecule can cross the blood-brain barrier (binary classification) with >85% accuracy.\u201d","title":"2.1 Problem Definition"},{"location":"day0/#22__data__collection__and__preparation","text":"","title":"2.2 Data Collection and Preparation"},{"location":"day0/#data__collection","text":"Experimental measurements Computational simulations (DFT, MD) Public databases (PubChem, ChEMBL, Materials Project) Literature mining","title":"Data Collection"},{"location":"day0/#data__quality__checks","text":"import pandas as pd import numpy as np # Load data data = pd . read_csv ( 'molecular_data.csv' ) # Check for missing values print ( data . isnull () . sum ()) # Check for duplicates print ( f \"Duplicates: { data . duplicated () . sum () } \" ) # Check distributions print ( data . describe ()) # Remove outliers (example: 3 sigma rule) z_scores = np . abs (( data [ 'property' ] - data [ 'property' ] . mean ()) / data [ 'property' ] . std ()) data_clean = data [ z_scores < 3 ]","title":"Data Quality Checks"},{"location":"day0/#feature__engineering","text":"Transform raw data into meaningful features: from rdkit import Chem from rdkit.Chem import Descriptors def calculate_features ( smiles ): mol = Chem . MolFromSmiles ( smiles ) features = { 'molecular_weight' : Descriptors . MolWt ( mol ), 'logP' : Descriptors . MolLogP ( mol ), 'num_h_donors' : Descriptors . NumHDonors ( mol ), 'num_h_acceptors' : Descriptors . NumHAcceptors ( mol ), 'tpsa' : Descriptors . TPSA ( mol ), 'num_rotatable_bonds' : Descriptors . NumRotatableBonds ( mol ), 'num_aromatic_rings' : Descriptors . NumAromaticRings ( mol ) } return features","title":"Feature Engineering"},{"location":"day0/#23__train-test__split","text":"Critical principle : Never test on training data! from sklearn.model_selection import train_test_split # Basic split (80% train, 20% test) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Stratified split (maintains class distribution) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , stratify = y , random_state = 42 ) Important for molecules : Use scaffold-based splits to test generalization to new chemical structures: from sklearn.model_selection import GroupShuffleSplit # Split by molecular scaffold (core structure) scaffolds = [ get_scaffold ( mol ) for mol in molecules ] splitter = GroupShuffleSplit ( n_splits = 1 , test_size = 0.2 , random_state = 42 ) train_idx , test_idx = next ( splitter . split ( X , y , groups = scaffolds )) X_train , X_test = X [ train_idx ], X [ test_idx ] y_train , y_test = y [ train_idx ], y [ test_idx ]","title":"2.3 Train-Test Split"},{"location":"day0/#3__overfitting__and__underfitting","text":"","title":"3. Overfitting and Underfitting"},{"location":"day0/#31__the__bias-variance__tradeoff","text":"Underfitting (High Bias) : - Model is too simple - Poor performance on both training and test data - Doesn\u2019t capture underlying patterns Overfitting (High Variance) : - Model is too complex - Excellent on training data, poor on test data - Memorizes noise instead of learning patterns Sweet Spot : - Balanced complexity - Good performance on both training and test data - Generalizes to new examples import matplotlib.pyplot as plt import numpy as np # Generate example data X = np . linspace ( 0 , 10 , 50 ) y = 2 * X + 1 + np . random . normal ( 0 , 2 , 50 ) # Underfitting: degree 1 polynomial (too simple) underfit_model = np . poly1d ( np . polyfit ( X , y , 1 )) # Good fit: degree 2 polynomial good_model = np . poly1d ( np . polyfit ( X , y , 2 )) # Overfitting: degree 15 polynomial (too complex) overfit_model = np . poly1d ( np . polyfit ( X , y , 15 )) # Visualize X_plot = np . linspace ( 0 , 10 , 200 ) plt . scatter ( X , y , alpha = 0.5 , label = 'Data' ) plt . plot ( X_plot , underfit_model ( X_plot ), label = 'Underfitting' , linestyle = '--' ) plt . plot ( X_plot , good_model ( X_plot ), label = 'Good Fit' ) plt . plot ( X_plot , overfit_model ( X_plot ), label = 'Overfitting' , linestyle = ':' ) plt . legend () plt . show ()","title":"3.1 The Bias-Variance Tradeoff"},{"location":"day0/#32__detecting__overfitting","text":"Learning Curves : Plot training and validation performance vs. training set size from sklearn.model_selection import learning_curve train_sizes , train_scores , val_scores = learning_curve ( model , X , y , train_sizes = np . linspace ( 0.1 , 1.0 , 10 ), cv = 5 , scoring = 'neg_mean_squared_error' ) # Plot plt . figure ( figsize = ( 10 , 6 )) plt . plot ( train_sizes , - train_scores . mean ( axis = 1 ), label = 'Training Error' ) plt . plot ( train_sizes , - val_scores . mean ( axis = 1 ), label = 'Validation Error' ) plt . xlabel ( 'Training Set Size' ) plt . ylabel ( 'Mean Squared Error' ) plt . legend () plt . title ( 'Learning Curves' ) plt . show () # Signs of overfitting: # - Large gap between training and validation curves # - Training error much lower than validation error # - Validation error increases or plateaus","title":"3.2 Detecting Overfitting"},{"location":"day0/#33__preventing__overfitting","text":"","title":"3.3 Preventing Overfitting"},{"location":"day0/#1__get__more__data","text":"The most effective solution when possible: # Data augmentation for molecules def augment_molecule ( smiles ): mol = Chem . MolFromSmiles ( smiles ) # Generate different SMILES representations augmented = [] for _ in range ( 5 ): random_smiles = Chem . MolToSmiles ( mol , doRandom = True ) augmented . append ( random_smiles ) return augmented","title":"1. Get More Data"},{"location":"day0/#2__regularization","text":"Add penalty for model complexity: L1 Regularization (Lasso) : Encourages sparsity from sklearn.linear_model import Lasso model = Lasso ( alpha = 0.1 ) # alpha controls regularization strength model . fit ( X_train , y_train ) L2 Regularization (Ridge) : Penalizes large weights from sklearn.linear_model import Ridge model = Ridge ( alpha = 1.0 ) model . fit ( X_train , y_train ) Elastic Net : Combines L1 and L2 from sklearn.linear_model import ElasticNet model = ElasticNet ( alpha = 0.1 , l1_ratio = 0.5 ) model . fit ( X_train , y_train )","title":"2. Regularization"},{"location":"day0/#3__cross-validation","text":"Use all data for both training and validation: from sklearn.model_selection import cross_val_score # 5-fold cross-validation scores = cross_val_score ( model , X , y , cv = 5 , scoring = 'r2' ) print ( f \"Cross-validation R\u00b2: { scores . mean () : .3f } \u00b1 { scores . std () : .3f } \" )","title":"3. Cross-Validation"},{"location":"day0/#4__feature__selection","text":"Remove irrelevant or redundant features: from sklearn.feature_selection import SelectKBest , f_regression # Select top 10 features selector = SelectKBest ( score_func = f_regression , k = 10 ) X_selected = selector . fit_transform ( X_train , y_train ) # Get selected feature names selected_features = X . columns [ selector . get_support ()] print ( f \"Selected features: { selected_features . tolist () } \" )","title":"4. Feature Selection"},{"location":"day0/#5__early__stopping__for__neural__networks","text":"Stop training when validation error starts increasing: from sklearn.neural_network import MLPRegressor model = MLPRegressor ( hidden_layer_sizes = ( 100 , 50 ), early_stopping = True , validation_fraction = 0.2 , n_iter_no_change = 10 # Stop if no improvement for 10 epochs )","title":"5. Early Stopping (for neural networks)"},{"location":"day0/#6__dropout__for__neural__networks","text":"Randomly deactivate neurons during training: import torch.nn as nn class MolecularNN ( nn . Module ): def __init__ ( self , input_dim , hidden_dim ): super () . __init__ () self . fc1 = nn . Linear ( input_dim , hidden_dim ) self . dropout1 = nn . Dropout ( 0.3 ) # Drop 30% of neurons self . fc2 = nn . Linear ( hidden_dim , hidden_dim ) self . dropout2 = nn . Dropout ( 0.3 ) self . fc3 = nn . Linear ( hidden_dim , 1 ) def forward ( self , x ): x = torch . relu ( self . fc1 ( x )) x = self . dropout1 ( x ) x = torch . relu ( self . fc2 ( x )) x = self . dropout2 ( x ) return self . fc3 ( x )","title":"6. Dropout (for neural networks)"},{"location":"day0/#34__addressing__underfitting","text":"Use a more complex model : Add more features, use deeper networks Remove regularization : Reduce alpha/lambda values Engineer better features : Domain knowledge can help Train longer : Increase number of epochs/iterations Check for errors : Ensure data is preprocessed correctly","title":"3.4 Addressing Underfitting"},{"location":"day0/#4__cross-validation","text":"","title":"4. Cross-Validation"},{"location":"day0/#41__why__cross-validation","text":"Makes efficient use of limited data Provides more reliable performance estimates Reduces sensitivity to train-test split Helps detect overfitting","title":"4.1 Why Cross-Validation?"},{"location":"day0/#42__k-fold__cross-validation","text":"Split data into K parts, train on K-1, test on 1, repeat K times: from sklearn.model_selection import KFold , cross_validate # 5-fold cross-validation kf = KFold ( n_splits = 5 , shuffle = True , random_state = 42 ) # Get multiple metrics scoring = { 'r2' : 'r2' , 'rmse' : 'neg_mean_squared_error' , 'mae' : 'neg_mean_absolute_error' } results = cross_validate ( model , X , y , cv = kf , scoring = scoring , return_train_score = True ) print ( f \"Test R\u00b2: { results [ 'test_r2' ] . mean () : .3f } \u00b1 { results [ 'test_r2' ] . std () : .3f } \" ) print ( f \"Test RMSE: { np . sqrt ( - results [ 'test_rmse' ] . mean ()) : .3f } \" ) print ( f \"Test MAE: { - results [ 'test_mae' ] . mean () : .3f } \" )","title":"4.2 K-Fold Cross-Validation"},{"location":"day0/#43__stratified__k-fold","text":"Maintains class distribution in each fold (for classification): from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold ( n_splits = 5 , shuffle = True , random_state = 42 ) for fold , ( train_idx , val_idx ) in enumerate ( skf . split ( X , y )): print ( f \"Fold { fold + 1 } :\" ) print ( f \" Training class distribution: { np . bincount ( y [ train_idx ]) } \" ) print ( f \" Validation class distribution: { np . bincount ( y [ val_idx ]) } \" )","title":"4.3 Stratified K-Fold"},{"location":"day0/#44__leave-one-out__cross-validation__loocv","text":"Each sample is used once as test set: from sklearn.model_selection import LeaveOneOut loo = LeaveOneOut () scores = cross_val_score ( model , X , y , cv = loo , scoring = 'r2' ) print ( f \"LOOCV R\u00b2: { scores . mean () : .3f } \" ) # Note: Computationally expensive for large datasets! # Use only when data is very limited","title":"4.4 Leave-One-Out Cross-Validation (LOOCV)"},{"location":"day0/#45__time__series__cross-validation","text":"For temporal data (don\u2019t peek into the future!): from sklearn.model_selection import TimeSeriesSplit tscv = TimeSeriesSplit ( n_splits = 5 ) for fold , ( train_idx , test_idx ) in enumerate ( tscv . split ( X )): print ( f \"Fold { fold + 1 } :\" ) print ( f \" Train: { train_idx . min () } to { train_idx . max () } \" ) print ( f \" Test: { test_idx . min () } to { test_idx . max () } \" )","title":"4.5 Time Series Cross-Validation"},{"location":"day0/#5__model__evaluation__metrics","text":"","title":"5. Model Evaluation Metrics"},{"location":"day0/#51__regression__metrics","text":"","title":"5.1 Regression Metrics"},{"location":"day0/#mean__absolute__error__mae","text":"Average absolute difference between predictions and true values: from sklearn.metrics import mean_absolute_error mae = mean_absolute_error ( y_true , y_pred ) print ( f \"MAE: { mae : .3f } \" ) # Interpretation: Average prediction error in original units # Lower is better # Robust to outliers","title":"Mean Absolute Error (MAE)"},{"location":"day0/#mean__squared__error__mse__and__root__mean__squared__error__rmse","text":"from sklearn.metrics import mean_squared_error mse = mean_squared_error ( y_true , y_pred ) rmse = np . sqrt ( mse ) print ( f \"MSE: { mse : .3f } \" ) print ( f \"RMSE: { rmse : .3f } \" ) # Interpretation: Penalizes large errors more than MAE # RMSE in same units as target variable # Lower is better","title":"Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)"},{"location":"day0/#r2__coefficient__of__determination","text":"from sklearn.metrics import r2_score r2 = r2_score ( y_true , y_pred ) print ( f \"R\u00b2: { r2 : .3f } \" ) # Interpretation: # R\u00b2 = 1: Perfect predictions # R\u00b2 = 0: As good as predicting mean # R\u00b2 < 0: Worse than predicting mean # Range: (-\u221e, 1]","title":"R\u00b2 (Coefficient of Determination)"},{"location":"day0/#visualization","text":"import matplotlib.pyplot as plt plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_true , y_pred , alpha = 0.5 ) plt . plot ([ y_true . min (), y_true . max ()], [ y_true . min (), y_true . max ()], 'r--' , linewidth = 2 , label = 'Perfect Prediction' ) plt . xlabel ( 'True Values' ) plt . ylabel ( 'Predicted Values' ) plt . title ( f 'R\u00b2 = { r2 : .3f } , RMSE = { rmse : .3f } ' ) plt . legend () plt . axis ( 'equal' ) plt . show ()","title":"Visualization"},{"location":"day0/#52__classification__metrics","text":"","title":"5.2 Classification Metrics"},{"location":"day0/#confusion__matrix","text":"from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay cm = confusion_matrix ( y_true , y_pred ) disp = ConfusionMatrixDisplay ( confusion_matrix = cm , display_labels = [ 'Inactive' , 'Active' ]) disp . plot () plt . show () # Predicted # Negative Positive # Actual Neg TN FP # Pos FN TP","title":"Confusion Matrix"},{"location":"day0/#accuracy__precision__recall__f1-score","text":"from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score accuracy = accuracy_score ( y_true , y_pred ) precision = precision_score ( y_true , y_pred ) recall = recall_score ( y_true , y_pred ) f1 = f1_score ( y_true , y_pred ) print ( f \"Accuracy: { accuracy : .3f } - (TP + TN) / Total\" ) print ( f \"Precision: { precision : .3f } - TP / (TP + FP) - How many predicted positives are correct?\" ) print ( f \"Recall: { recall : .3f } - TP / (TP + FN) - How many actual positives did we find?\" ) print ( f \"F1-Score: { f1 : .3f } - Harmonic mean of precision and recall\" )","title":"Accuracy, Precision, Recall, F1-Score"},{"location":"day0/#roc__curve__and__auc","text":"from sklearn.metrics import roc_curve , roc_auc_score # Get probability predictions y_prob = model . predict_proba ( X_test )[:, 1 ] # Calculate ROC curve fpr , tpr , thresholds = roc_curve ( y_test , y_prob ) auc = roc_auc_score ( y_test , y_prob ) # Plot plt . figure ( figsize = ( 8 , 6 )) plt . plot ( fpr , tpr , label = f 'ROC Curve (AUC = { auc : .3f } )' ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], 'k--' , label = 'Random Classifier' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver Operating Characteristic (ROC) Curve' ) plt . legend () plt . show () # AUC interpretation: # 1.0: Perfect classifier # 0.5: Random guessing # < 0.5: Worse than random (flip predictions!)","title":"ROC Curve and AUC"},{"location":"day0/#precision-recall__curve","text":"Better than ROC for imbalanced datasets: from sklearn.metrics import precision_recall_curve , average_precision_score precision , recall , thresholds = precision_recall_curve ( y_test , y_prob ) ap = average_precision_score ( y_test , y_prob ) plt . figure ( figsize = ( 8 , 6 )) plt . plot ( recall , precision , label = f 'PR Curve (AP = { ap : .3f } )' ) plt . xlabel ( 'Recall' ) plt . ylabel ( 'Precision' ) plt . title ( 'Precision-Recall Curve' ) plt . legend () plt . show ()","title":"Precision-Recall Curve"},{"location":"day0/#53__choosing__the__right__metric","text":"For Regression : - MAE: Easy to interpret, robust to outliers - RMSE: Penalizes large errors - R\u00b2: Normalized, easy to compare across datasets For Classification : - Balanced classes: Accuracy, F1-score - Imbalanced classes: Precision, Recall, AUC, Average Precision - Cost-sensitive: Define custom metric based on FP/FN costs For Molecular Applications : - Drug discovery: Prioritize recall (find all active compounds) - Toxicity prediction: Prioritize precision (avoid false negatives) - Property prediction: RMSE or MAE depending on outlier sensitivity","title":"5.3 Choosing the Right Metric"},{"location":"day0/#6__hyperparameter__tuning","text":"","title":"6. Hyperparameter Tuning"},{"location":"day0/#61__what__are__hyperparameters","text":"Parameters set before training (not learned from data): - Learning rate - Number of layers/neurons - Regularization strength - Number of trees in random forest - Kernel parameters in SVM","title":"6.1 What are Hyperparameters?"},{"location":"day0/#62__grid__search","text":"Try all combinations in a grid: from sklearn.model_selection import GridSearchCV from sklearn.ensemble import RandomForestRegressor # Define parameter grid param_grid = { 'n_estimators' : [ 100 , 200 , 500 ], 'max_depth' : [ 10 , 20 , 30 , None ], 'min_samples_split' : [ 2 , 5 , 10 ], 'min_samples_leaf' : [ 1 , 2 , 4 ] } # Grid search with cross-validation grid_search = GridSearchCV ( RandomForestRegressor ( random_state = 42 ), param_grid , cv = 5 , scoring = 'r2' , n_jobs =- 1 , # Use all CPUs verbose = 2 ) grid_search . fit ( X_train , y_train ) print ( f \"Best parameters: { grid_search . best_params_ } \" ) print ( f \"Best CV score: { grid_search . best_score_ : .3f } \" ) # Use best model best_model = grid_search . best_estimator_","title":"6.2 Grid Search"},{"location":"day0/#63__random__search","text":"Sample random combinations (more efficient): from sklearn.model_selection import RandomizedSearchCV from scipy.stats import randint , uniform # Define parameter distributions param_distributions = { 'n_estimators' : randint ( 100 , 1000 ), 'max_depth' : randint ( 10 , 50 ), 'min_samples_split' : randint ( 2 , 20 ), 'min_samples_leaf' : randint ( 1 , 10 ), 'max_features' : uniform ( 0.1 , 0.9 ) } random_search = RandomizedSearchCV ( RandomForestRegressor ( random_state = 42 ), param_distributions , n_iter = 50 , # Number of random combinations to try cv = 5 , scoring = 'r2' , n_jobs =- 1 , random_state = 42 , verbose = 2 ) random_search . fit ( X_train , y_train ) print ( f \"Best parameters: { random_search . best_params_ } \" ) print ( f \"Best CV score: { random_search . best_score_ : .3f } \" )","title":"6.3 Random Search"},{"location":"day0/#64__bayesian__optimization","text":"Intelligent search using previous results: from skopt import BayesSearchCV from skopt.space import Real , Integer # Define search space search_spaces = { 'n_estimators' : Integer ( 100 , 1000 ), 'max_depth' : Integer ( 10 , 50 ), 'min_samples_split' : Integer ( 2 , 20 ), 'min_samples_leaf' : Integer ( 1 , 10 ), 'max_features' : Real ( 0.1 , 1.0 ) } bayes_search = BayesSearchCV ( RandomForestRegressor ( random_state = 42 ), search_spaces , n_iter = 50 , cv = 5 , scoring = 'r2' , n_jobs =- 1 , random_state = 42 ) bayes_search . fit ( X_train , y_train ) print ( f \"Best parameters: { bayes_search . best_params_ } \" ) print ( f \"Best CV score: { bayes_search . best_score_ : .3f } \" )","title":"6.4 Bayesian Optimization"},{"location":"day0/#7__neural__networks__basics","text":"","title":"7. Neural Networks Basics"},{"location":"day0/#71__architecture__components","text":"","title":"7.1 Architecture Components"},{"location":"day0/#neuron__perceptron","text":"Basic building block: output = activation(w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b) import numpy as np class Neuron : def __init__ ( self , input_dim ): self . weights = np . random . randn ( input_dim ) self . bias = np . random . randn () def forward ( self , x ): return np . dot ( self . weights , x ) + self . bias def activate ( self , z ): # ReLU activation return np . maximum ( 0 , z )","title":"Neuron (Perceptron)"},{"location":"day0/#layers","text":"import torch.nn as nn # Feedforward network model = nn . Sequential ( nn . Linear ( input_dim , 128 ), # Input layer nn . ReLU (), # Activation nn . Dropout ( 0.2 ), # Regularization nn . Linear ( 128 , 64 ), # Hidden layer nn . ReLU (), nn . Dropout ( 0.2 ), nn . Linear ( 64 , output_dim ) # Output layer )","title":"Layers"},{"location":"day0/#72__activation__functions","text":"","title":"7.2 Activation Functions"},{"location":"day0/#common__activations","text":"import torch import torch.nn.functional as F x = torch . linspace ( - 3 , 3 , 100 ) # ReLU: max(0, x) relu = F . relu ( x ) # Sigmoid: 1 / (1 + e^-x) sigmoid = torch . sigmoid ( x ) # Tanh: (e^x - e^-x) / (e^x + e^-x) tanh = torch . tanh ( x ) # Leaky ReLU: max(0.01x, x) leaky_relu = F . leaky_relu ( x , negative_slope = 0.01 ) # Softmax (for multiple classes) logits = torch . tensor ([[ 1.0 , 2.0 , 3.0 ]]) softmax = F . softmax ( logits , dim = 1 ) print ( f \"Softmax output: { softmax } \" ) # Sums to 1 When to use : - ReLU : Default choice for hidden layers - Sigmoid : Binary classification output - Tanh : When you want outputs in [-1, 1] - Softmax : Multi-class classification output - Leaky ReLU : When dealing with dying ReLU problem","title":"Common Activations"},{"location":"day0/#73__loss__functions","text":"","title":"7.3 Loss Functions"},{"location":"day0/#regression","text":"import torch.nn as nn # Mean Squared Error mse_loss = nn . MSELoss () loss = mse_loss ( predictions , targets ) # Mean Absolute Error mae_loss = nn . L1Loss () loss = mae_loss ( predictions , targets ) # Huber Loss (robust to outliers) huber_loss = nn . SmoothL1Loss () loss = huber_loss ( predictions , targets )","title":"Regression"},{"location":"day0/#classification","text":"# Binary Cross-Entropy bce_loss = nn . BCEWithLogitsLoss () # Includes sigmoid loss = bce_loss ( logits , targets ) # Multi-class Cross-Entropy ce_loss = nn . CrossEntropyLoss () # Includes softmax loss = ce_loss ( logits , targets )","title":"Classification"},{"location":"day0/#74__optimization","text":"","title":"7.4 Optimization"},{"location":"day0/#gradient__descent","text":"# Basic gradient descent learning_rate = 0.01 for epoch in range ( num_epochs ): # Forward pass predictions = model ( X ) loss = loss_function ( predictions , y ) # Backward pass loss . backward () # Compute gradients # Update weights with torch . no_grad (): for param in model . parameters (): param -= learning_rate * param . grad # Zero gradients model . zero_grad ()","title":"Gradient Descent"},{"location":"day0/#common__optimizers","text":"import torch.optim as optim # Stochastic Gradient Descent (SGD) optimizer = optim . SGD ( model . parameters (), lr = 0.01 , momentum = 0.9 ) # Adam (Adaptive Moment Estimation) - most common optimizer = optim . Adam ( model . parameters (), lr = 0.001 , betas = ( 0.9 , 0.999 )) # AdamW (Adam with weight decay) optimizer = optim . AdamW ( model . parameters (), lr = 0.001 , weight_decay = 0.01 ) # RMSprop optimizer = optim . RMSprop ( model . parameters (), lr = 0.001 ) # Training loop for epoch in range ( num_epochs ): optimizer . zero_grad () predictions = model ( X ) loss = loss_function ( predictions , y ) loss . backward () optimizer . step ()","title":"Common Optimizers"},{"location":"day0/#75__batch__training","text":"from torch.utils.data import DataLoader , TensorDataset # Create dataset and dataloader dataset = TensorDataset ( X_tensor , y_tensor ) dataloader = DataLoader ( dataset , batch_size = 32 , shuffle = True ) # Training with batches for epoch in range ( num_epochs ): epoch_loss = 0 for batch_X , batch_y in dataloader : # Forward pass predictions = model ( batch_X ) loss = loss_function ( predictions , batch_y ) # Backward pass optimizer . zero_grad () loss . backward () optimizer . step () epoch_loss += loss . item () avg_loss = epoch_loss / len ( dataloader ) print ( f \"Epoch { epoch + 1 } , Loss: { avg_loss : .4f } \" )","title":"7.5 Batch Training"},{"location":"day0/#76__learning__rate__scheduling","text":"from torch.optim.lr_scheduler import StepLR , ReduceLROnPlateau , CosineAnnealingLR # Step decay: reduce LR every N epochs scheduler = StepLR ( optimizer , step_size = 30 , gamma = 0.1 ) # Reduce on plateau: reduce when metric stops improving scheduler = ReduceLROnPlateau ( optimizer , mode = 'min' , factor = 0.1 , patience = 10 ) # Cosine annealing scheduler = CosineAnnealingLR ( optimizer , T_max = 50 , eta_min = 1e-6 ) # Training with scheduler for epoch in range ( num_epochs ): train_loss = train_one_epoch ( model , dataloader , optimizer ) val_loss = validate ( model , val_dataloader ) # Update learning rate scheduler . step () # For StepLR and CosineAnnealingLR # scheduler.step(val_loss) # For ReduceLROnPlateau print ( f \"Epoch { epoch + 1 } , LR: { optimizer . param_groups [ 0 ][ 'lr' ] : .6f } \" )","title":"7.6 Learning Rate Scheduling"},{"location":"day0/#8__common__pitfalls__and__best__practices","text":"","title":"8. Common Pitfalls and Best Practices"},{"location":"day0/#81__data__leakage","text":"Problem : Information from test set influences training Common mistakes : # WRONG: Standardize before splitting from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_scaled = scaler . fit_transform ( X ) # Uses info from entire dataset! X_train , X_test , y_train , y_test = train_test_split ( X_scaled , y ) # CORRECT: Fit on training, transform both X_train , X_test , y_train , y_test = train_test_split ( X , y ) scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) # Fit only on training X_test_scaled = scaler . transform ( X_test ) # Transform using training stats # WRONG: Feature selection on entire dataset selector = SelectKBest ( k = 10 ) X_selected = selector . fit_transform ( X , y ) # Leakage! X_train , X_test = train_test_split ( X_selected ) # CORRECT: Feature selection in each CV fold from sklearn.pipeline import Pipeline pipeline = Pipeline ([ ( 'scaler' , StandardScaler ()), ( 'selector' , SelectKBest ( k = 10 )), ( 'model' , RandomForestRegressor ()) ]) scores = cross_val_score ( pipeline , X , y , cv = 5 )","title":"8.1 Data Leakage"},{"location":"day0/#82__not__using__validation__set","text":"Problem : Tuning hyperparameters on test set # WRONG: Tune on test set best_accuracy = 0 best_params = None for params in parameter_grid : model . set_params ( ** params ) model . fit ( X_train , y_train ) accuracy = model . score ( X_test , y_test ) # Leakage! if accuracy > best_accuracy : best_accuracy = accuracy best_params = params # CORRECT: Use separate validation set or cross-validation X_train , X_temp , y_train , y_temp = train_test_split ( X , y , test_size = 0.3 ) X_val , X_test , y_val , y_test = train_test_split ( X_temp , y_temp , test_size = 0.5 ) best_accuracy = 0 best_params = None for params in parameter_grid : model . set_params ( ** params ) model . fit ( X_train , y_train ) accuracy = model . score ( X_val , y_val ) # Tune on validation if accuracy > best_accuracy : best_accuracy = accuracy best_params = params # Final evaluation on test set model . set_params ( ** best_params ) model . fit ( X_train , y_train ) test_accuracy = model . score ( X_test , y_test )","title":"8.2 Not Using Validation Set"},{"location":"day0/#83__ignoring__class__imbalance","text":"Problem : Poor performance on minority class Solutions : # 1. Class weights from sklearn.utils.class_weight import compute_class_weight class_weights = compute_class_weight ( 'balanced' , classes = np . unique ( y_train ), y = y_train ) class_weight_dict = dict ( enumerate ( class_weights )) model = RandomForestClassifier ( class_weight = class_weight_dict ) # 2. Resampling from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler # Oversample minority class smote = SMOTE ( random_state = 42 ) X_resampled , y_resampled = smote . fit_resample ( X_train , y_train ) # Undersample majority class undersampler = RandomUnderSampler ( random_state = 42 ) X_resampled , y_resampled = undersampler . fit_resample ( X_train , y_train ) # 3. Use appropriate metrics # Don't use accuracy! Use precision, recall, F1, or AUC from sklearn.metrics import classification_report y_pred = model . predict ( X_test ) print ( classification_report ( y_test , y_pred ))","title":"8.3 Ignoring Class Imbalance"},{"location":"day0/#84__not__checking__for__errors","text":"# Always validate your preprocessing def validate_data ( X , y ): \"\"\"Comprehensive data validation\"\"\" # Check for NaN assert not np . isnan ( X ) . any (), \"Features contain NaN values\" assert not np . isnan ( y ) . any (), \"Targets contain NaN values\" # Check for infinite values assert not np . isinf ( X ) . any (), \"Features contain infinite values\" # Check shapes assert X . shape [ 0 ] == y . shape [ 0 ], \"X and y have different number of samples\" # Check for constant features constant_features = ( X . std ( axis = 0 ) == 0 ) . sum () if constant_features > 0 : print ( f \"Warning: { constant_features } constant features detected\" ) # Check target distribution print ( f \"Target distribution: mean= { y . mean () : .3f } , std= { y . std () : .3f } \" ) print ( f \"Target range: [ { y . min () : .3f } , { y . max () : .3f } ]\" ) return True validate_data ( X_train , y_train )","title":"8.4 Not Checking for Errors"},{"location":"day0/#85__forgetting__to__set__random__seeds","text":"# For reproducibility, set all random seeds import random import numpy as np import torch def set_seed ( seed = 42 ): random . seed ( seed ) np . random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) torch . backends . cudnn . deterministic = True torch . backends . cudnn . benchmark = False set_seed ( 42 )","title":"8.5 Forgetting to Set Random Seeds"},{"location":"day0/#9__practical__exercise__complete__ml__pipeline","text":"","title":"9. Practical Exercise: Complete ML Pipeline"},{"location":"day0/#task","text":"Build a complete machine learning pipeline to predict molecular solubility. The dataset can be downloaded from: J. Chem. Inf. Comput. Sci. 2004, 44, 3, 1000\u20131005 (https://pubs.acs.org/doi/10.1021/ci034243x)","title":"Task"},{"location":"day0/#dataset","text":"import pandas as pd from rdkit import Chem from rdkit.Chem import Descriptors from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split , cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error import numpy as np # Load data - adjust column name for target variable data = pd . read_csv ( 'esol.csv' ) print ( f \"Dataset size: { len ( data ) } \" ) print ( data . head ()) print ( f \"Column names: { data . columns . tolist () } \" )","title":"Dataset"},{"location":"day0/#step__1__feature__engineering","text":"def calculate_molecular_features ( smiles ): \"\"\"Calculate molecular descriptors from SMILES\"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None features = { 'MolWt' : Descriptors . MolWt ( mol ), 'LogP' : Descriptors . MolLogP ( mol ), 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'NumHeteroatoms' : Descriptors . NumHeteroatoms ( mol ), 'NumRings' : Descriptors . RingCount ( mol ), 'NumSaturatedRings' : Descriptors . NumSaturatedRings ( mol ) } return features # Calculate features for all molecules features_list = [] valid_indices = [] for idx , smiles in enumerate ( data [ 'SMILES' ]): features = calculate_molecular_features ( smiles ) if features is not None : features_list . append ( features ) valid_indices . append ( idx ) # Create feature DataFrame X = pd . DataFrame ( features_list ) # Use the correct column name for solubility y = data . loc [ valid_indices , 'measured log(solubility:mol/L)' ] . values print ( f \" \\n Features shape: { X . shape } \" ) print ( f \"Targets shape: { y . shape } \" ) print ( f \"Valid molecules: { len ( valid_indices ) } / { len ( data ) } \" )","title":"Step 1: Feature Engineering"},{"location":"day0/#step__2__data__validation__and__exploration","text":"# Check for missing values print ( \" \\n Missing values:\" ) print ( X . isnull () . sum ()) # Check distributions print ( \" \\n Feature statistics:\" ) print ( X . describe ()) print ( \" \\n Target statistics:\" ) print ( f \"Mean: { y . mean () : .3f } \" ) print ( f \"Std: { y . std () : .3f } \" ) print ( f \"Min: { y . min () : .3f } \" ) print ( f \"Max: { y . max () : .3f } \" ) # Visualize correlations import matplotlib.pyplot as plt import seaborn as sns plt . figure ( figsize = ( 10 , 8 )) correlations = X . corrwith ( pd . Series ( y , index = X . index )) correlations . sort_values () . plot ( kind = 'barh' ) plt . xlabel ( 'Correlation with Solubility' ) plt . title ( 'Feature Correlations' ) plt . tight_layout () #plt.show() plt . savefig ( 'feature_importance.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close ()","title":"Step 2: Data Validation and Exploration"},{"location":"day0/#step__3__train-test__split","text":"# Split data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) print ( f \" \\n Training set size: { len ( X_train ) } \" ) print ( f \"Test set size: { len ( X_test ) } \" )","title":"Step 3: Train-Test Split"},{"location":"day0/#step__4__preprocessing","text":"# Standardize features scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test )","title":"Step 4: Preprocessing"},{"location":"day0/#step__5__model__training__with__cross-validation","text":"# Try different models from sklearn.linear_model import Ridge , Lasso from sklearn.ensemble import RandomForestRegressor , GradientBoostingRegressor from sklearn.svm import SVR models = { 'Ridge' : Ridge ( alpha = 1.0 ), 'Lasso' : Lasso ( alpha = 0.1 ), 'Random Forest' : RandomForestRegressor ( n_estimators = 100 , random_state = 42 ), 'Gradient Boosting' : GradientBoostingRegressor ( n_estimators = 100 , random_state = 42 ), 'SVR' : SVR ( kernel = 'rbf' , C = 1.0 ) } # Cross-validation print ( \" \\n Cross-validation results:\" ) cv_results = {} for name , model in models . items (): scores = cross_val_score ( model , X_train_scaled , y_train , cv = 5 , scoring = 'r2' , n_jobs =- 1 ) cv_results [ name ] = scores print ( f \" { name : 20s } : R\u00b2 = { scores . mean () : .3f } \u00b1 { scores . std () : .3f } \" ) # Select best model best_model_name = max ( cv_results , key = lambda k : cv_results [ k ] . mean ()) best_model = models [ best_model_name ] print ( f \" \\n Best model: { best_model_name } \" )","title":"Step 5: Model Training with Cross-Validation"},{"location":"day0/#step__6__hyperparameter__tuning","text":"from sklearn.model_selection import RandomizedSearchCV # Tune the best model (Random Forest in this example) if best_model_name == 'Random Forest' : param_distributions = { 'n_estimators' : [ 100 , 200 , 500 ], 'max_depth' : [ 10 , 20 , 30 , None ], 'min_samples_split' : [ 2 , 5 , 10 ], 'min_samples_leaf' : [ 1 , 2 , 4 ], 'max_features' : [ 'sqrt' , 'log2' , 0.5 , 1.0 , None ] # Changed: removed 'auto', added valid options } random_search = RandomizedSearchCV ( RandomForestRegressor ( random_state = 42 ), param_distributions , n_iter = 20 , cv = 5 , scoring = 'r2' , n_jobs =- 1 , random_state = 42 , verbose = 1 ) random_search . fit ( X_train_scaled , y_train ) print ( f \" \\n Best parameters: { random_search . best_params_ } \" ) print ( f \"Best CV R\u00b2: { random_search . best_score_ : .3f } \" ) best_model = random_search . best_estimator_","title":"Step 6: Hyperparameter Tuning"},{"location":"day0/#step__7__final__evaluation","text":"# Train on full training set best_model . fit ( X_train_scaled , y_train ) # Predict on test set y_pred_train = best_model . predict ( X_train_scaled ) y_pred_test = best_model . predict ( X_test_scaled ) # Calculate metrics train_r2 = r2_score ( y_train , y_pred_train ) train_rmse = np . sqrt ( mean_squared_error ( y_train , y_pred_train )) train_mae = mean_absolute_error ( y_train , y_pred_train ) test_r2 = r2_score ( y_test , y_pred_test ) test_rmse = np . sqrt ( mean_squared_error ( y_test , y_pred_test )) test_mae = mean_absolute_error ( y_test , y_pred_test ) print ( \" \\n Final Results:\" ) print ( f \"Training - R\u00b2: { train_r2 : .3f } , RMSE: { train_rmse : .3f } , MAE: { train_mae : .3f } \" ) print ( f \"Test - R\u00b2: { test_r2 : .3f } , RMSE: { test_rmse : .3f } , MAE: { test_mae : .3f } \" ) # Check for overfitting if train_r2 - test_r2 > 0.1 : print ( \" \\n Warning: Possible overfitting detected!\" ) else : print ( \" \\n Model generalizes well!\" )","title":"Step 7: Final Evaluation"},{"location":"day0/#step__8__visualization__and__analysis","text":"# Prediction plots fig , axes = plt . subplots ( 1 , 2 , figsize = ( 15 , 6 )) # Training set axes [ 0 ] . scatter ( y_train , y_pred_train , alpha = 0.5 ) axes [ 0 ] . plot ([ y_train . min (), y_train . max ()], [ y_train . min (), y_train . max ()], 'r--' , lw = 2 ) axes [ 0 ] . set_xlabel ( 'True Solubility' ) axes [ 0 ] . set_ylabel ( 'Predicted Solubility' ) axes [ 0 ] . set_title ( f 'Training Set (R\u00b2 = { train_r2 : .3f } )' ) axes [ 0 ] . axis ( 'equal' ) # Test set axes [ 1 ] . scatter ( y_test , y_pred_test , alpha = 0.5 ) axes [ 1 ] . plot ([ y_test . min (), y_test . max ()], [ y_test . min (), y_test . max ()], 'r--' , lw = 2 ) axes [ 1 ] . set_xlabel ( 'True Solubility' ) axes [ 1 ] . set_ylabel ( 'Predicted Solubility' ) axes [ 1 ] . set_title ( f 'Test Set (R\u00b2 = { test_r2 : .3f } )' ) axes [ 1 ] . axis ( 'equal' ) plt . tight_layout () #plt.show() plt . savefig ( 'predicted_solubility.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () # Feature importance (for tree-based models) if hasattr ( best_model , 'feature_importances_' ): importances = pd . DataFrame ({ 'feature' : X . columns , 'importance' : best_model . feature_importances_ }) . sort_values ( 'importance' , ascending = False ) print ( \" \\n Top 5 Most Important Features:\" ) print ( importances . head ()) plt . figure ( figsize = ( 10 , 6 )) importances . plot ( x = 'feature' , y = 'importance' , kind = 'barh' ) plt . xlabel ( 'Importance' ) plt . title ( 'Feature Importance' ) plt . tight_layout () #plt.show() plt . savefig ( 'best_feature_importance.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () # Residual analysis residuals = y_test - y_pred_test plt . figure ( figsize = ( 12 , 4 )) plt . subplot ( 131 ) plt . scatter ( y_pred_test , residuals , alpha = 0.5 ) plt . axhline ( y = 0 , color = 'r' , linestyle = '--' ) plt . xlabel ( 'Predicted Values' ) plt . ylabel ( 'Residuals' ) plt . title ( 'Residual Plot' ) plt . subplot ( 132 ) plt . hist ( residuals , bins = 30 , edgecolor = 'black' ) plt . xlabel ( 'Residuals' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Residual Distribution' ) plt . subplot ( 133 ) from scipy import stats stats . probplot ( residuals , dist = \"norm\" , plot = plt ) plt . title ( 'Q-Q Plot' ) plt . tight_layout () #plt.show() plt . savefig ( 'residuals.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close ()","title":"Step 8: Visualization and Analysis"},{"location":"day0/#step__9__model__persistence","text":"import joblib # Save model and scaler joblib . dump ( best_model , 'solubility_model.pkl' ) joblib . dump ( scaler , 'solubility_scaler.pkl' ) print ( \" \\n Model saved successfully!\" ) # Load and use model loaded_model = joblib . load ( 'solubility_model.pkl' ) loaded_scaler = joblib . load ( 'solubility_scaler.pkl' ) # Make prediction for new molecule new_smiles = \"CCO\" # Ethanol new_features = calculate_molecular_features ( new_smiles ) new_X = pd . DataFrame ([ new_features ]) new_X_scaled = loaded_scaler . transform ( new_X ) prediction = loaded_model . predict ( new_X_scaled ) print ( f \" \\n Prediction for { new_smiles } : { prediction [ 0 ] : .3f } \" )","title":"Step 9: Model Persistence"},{"location":"day0/#10__key__takeaways","text":"","title":"10. Key Takeaways"},{"location":"day0/#essential__concepts","text":"Always split your data before any preprocessing Use cross-validation for reliable performance estimates Watch for overfitting : Monitor both training and validation performance Choose appropriate metrics based on your problem Tune hyperparameters systematically Validate your data : Check for errors, outliers, and leakage Set random seeds for reproducibility Document everything : Parameters, preprocessing steps, results","title":"Essential Concepts"},{"location":"day0/#machine__learning__workflow__summary","text":"1. Define Problem \u2192 2. Collect Data \u2192 3. Explore Data \u2192 4. Preprocess \u2192 5. Split Data \u2192 6. Train Models \u2192 7. Cross-Validate \u2192 8. Tune Hyperparameters \u2192 9. Evaluate on Test Set \u2192 10. Deploy/Iterate","title":"Machine Learning Workflow Summary"},{"location":"day0/#red__flags","text":"Training accuracy >> Test accuracy \u2192 Overfitting Both accuracies low \u2192 Underfitting Test accuracy > Training accuracy \u2192 Data leakage Inconsistent CV scores \u2192 Data problems or small dataset Perfect scores \u2192 Check for data leakage!","title":"Red Flags"},{"location":"day0/#11__preparation__for__days__1-5","text":"","title":"11. Preparation for Days 1-5"},{"location":"day0/#prerequisites__check","text":"You should now understand: - \u2713 Supervised vs unsupervised learning - \u2713 Training, validation, and test sets - \u2713 Overfitting and underfitting - \u2713 Cross-validation - \u2713 Common metrics (MSE, MAE, R\u00b2, accuracy, precision, recall, AUC) - \u2713 Hyperparameter tuning - \u2713 Basic neural network concepts","title":"Prerequisites Check"},{"location":"day0/#whats__next","text":"Day 1 : Apply these concepts to molecular representations Day 2 : Deep learning for molecular property prediction Day 3 : Graph neural networks for molecular structures Day 4 : Generative models for molecular design Day 5 : Advanced applications and deployment","title":"What&rsquo;s Next?"},{"location":"day0/#recommended__practice","text":"Before Day 1, try: 1. Implement the solubility prediction exercise above 2. Experiment with different models and hyperparameters 3. Try other sklearn datasets (boston housing, wine quality) 4. Read sklearn documentation on your favorite algorithms","title":"Recommended Practice"},{"location":"day0/#12__additional__resources","text":"","title":"12. Additional Resources"},{"location":"day0/#books","text":"\u201cHands-On Machine Learning\u201d - Aur\u00e9lien G\u00e9ron \u201cPattern Recognition and Machine Learning\u201d - Christopher Bishop \u201cDeep Learning\u201d - Goodfellow, Bengio, and Courville","title":"Books"},{"location":"day0/#online__courses","text":"Andrew Ng\u2019s Machine Learning (Coursera) Fast.ai Practical Deep Learning Stanford CS229 (Machine Learning)","title":"Online Courses"},{"location":"day0/#documentation","text":"Scikit-learn: https://scikit-learn.org/ PyTorch: https://pytorch.org/ TensorFlow: https://tensorflow.org/","title":"Documentation"},{"location":"day0/#practice__platforms","text":"Kaggle: https://kaggle.com/ Google Colab: https://colab.research.google.com/ Papers with Code: https://paperswithcode.com/","title":"Practice Platforms"},{"location":"day0/#homework__assignment","text":"Complete the following before Day 1: Implement a basic ML pipeline : Use the solubility prediction example or choose your own dataset Compare 3 models : Try Random Forest, SVR, and a neural network Perform hyperparameter tuning : Use Grid Search or Random Search Analyze results : Create visualizations and interpret feature importance Document your findings : What worked? What didn\u2019t? Why?","title":"Homework Assignment"},{"location":"day0/#bonus__challenges","text":"Implement k-fold cross-validation from scratch Build a simple neural network using only NumPy Create a function to detect and handle data leakage Visualize the decision boundary of a classifier","title":"Bonus Challenges"},{"location":"day1/","text":"Day 1: Foundations of Machine Learning for Molecular Systems \u00b6 Course Overview \u00b6 Welcome to the Machine Learning and Deep Learning for Biomolecular Systems and Material Science course. This intensive 5-day program will equip you with the knowledge and skills to apply cutting-edge ML techniques to molecular design, property prediction, and materials discovery. Learning Objectives \u00b6 By the end of Day 1, you will: - Understand the fundamental ML concepts relevant to molecular sciences - Learn multiple molecular representation methods and their trade-offs - Implement basic ML models for molecular property prediction - Work with chemical databases and molecular descriptors - Understand the unique challenges of applying ML to chemistry 1. Introduction to ML in Molecular Sciences \u00b6 1.1 Why Machine Learning for Molecules? \u00b6 The chemical space is vast\u2014estimates suggest there are 10^60 possible drug-like molecules, far more than atoms in the universe. Traditional approaches to drug discovery and materials design involve: - Synthesizing and testing compounds one by one (expensive, slow) - Running quantum mechanical calculations for each molecule (computationally expensive) - Trial-and-error experimentation (low success rate) Machine learning has revolutionized our ability to: Predict Properties Without Experiments - Calculate solubility, toxicity, binding affinity computationally - Screen millions of compounds in silico before synthesis - Reduce time from years to weeks Discover Structure-Property Relationships - Identify which molecular features drive desired properties - Understand mechanisms of action - Transfer knowledge across molecular families Navigate Chemical Space Efficiently - Explore 10^60 possible molecules intelligently - Focus experimental resources on most promising candidates - Find novel scaffolds outside known chemistry Accelerate Discovery Pipelines - Traditional drug discovery: 10-15 years, $2.6B per drug - ML-assisted discovery: Potentially 2-3x faster, significantly cheaper - Example: Insilico Medicine designed a novel drug candidate in 46 days 1.2 Success Stories \u00b6 COVID-19 Drug Repurposing - ML models screened 6,000+ FDA-approved drugs against SARS-CoV-2 - Identified Baricitinib (arthritis drug) as potential treatment - Approved by FDA for COVID-19 treatment in 2020 Antibiotic Discovery - ML identified Halicin, a novel antibiotic - Effective against drug-resistant bacteria - Different from existing antibiotics (discovered through ML, not traditional chemistry) Materials Science - ML accelerated discovery of solid electrolytes for batteries - Predicted thermal conductivity of materials 1000x faster than simulations - Identified new photovoltaic materials 1.3 Key Challenges in Molecular ML \u00b6 High Dimensionality \u00b6 Molecules exist in complex, high-dimensional spaces: - 3D coordinates for each atom - Electronic structure information - Conformational flexibility - Quantum mechanical properties Solution : Learn compact representations that capture essential features Data Scarcity \u00b6 Unlike computer vision (millions of labeled images), molecular datasets are small: - Typical drug dataset: 1,000 - 100,000 compounds - Experimental measurements are expensive and time-consuming - Many properties are difficult to measure accurately Solution : Transfer learning, data augmentation, semi-supervised learning Physical Constraints \u00b6 Models must respect fundamental laws: - Conservation of energy - Valence rules (atoms have specific bonding patterns) - Symmetries (rotation, translation, permutation) - Quantum mechanical principles Solution : Physics-informed neural networks, equivariant architectures Interpretability Requirements \u00b6 Black-box predictions aren\u2019t enough in science: - Need to understand WHY predictions work - Identify key molecular features - Generate hypotheses for experiments - Build trust with domain experts Solution : Attention mechanisms, feature importance analysis, explainable AI Distribution Shift \u00b6 Models trained on one chemical space may fail on another: - Different molecular scaffolds - Novel functional groups - Extreme property values Solution : Domain adaptation, uncertainty quantification, active learning 2. Molecular Representations \u00b6 The choice of molecular representation is crucial\u2014it determines what information is available to the model and how efficiently it can learn. 2.1 SMILES (Simplified Molecular Input Line Entry System) \u00b6 SMILES is a text-based notation that represents molecular structure as a string. Basic SMILES Syntax \u00b6 Simple Molecules : Methane: C Ethanol: CCO Benzene: c1ccccc1 (lowercase = aromatic) Water: O Branches : Isobutane: CC(C)C \u2514\u2500 branch in parentheses Double and Triple Bonds : Ethene: C=C Ethyne: C#C CO2: O=C=O Rings : Cyclohexane: C1CCCCC1 \u2514\u2500 matching numbers close ring Naphthalene: c1ccc2ccccc2c1 \u2514\u2500 fused rings Stereochemistry : (R)-Alanine: N[C@@H](C)C(=O)O \u2514\u2500 @ indicates chirality Working with SMILES in Python \u00b6 from rdkit import Chem from rdkit.Chem import Draw import matplotlib.pyplot as plt # Create molecule from SMILES smiles = \"CC(=O)Oc1ccccc1C(=O)O\" # Aspirin mol = Chem . MolFromSmiles ( smiles ) # Check validity if mol is None : print ( \"Invalid SMILES!\" ) else : print ( f \"Valid molecule with { mol . GetNumAtoms () } atoms\" ) # Visualize img = Draw . MolToImage ( mol , size = ( 300 , 300 )) plt . imshow ( img ) plt . axis ( 'off' ) plt . title ( 'Aspirin' ) #plt.show() plt . savefig ( 'aspirine.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () # Get canonical SMILES (standardized form) canonical_smiles = Chem . MolToSmiles ( mol ) print ( f \"Canonical SMILES: { canonical_smiles } \" ) # Generate randomized SMILES (useful for data augmentation) for i in range ( 5 ): random_smiles = Chem . MolToSmiles ( mol , doRandom = True ) print ( f \"Random SMILES { i + 1 } : { random_smiles } \" ) Advantages of SMILES \u00b6 Compact : Short strings for complex molecules Human-readable : Chemists can interpret them Widely used : Most databases provide SMILES Easy to store : Plain text format Limitations of SMILES \u00b6 Not unique : Same molecule can have multiple SMILES representations # All represent ethanol: smiles_variants = [ \"CCO\" , \"OCC\" , \"C(O)C\" ] # Solution: Use canonical SMILES No 3D information : Only connectivity, not geometry # Both are C3H8O but different 3D shapes: propanol = \"CCCO\" # Linear isopropanol = \"CC(O)C\" # Branched Sequence-based : Hard to capture graph structure directly Fragile : Single character error invalidates entire SMILES valid = \"CCO\" invalid = \"C CO\" # Space breaks it 2.2 SELFIES (Self-Referencing Embedded Strings) \u00b6 SELFIES is an alternative to SMILES that guarantees 100% valid molecules. import selfies as sf from rdkit import Chem # Convert SMILES to SELFIES smiles = \"CCO\" selfies_str = sf . encoder ( smiles ) print ( f \"SMILES: { smiles } \" ) print ( f \"SELFIES: { selfies_str } \" ) # Convert SELFIES back to SMILES smiles_back = sf . decoder ( selfies_str ) print ( f \"Back to SMILES: { smiles_back } \" ) # Verify it's a valid molecule mol = Chem . MolFromSmiles ( smiles_back ) print ( f \"Valid molecule: { mol is not None } \" ) 2.2 Molecular Fingerprints \u00b6 Fingerprints are fixed-length binary or count vectors that encode molecular structure. Morgan Fingerprints (ECFP - Extended Connectivity Fingerprints) \u00b6 Morgan fingerprints capture circular neighborhoods around each atom. Algorithm : 1. Initialize each atom with a unique identifier based on properties 2. For each radius (0, 1, 2, \u2026), update atom identifiers based on neighbors 3. Hash identifiers to fixed-length bit vector from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator import numpy as np import warnings # Create molecule mol = Chem . MolFromSmiles ( \"CCO\" ) # NEW METHOD: Using MorganGenerator morgan_gen = GetMorganGenerator ( radius = 2 , fpSize = 2048 ) # Generate fingerprint as bit vector morgan_fp = morgan_gen . GetFingerprint ( mol ) # Convert to numpy array fp_array = np . zeros (( 2048 ,)) from rdkit import DataStructs DataStructs . ConvertToNumpyArray ( morgan_fp , fp_array ) print ( f \"Fingerprint shape: { fp_array . shape } \" ) print ( f \"Number of set bits: { fp_array . sum () } \" ) # Morgan fingerprint with counts (count version) morgan_gen_count = GetMorganGenerator ( radius = 2 , fpSize = 2048 , countSimulation = True ) morgan_count = morgan_gen_count . GetFingerprint ( mol ) print ( f \"Count fingerprint generated\" ) Visualization of Circular Neighborhoods : # Visualize which atoms contribute to which bits from rdkit.Chem import Draw #info = {} #fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048, bitInfo=info) info = {} fp_old = AllChem . GetMorganFingerprintAsBitVect ( mol , radius = 2 , nBits = 2048 , bitInfo = info ) # Show atom environments for specific bits for bit_id in list ( info . keys ())[: 5 ]: # First 5 bits print ( f \"Bit { bit_id } : { info [ bit_id ] } \" ) # atom_ids = [atom_id for atom_id, radius in info[bit_id]] # img = Draw.DrawMorganBit(mol, bit_id, info) # Uncomment if you want to visualize Parameters : - Radius : Larger radius captures more context - Radius 1 (ECFP2): Immediate neighbors - Radius 2 (ECFP4): Common choice, balances local and broader context - Radius 3 (ECFP6): Larger substructures nBits : Fingerprint length 1024: Fast, but more collisions 2048: Common default 4096: More unique features, slower MACCS Keys \u00b6 166 predefined structural keys based on common molecular features. from rdkit import Chem from rdkit.Chem import MACCSkeys import numpy as np maccs = MACCSkeys . GenMACCSKeys ( mol ) print ( f \"MACCS keys length: { len ( maccs ) } \" ) # Each bit represents specific structural feature: # Bit 1: Contains isotope # Bit 44: C-O bond # Bit 79: Aromatic ring # etc. # Convert to numpy maccs_array = np . array ( list ( maccs . ToBitString ()), dtype = int ) Advantages : - Interpretable: Each bit has defined meaning - Compact: Only 166 bits - Good for similarity searching Limitations : - Fixed features: Can\u2019t capture novel patterns - Less flexible than Morgan fingerprints RDKit Fingerprints \u00b6 Topological fingerprints based on molecular paths. from rdkit import Chem from rdkit.Chem import RDKFingerprint rdkit_fp = RDKFingerprint ( mol , fpSize = 2048 , maxPath = 7 ) # maxPath: maximum path length to consider Atom Pair and Topological Torsion Fingerprints \u00b6 Encode distances between atom pairs or torsion angles. from rdkit.Chem import rdMolDescriptors # Atom pairs atom_pairs = rdMolDescriptors . GetHashedAtomPairFingerprintAsBitVect ( mol , nBits = 2048 ) # Topological torsions (4 consecutive atoms) torsions = rdMolDescriptors . GetHashedTopologicalTorsionFingerprintAsBitVect ( mol , nBits = 2048 ) Comparing Molecules with Fingerprints \u00b6 from rdkit import DataStructs mol1 = Chem . MolFromSmiles ( \"CCO\" ) mol2 = Chem . MolFromSmiles ( \"CCCO\" ) mol3 = Chem . MolFromSmiles ( \"c1ccccc1\" ) fp1 = AllChem . GetMorganFingerprintAsBitVect ( mol1 , 2 , 2048 ) fp2 = AllChem . GetMorganFingerprintAsBitVect ( mol2 , 2 , 2048 ) fp3 = AllChem . GetMorganFingerprintAsBitVect ( mol3 , 2 , 2048 ) # Tanimoto similarity (Jaccard index for binary vectors) sim_12 = DataStructs . TanimotoSimilarity ( fp1 , fp2 ) sim_13 = DataStructs . TanimotoSimilarity ( fp1 , fp3 ) print ( f \"Similarity(ethanol, propanol): { sim_12 : .3f } \" ) # High (similar structures) print ( f \"Similarity(ethanol, benzene): { sim_13 : .3f } \" ) # Low (different structures) # Other similarity metrics dice = DataStructs . DiceSimilarity ( fp1 , fp2 ) cosine = DataStructs . CosineSimilarity ( fp1 , fp2 ) 2.3 Molecular Descriptors \u00b6 Numerical features that capture molecular properties. Types of Descriptors \u00b6 1. Physical Descriptors from rdkit.Chem import Descriptors , Crippen mol = Chem . MolFromSmiles ( \"CC(=O)Oc1ccccc1C(=O)O\" ) # Aspirin # Molecular weight mw = Descriptors . MolWt ( mol ) print ( f \"Molecular Weight: { mw : .2f } g/mol\" ) # Lipophilicity (logP - octanol/water partition coefficient) logp = Crippen . MolLogP ( mol ) print ( f \"LogP: { logp : .2f } \" ) # LogP > 5: Too lipophilic (Lipinski's Rule of Five) # Polar Surface Area tpsa = Descriptors . TPSA ( mol ) print ( f \"TPSA: { tpsa : .2f } \u0172\" ) # TPSA < 140: Likely to cross blood-brain barrier # Molar Refractivity mr = Crippen . MolMR ( mol ) print ( f \"Molar Refractivity: { mr : .2f } \" ) 2. Structural Descriptors # Hydrogen bond donors and acceptors h_donors = Descriptors . NumHDonors ( mol ) h_acceptors = Descriptors . NumHAcceptors ( mol ) print ( f \"H-Bond Donors: { h_donors } \" ) print ( f \"H-Bond Acceptors: { h_acceptors } \" ) # Rotatable bonds (flexibility) rot_bonds = Descriptors . NumRotatableBonds ( mol ) print ( f \"Rotatable Bonds: { rot_bonds } \" ) # Ring information num_rings = Descriptors . RingCount ( mol ) aromatic_rings = Descriptors . NumAromaticRings ( mol ) print ( f \"Total Rings: { num_rings } , Aromatic: { aromatic_rings } \" ) # Fraction of sp3 carbons (saturation) frac_sp3 = Descriptors . FractionCSP3 ( mol ) print ( f \"Fraction Csp3: { frac_sp3 : .2f } \" ) 3. Topological Descriptors from rdkit.Chem import GraphDescriptors # Balaban J index (molecular branching) balaban = GraphDescriptors . BalabanJ ( mol ) # Bertz complexity index bertz = GraphDescriptors . BertzCT ( mol ) # Chi indices (connectivity) chi0 = GraphDescriptors . Chi0 ( mol ) chi1 = GraphDescriptors . Chi1 ( mol ) 4. 3D Descriptors from rdkit.Chem import AllChem , Descriptors3D # Generate 3D coordinates mol_3d = Chem . AddHs ( mol ) AllChem . EmbedMolecule ( mol_3d , randomSeed = 42 ) AllChem . MMFFOptimizeMolecule ( mol_3d ) # 3D descriptors asphericity = Descriptors3D . Asphericity ( mol_3d ) eccentricity = Descriptors3D . Eccentricity ( mol_3d ) inertial_shape = Descriptors3D . InertialShapeFactor ( mol_3d ) radius_of_gyration = Descriptors3D . RadiusOfGyration ( mol_3d ) print ( f \"Radius of Gyration: { radius_of_gyration : .2f } \u0172\" ) Drug-Likeness Metrics \u00b6 Lipinski\u2019s Rule of Five def lipinski_rule_of_five ( mol ): \"\"\" Predicts if molecule is drug-like Rules: - MW <= 500 - LogP <= 5 - H-bond donors <= 5 - H-bond acceptors <= 10 \"\"\" mw = Descriptors . MolWt ( mol ) logp = Crippen . MolLogP ( mol ) hbd = Descriptors . NumHDonors ( mol ) hba = Descriptors . NumHAcceptors ( mol ) violations = 0 if mw > 500 : violations += 1 if logp > 5 : violations += 1 if hbd > 5 : violations += 1 if hba > 10 : violations += 1 return violations <= 1 # Allow 1 violation is_druglike = lipinski_rule_of_five ( mol ) print ( f \"Passes Lipinski's Rule: { is_druglike } \" ) QED (Quantitative Estimate of Drug-likeness) from rdkit.Chem import QED qed_score = QED . qed ( mol ) print ( f \"QED Score: { qed_score : .3f } \" ) # Range: [0, 1], higher is more drug-like # Based on 8 molecular properties Synthetic Accessibility Score from rdkit.Chem import RDConfig import sys sys . path . append ( f ' { RDConfig . RDContribDir } /SA_Score' ) import sascorer sa_score = sascorer . calculateScore ( mol ) print ( f \"SA Score: { sa_score : .2f } \" ) # Range: [1, 10] # 1: Easy to synthesize # 10: Difficult to synthesize Creating Feature Vectors \u00b6 def calculate_molecular_descriptors ( smiles ): \"\"\" Comprehensive descriptor calculation \"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None # Add hydrogens for accurate calculations mol = Chem . AddHs ( mol ) descriptors = { # Physical 'MW' : Descriptors . MolWt ( mol ), 'LogP' : Crippen . MolLogP ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'MolMR' : Crippen . MolMR ( mol ), # Structural 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumHeteroatoms' : Descriptors . NumHeteroatoms ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'NumSaturatedRings' : Descriptors . NumSaturatedRings ( mol ), 'NumAliphaticRings' : Descriptors . NumAliphaticRings ( mol ), 'RingCount' : Descriptors . RingCount ( mol ), # Complexity 'BertzCT' : GraphDescriptors . BertzCT ( mol ), 'NumBridgeheadAtoms' : Descriptors . NumBridgeheadAtoms ( mol ), 'NumSpiroAtoms' : Descriptors . NumSpiroAtoms ( mol ), # Electronic 'LabuteASA' : Descriptors . LabuteASA ( mol ), 'PEOE_VSA1' : Descriptors . PEOE_VSA1 ( mol ), # Counts 'NumCarbon' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 6 ]), 'NumNitrogen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 7 ]), 'NumOxygen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 8 ]), 'NumHalogens' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () in [ 9 , 17 , 35 , 53 ]]), # Saturation 'FractionCsp3' : Descriptors . FractionCSP3 ( mol ), # Drug-likeness 'QED' : QED . qed ( mol ), } return descriptors # Example usage smiles_list = [ \"CCO\" , \"CC(=O)Oc1ccccc1C(=O)O\" , \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\" ] import pandas as pd descriptor_list = [ calculate_molecular_descriptors ( s ) for s in smiles_list ] df_descriptors = pd . DataFrame ( descriptor_list ) df_descriptors [ 'SMILES' ] = smiles_list print ( df_descriptors ) 2.4 Graph Representations \u00b6 Molecules as graphs where atoms are nodes and bonds are edges. Graph Structure \u00b6 import networkx as nx def mol_to_graph ( smiles ): \"\"\"Convert molecule to NetworkX graph\"\"\" mol = Chem . MolFromSmiles ( smiles ) # Create graph G = nx . Graph () # Add nodes (atoms) for atom in mol . GetAtoms (): G . add_node ( atom . GetIdx (), atomic_num = atom . GetAtomicNum (), symbol = atom . GetSymbol (), degree = atom . GetDegree (), formal_charge = atom . GetFormalCharge (), num_h = atom . GetTotalNumHs (), hybridization = str ( atom . GetHybridization ()), is_aromatic = atom . GetIsAromatic () ) # Add edges (bonds) for bond in mol . GetBonds (): G . add_edge ( bond . GetBeginAtomIdx (), bond . GetEndAtomIdx (), bond_type = str ( bond . GetBondType ()), is_conjugated = bond . GetIsConjugated (), is_aromatic = bond . GetIsAromatic () ) return G # Example G = mol_to_graph ( \"CCO\" ) print ( f \"Nodes: { G . number_of_nodes () } \" ) print ( f \"Edges: { G . number_of_edges () } \" ) print ( f \"Node features: { G . nodes [ 0 ] } \" ) Adjacency Matrix Representation \u00b6 def get_adjacency_matrix ( smiles , max_atoms = 50 ): \"\"\"Get adjacency matrix with padding\"\"\" mol = Chem . MolFromSmiles ( smiles ) num_atoms = mol . GetNumAtoms () # Initialize matrix adj_matrix = np . zeros (( max_atoms , max_atoms )) # Fill adjacency matrix for bond in mol . GetBonds (): i = bond . GetBeginAtomIdx () j = bond . GetEndAtomIdx () adj_matrix [ i , j ] = 1 adj_matrix [ j , i ] = 1 # Symmetric return adj_matrix , num_atoms adj , n_atoms = get_adjacency_matrix ( \"CCO\" ) print ( f \"Adjacency matrix shape: { adj . shape } \" ) print ( f \"Actual atoms: { n_atoms } \" ) Node and Edge Features \u00b6 def get_node_features ( atom ): \"\"\"Extract features for a single atom\"\"\" return np . array ([ atom . GetAtomicNum (), # Atomic number atom . GetDegree (), # Number of bonds atom . GetFormalCharge (), # Charge atom . GetNumRadicalElectrons (), # Radicals atom . GetHybridization () . real , # sp, sp2, sp3 atom . GetIsAromatic (), # Aromaticity atom . GetTotalNumHs (), # Hydrogens ]) def get_edge_features ( bond ): \"\"\"Extract features for a single bond\"\"\" bond_type_map = { Chem . rdchem . BondType . SINGLE : 1 , Chem . rdchem . BondType . DOUBLE : 2 , Chem . rdchem . BondType . TRIPLE : 3 , Chem . rdchem . BondType . AROMATIC : 4 , } return np . array ([ bond_type_map . get ( bond . GetBondType (), 0 ), bond . GetIsConjugated (), bond . GetIsAromatic (), ]) Advantages of Graph Representations : - Natural for molecules (atoms connected by bonds) - Permutation invariant (atom order doesn\u2019t matter) - Captures topology and local structure - Enables Graph Neural Networks (Day 3) Limitations : - More complex to implement - Computationally expensive for large molecules - Requires specialized neural network architectures Hasta aca 3. Traditional Machine Learning Methods \u00b6 Before deep learning, these methods were (and still are) workhorses of molecular ML. 3.1 Feature Engineering Principles \u00b6 Domain Knowledge is Key : - Choose descriptors relevant to property being predicted - For solubility: polarity, surface area, H-bond capacity - For toxicity: reactive functional groups, lipophilicity Feature Scaling : from sklearn.preprocessing import StandardScaler , MinMaxScaler , RobustScaler # Standardization (zero mean, unit variance) scaler = StandardScaler () X_scaled = scaler . fit_transform ( X ) # Good for: Most ML algorithms, assumes normal distribution # Min-Max scaling (range [0, 1]) scaler = MinMaxScaler () X_scaled = scaler . fit_transform ( X ) # Good for: Neural networks, when you need bounded values # Robust scaling (uses median and IQR) scaler = RobustScaler () X_scaled = scaler . fit_transform ( X ) # Good for: Data with outliers Feature Selection : from sklearn.feature_selection import ( VarianceThreshold , SelectKBest , f_regression , RFE , SelectFromModel ) from sklearn.ensemble import RandomForestRegressor # Remove low-variance features selector = VarianceThreshold ( threshold = 0.01 ) X_high_var = selector . fit_transform ( X ) # Univariate feature selection selector = SelectKBest ( score_func = f_regression , k = 10 ) X_selected = selector . fit_transform ( X , y ) # Recursive Feature Elimination model = RandomForestRegressor () rfe = RFE ( model , n_features_to_select = 10 ) X_rfe = rfe . fit_transform ( X , y ) # Feature importance from model model = RandomForestRegressor () model . fit ( X , y ) selector = SelectFromModel ( model , prefit = True , threshold = 'median' ) X_important = selector . transform ( X ) 3.2 Random Forests \u00b6 Ensemble of decision trees, each trained on random subset of data and features. How it Works \u00b6 Bootstrap Sampling : Create N random subsets of training data (with replacement) Random Feature Selection : At each split, consider random subset of features Tree Building : Build deep trees without pruning Prediction : Average predictions from all trees (regression) or vote (classification) from sklearn.ensemble import RandomForestRegressor , RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np # Regression example rf_reg = RandomForestRegressor ( n_estimators = 100 , # Number of trees max_depth = None , # Grow trees fully min_samples_split = 2 , # Minimum samples to split node min_samples_leaf = 1 , # Minimum samples in leaf max_features = 'sqrt' , # Features to consider at each split bootstrap = True , # Use bootstrap sampling random_state = 42 , n_jobs =- 1 # Use all CPUs ) # Train rf_reg . fit ( X_train , y_train ) # Predict y_pred = rf_reg . predict ( X_test ) # Cross-validation cv_scores = cross_val_score ( rf_reg , X , y , cv = 5 , scoring = 'r2' ) print ( f \"Cross-validation R\u00b2: { cv_scores . mean () : .3f } \u00b1 { cv_scores . std () : .3f } \" ) # Feature importance importances = rf_reg . feature_importances_ indices = np . argsort ( importances )[:: - 1 ] print ( \"Feature ranking:\" ) for i , idx in enumerate ( indices [: 10 ]): print ( f \" { i + 1 } . Feature { idx } : { importances [ idx ] : .4f } \" ) # Visualize feature importance import matplotlib.pyplot as plt plt . figure ( figsize = ( 10 , 6 )) plt . bar ( range ( len ( importances )), importances [ indices ]) plt . xlabel ( 'Features' ) plt . ylabel ( 'Importance' ) plt . title ( 'Random Forest Feature Importance' ) plt . tight_layout () plt . show () Advantages : - Handles non-linear relationships - Robust to outliers - Provides feature importance - Little hyperparameter tuning needed - Resistant to overfitting (with enough trees) Limitations : - Can be slow for very large datasets - Not great for extrapolation - Black-box model (hard to interpret individual predictions) Hyperparameter Tuning : from sklearn.model_selection import RandomizedSearchCV param_dist = { 'n_estimators' : [ 100 , 200 , 500 , 1000 ], 'max_depth' : [ 10 , 20 , 30 , None ], 'min_samples_split' : [ 2 , 5 , 10 ], 'min_samples_leaf' : [ 1 , 2 , 4 ], 'max_features' : [ 'sqrt' , 'log2' , 0.5 ] } random_search = RandomizedSearchCV ( RandomForestRegressor ( random_state = 42 ), param_distributions = param_dist , n_iter = 50 , cv = 5 , scoring = 'r2' , random_state = 42 , n_jobs =- 1 ) random_search . fit ( X_train , y_train ) print ( f \"Best parameters: { random_search . best_params_ } \" ) print ( f \"Best score: { random_search . best_score_ : .3f } \" ) 3.3 Model Evaluation Metrics \u00b6 Regression Metrics \u00b6 from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score # Mean Absolute Error mae = mean_absolute_error ( y_true , y_pred ) print ( f \"MAE: { mae : .3f } \" ) # Root Mean Squared Error rmse = np . sqrt ( mean_squared_error ( y_true , y_pred )) print ( f \"RMSE: { rmse : .3f } \" ) # R\u00b2 Score r2 = r2_score ( y_true , y_pred ) print ( f \"R\u00b2: { r2 : .3f } \" ) 4. Working with Chemical Databases \u00b6 4.1 Public Databases \u00b6 PubChem \u00b6 import pubchempy as pcp # Search by name results = pcp . get_compounds ( 'aspirin' , 'name' ) compound = results [ 0 ] print ( f \"IUPAC Name: { compound . iupac_name } \" ) print ( f \"SMILES: { compound . isomeric_smiles } \" ) print ( f \"Molecular Formula: { compound . molecular_formula } \" ) print ( f \"Molecular Weight: { compound . molecular_weight } \" ) # Get properties properties = pcp . get_properties ( [ 'MolecularWeight' , 'XLogP' , 'TPSA' , 'Complexity' ], 'aspirin' , 'name' ) print ( properties ) ChEMBL \u00b6 from chembl_webresource_client.new_client import new_client # Target search target = new_client . target targets = target . filter ( target_synonym__icontains = 'EGFR' ) for t in targets [: 5 ]: print ( f \" { t [ 'pref_name' ] } : { t [ 'target_chembl_id' ] } \" ) # Activity search activity = new_client . activity activities = activity . filter ( target_chembl_id = 'CHEMBL203' , standard_type = 'IC50' , standard_relation = '=' , pchembl_value__isnull = False ) # Convert to DataFrame import pandas as pd data = [] for act in activities [: 1000 ]: data . append ({ 'molecule_chembl_id' : act [ 'molecule_chembl_id' ], 'smiles' : act [ 'canonical_smiles' ], 'ic50' : act [ 'standard_value' ], 'pchembl_value' : act [ 'pchembl_value' ] }) df = pd . DataFrame ( data ) print ( df . head ()) 4.2 Data Preprocessing \u00b6 Molecular Standardization \u00b6 from rdkit.Chem import SaltRemover def standardize_molecule ( smiles ): \"\"\"Standardize molecular representation\"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None # Remove salts remover = SaltRemover . SaltRemover () mol = remover . StripMol ( mol ) # Get canonical SMILES canonical_smiles = Chem . MolToSmiles ( mol ) return canonical_smiles # Process dataset smiles_list = [ \"CC(=O)O.Na\" , \"CCO\" , \"[NH3+]CC[O-]\" ] standardized = [ standardize_molecule ( s ) for s in smiles_list ] print ( standardized ) Train/Test Splitting \u00b6 from sklearn.model_selection import train_test_split # Random split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Scaffold split for molecules from rdkit.Chem.Scaffolds import MurckoScaffold from collections import defaultdict def scaffold_split ( smiles_list , test_size = 0.2 ): \"\"\"Split by molecular scaffolds\"\"\" scaffolds = defaultdict ( list ) for idx , smiles in enumerate ( smiles_list ): mol = Chem . MolFromSmiles ( smiles ) scaffold = MurckoScaffold . MurckoScaffoldSmiles ( mol = mol , includeChirality = False ) scaffolds [ scaffold ] . append ( idx ) # Distribute scaffolds scaffold_sets = sorted ( list ( scaffolds . values ()), key = len , reverse = True ) n_total = len ( smiles_list ) n_test = int ( n_total * test_size ) train_idx , test_idx = [], [] train_count = 0 for scaffold_set in scaffold_sets : if train_count + len ( scaffold_set ) <= n_total - n_test : train_idx . extend ( scaffold_set ) train_count += len ( scaffold_set ) else : test_idx . extend ( scaffold_set ) return train_idx , test_idx 5. Practical Exercise: Solubility Prediction \u00b6 Complete Workflow \u00b6 import pandas as pd import numpy as np from rdkit import Chem from rdkit.Chem import Descriptors , AllChem from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error import matplotlib.pyplot as plt # Step 1: Load Data url = \"https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/delaney-processed.csv\" df = pd . read_csv ( url ) print ( f \"Dataset size: { len ( df ) } \" ) # Step 2: Calculate Descriptors def calculate_descriptors ( smiles ): mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None return { 'MW' : Descriptors . MolWt ( mol ), 'LogP' : Descriptors . MolLogP ( mol ), 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'NumRings' : Descriptors . RingCount ( mol ), 'MolMR' : Descriptors . MolMR ( mol ), 'FractionCsp3' : Descriptors . FractionCsp3 ( mol ) } desc_list = [ calculate_descriptors ( s ) for s in df [ 'smiles' ]] X_desc = pd . DataFrame ([ d for d in desc_list if d is not None ]) y = df [ 'measured log solubility in mols per litre' ] . values [: len ( X_desc )] # Step 3: Train-Test Split X_train , X_test , y_train , y_test = train_test_split ( X_desc , y , test_size = 0.2 , random_state = 42 ) # Step 4: Scale Features scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # Step 5: Train Model model = RandomForestRegressor ( n_estimators = 100 , random_state = 42 ) model . fit ( X_train_scaled , y_train ) # Step 6: Evaluate y_pred = model . predict ( X_test_scaled ) rmse = np . sqrt ( mean_squared_error ( y_test , y_pred )) mae = mean_absolute_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) print ( f \" \\n Results:\" ) print ( f \"RMSE: { rmse : .3f } \" ) print ( f \"MAE: { mae : .3f } \" ) print ( f \"R\u00b2: { r2 : .3f } \" ) # Step 7: Visualize plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_test , y_pred , alpha = 0.5 ) plt . plot ([ y_test . min (), y_test . max ()], [ y_test . min (), y_test . max ()], 'r--' , lw = 2 ) plt . xlabel ( 'True Solubility (log M)' ) plt . ylabel ( 'Predicted Solubility (log M)' ) plt . title ( f 'Predictions (R\u00b2 = { r2 : .3f } )' ) plt . axis ( 'equal' ) plt . tight_layout () plt . show () # Step 8: Feature Importance importances = model . feature_importances_ feature_importance_df = pd . DataFrame ({ 'feature' : X_desc . columns , 'importance' : importances }) . sort_values ( 'importance' , ascending = False ) print ( \" \\n Top 5 Most Important Features:\" ) print ( feature_importance_df . head ()) Expected Results : - Test RMSE: ~0.7-0.9 log units - R\u00b2: ~0.75-0.85 - Key features: LogP, molecular weight, polar surface area 6. Example with SELFIES \u00b6 import numpy as np import selfies as sf from rdkit import Chem from rdkit.Chem import Descriptors , Crippen # Convert SELFIES to molecule selfies_str = sf . encoder ( \"CC(=O)Oc1ccccc1C(=O)O\" ) # Aspirin in SELFIES print ( f \"SELFIES representation: { selfies_str } \" ) smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) # Molecular weight mw = Descriptors . MolWt ( mol ) print ( f \"Molecular Weight: { mw : .2f } g/mol\" ) # Lipophilicity (logP - octanol/water partition coefficient) logp = Crippen . MolLogP ( mol ) print ( f \"LogP: { logp : .2f } \" ) # LogP > 5: Too lipophilic (Lipinski's Rule of Five) # Polar Surface Area tpsa = Descriptors . TPSA ( mol ) print ( f \"TPSA: { tpsa : .2f } \u0172\" ) # TPSA < 140: Likely to cross blood-brain barrier # Molar Refractivity mr = Crippen . MolMR ( mol ) print ( f \"Molar Refractivity: { mr : .2f } \" ) #### # Hydrogen bond donors and acceptors h_donors = Descriptors . NumHDonors ( mol ) h_acceptors = Descriptors . NumHAcceptors ( mol ) print ( f \"H-Bond Donors: { h_donors } \" ) print ( f \"H-Bond Acceptors: { h_acceptors } \" ) # Rotatable bonds (flexibility) rot_bonds = Descriptors . NumRotatableBonds ( mol ) print ( f \"Rotatable Bonds: { rot_bonds } \" ) # Ring information num_rings = Descriptors . RingCount ( mol ) aromatic_rings = Descriptors . NumAromaticRings ( mol ) print ( f \"Total Rings: { num_rings } , Aromatic: { aromatic_rings } \" ) # Fraction of sp3 carbons (saturation) frac_sp3 = Descriptors . FractionCSP3 ( mol ) print ( f \"Fraction Csp3: { frac_sp3 : .2f } \" ) #### from rdkit.Chem import GraphDescriptors # Balaban J index (molecular branching) balaban = GraphDescriptors . BalabanJ ( mol ) # Bertz complexity index bertz = GraphDescriptors . BertzCT ( mol ) # Chi indices (connectivity) chi0 = GraphDescriptors . Chi0 ( mol ) chi1 = GraphDescriptors . Chi1 ( mol ) #### from rdkit.Chem import AllChem , Descriptors3D # Generate 3D coordinates mol_3d = Chem . AddHs ( mol ) AllChem . EmbedMolecule ( mol_3d , randomSeed = 42 ) AllChem . MMFFOptimizeMolecule ( mol_3d ) # 3D descriptors asphericity = Descriptors3D . Asphericity ( mol_3d ) eccentricity = Descriptors3D . Eccentricity ( mol_3d ) inertial_shape = Descriptors3D . InertialShapeFactor ( mol_3d ) radius_of_gyration = Descriptors3D . RadiusOfGyration ( mol_3d ) print ( f \"Radius of Gyration: { radius_of_gyration : .2f } \u0172\" ) #### def lipinski_rule_of_five ( mol ): \"\"\" Predicts if molecule is drug-like Rules: - MW <= 500 - LogP <= 5 - H-bond donors <= 5 - H-bond acceptors <= 10 \"\"\" mw = Descriptors . MolWt ( mol ) logp = Crippen . MolLogP ( mol ) hbd = Descriptors . NumHDonors ( mol ) hba = Descriptors . NumHAcceptors ( mol ) violations = 0 if mw > 500 : violations += 1 if logp > 5 : violations += 1 if hbd > 5 : violations += 1 if hba > 10 : violations += 1 return violations <= 1 # Allow 1 violation is_druglike = lipinski_rule_of_five ( mol ) print ( f \"Passes Lipinski's Rule: { is_druglike } \" ) #### from rdkit.Chem import QED qed_score = QED . qed ( mol ) print ( f \"QED Score: { qed_score : .3f } \" ) # Range: [0, 1], higher is more drug-like # Based on 8 molecular properties #### from rdkit.Chem import RDConfig import sys sys . path . append ( f ' { RDConfig . RDContribDir } /SA_Score' ) import sascorer sa_score = sascorer . calculateScore ( mol ) print ( f \"SA Score: { sa_score : .2f } \" ) # Range: [1, 10] # 1: Easy to synthesize # 10: Difficult to synthesize #### def calculate_molecular_descriptors ( selfies_str ): \"\"\" Comprehensive descriptor calculation from SELFIES \"\"\" # Convert SELFIES to SMILES then to molecule smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None # Add hydrogens for accurate calculations mol = Chem . AddHs ( mol ) descriptors = { # Physical 'MW' : Descriptors . MolWt ( mol ), 'LogP' : Crippen . MolLogP ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'MolMR' : Crippen . MolMR ( mol ), # Structural 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumHeteroatoms' : Descriptors . NumHeteroatoms ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'NumSaturatedRings' : Descriptors . NumSaturatedRings ( mol ), 'NumAliphaticRings' : Descriptors . NumAliphaticRings ( mol ), 'RingCount' : Descriptors . RingCount ( mol ), # Complexity 'BertzCT' : GraphDescriptors . BertzCT ( mol ), 'NumBridgeheadAtoms' : Descriptors . NumBridgeheadAtoms ( mol ), 'NumSpiroAtoms' : Descriptors . NumSpiroAtoms ( mol ), # Electronic 'LabuteASA' : Descriptors . LabuteASA ( mol ), 'PEOE_VSA1' : Descriptors . PEOE_VSA1 ( mol ), # Counts 'NumCarbon' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 6 ]), 'NumNitrogen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 7 ]), 'NumOxygen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 8 ]), 'NumHalogens' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () in [ 9 , 17 , 35 , 53 ]]), # Saturation 'FractionCsp3' : Descriptors . FractionCSP3 ( mol ), # Drug-likeness 'QED' : QED . qed ( mol ), } return descriptors # Example usage - Convert SMILES to SELFIES first smiles_list = [ \"CCO\" , \"CC(=O)Oc1ccccc1C(=O)O\" , \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\" ] selfies_list = [ sf . encoder ( s ) for s in smiles_list ] print ( \" \\n SELFIES representations:\" ) for smiles , selfies_str in zip ( smiles_list , selfies_list ): print ( f \"SMILES: { smiles } \" ) print ( f \"SELFIES: { selfies_str } \" ) print () import pandas as pd descriptor_list = [ calculate_molecular_descriptors ( s ) for s in selfies_list ] df_descriptors = pd . DataFrame ( descriptor_list ) df_descriptors [ 'SELFIES' ] = selfies_list df_descriptors [ 'SMILES' ] = smiles_list # Optional: keep original SMILES for reference print ( df_descriptors ) #### import networkx as nx def mol_to_graph ( selfies_str ): \"\"\"Convert SELFIES molecule to NetworkX graph\"\"\" smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) # Create graph G = nx . Graph () # Add nodes (atoms) for atom in mol . GetAtoms (): G . add_node ( atom . GetIdx (), atomic_num = atom . GetAtomicNum (), symbol = atom . GetSymbol (), degree = atom . GetDegree (), formal_charge = atom . GetFormalCharge (), num_h = atom . GetTotalNumHs (), hybridization = str ( atom . GetHybridization ()), is_aromatic = atom . GetIsAromatic () ) # Add edges (bonds) for bond in mol . GetBonds (): G . add_edge ( bond . GetBeginAtomIdx (), bond . GetEndAtomIdx (), bond_type = str ( bond . GetBondType ()), is_conjugated = bond . GetIsConjugated (), is_aromatic = bond . GetIsAromatic () ) return G # Example selfies_ethanol = sf . encoder ( \"CCO\" ) G = mol_to_graph ( selfies_ethanol ) print ( f \" \\n Graph from SELFIES: { selfies_ethanol } \" ) print ( f \"Nodes: { G . number_of_nodes () } \" ) print ( f \"Edges: { G . number_of_edges () } \" ) print ( f \"Node features: { G . nodes [ 0 ] } \" ) #### def get_adjacency_matrix ( selfies_str , max_atoms = 50 ): \"\"\"Get adjacency matrix with padding from SELFIES\"\"\" smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) num_atoms = mol . GetNumAtoms () # Initialize matrix adj_matrix = np . zeros (( max_atoms , max_atoms )) # Fill adjacency matrix for bond in mol . GetBonds (): i = bond . GetBeginAtomIdx () j = bond . GetEndAtomIdx () adj_matrix [ i , j ] = 1 adj_matrix [ j , i ] = 1 # Symmetric return adj_matrix , num_atoms selfies_ethanol = sf . encoder ( \"CCO\" ) adj , n_atoms = get_adjacency_matrix ( selfies_ethanol ) print ( f \" \\n Adjacency matrix from SELFIES: { selfies_ethanol } \" ) print ( f \"Adjacency matrix shape: { adj . shape } \" ) print ( f \"Actual atoms: { n_atoms } \" ) 7. Key Takeaways \u00b6 Molecular Representations \u00b6 SMILES : Compact text representation, requires careful handling Fingerprints : Fixed-length vectors, good for similarity and ML Descriptors : Interpretable features, require domain knowledge Graphs : Natural representation, enables GNNs (Day 3) Traditional ML Methods \u00b6 Random Forests : Robust baseline, handles non-linearity, provides feature importance SVMs : Effective in high dimensions, requires scaling Gaussian Processes : Provides uncertainty, excellent for active learning Gradient Boosting : Often best performance, requires careful tuning Best Practices \u00b6 Always validate molecular structures Use scaffold-based splits for realistic evaluation Scale features appropriately for each algorithm Compare multiple representations (descriptors vs fingerprints) Report multiple metrics (RMSE, MAE, R\u00b2) Analyze feature importance for insights Check for data leakage in preprocessing Common Pitfalls \u00b6 Using random splits instead of scaffold splits Forgetting to scale features for SVMs Not handling invalid SMILES Overfitting due to small datasets Ignoring uncertainty in predictions 8. Resources and Further Reading \u00b6 Software Libraries \u00b6 RDKit : Cheminformatics toolkit - https://www.rdkit.org/ scikit-learn : Machine learning library - https://scikit-learn.org/ Pandas : Data manipulation - https://pandas.pydata.org/ Matplotlib : Visualization Databases \u00b6 PubChem : https://pubchem.ncbi.nlm.nih.gov/ ChEMBL : https://www.ebi.ac.uk/chembl/ ZINC : https://zinc.docking.org/ Materials Project : https://materialsproject.org/ QM9 : http://quantum-machine.org/datasets/ Papers \u00b6 \u201cMolecular descriptors for chemoinformatics\u201d - Todeschini & Consonni \u201cMachine Learning in Materials Informatics\u201d - Butler et al., 2018 \u201cGuidelines for ML predictive models in biomedical research\u201d - Luo et al., 2016 \u201cDeep Learning for Molecular Design\u201d - Elton et al., 2019 Tutorials \u00b6 RDKit Cookbook - https://www.rdkit.org/docs/Cookbook.html Scikit-learn User Guide - https://scikit-learn.org/stable/user_guide.html DeepChem Tutorials - https://deepchem.io/ Homework Assignment \u00b6 Data Exploration Download QM9 dataset Calculate descriptors for 1000 random molecules Visualize descriptor distributions and correlations Classification Model Build a model to predict if a molecule has dipole moment > 2 Debye Use both descriptors and fingerprints Report precision, recall, and AUC Model Comparison Compare Random Forest, SVM, and Gradient Boosting Use 5-fold cross-validation Create visualizations comparing performance Feature Analysis Identify the top 5 most important features Explain why these features are relevant Visualize feature importance Advanced Challenge Implement scaffold-based splitting Compare performance with random split Discuss implications for model generalization Prepare Questions Review neural networks basics (Day 0) Think about limitations of traditional ML for molecules Prepare questions for Day 2 on deep learning Appendix: Quick Reference Tables \u00b6 Molecular Descriptor Ranges \u00b6 Descriptor Range Interpretation Drug-like Range Molecular Weight 0-\u221e Size 150-500 g/mol LogP -\u221e to +\u221e Lipophilicity 0-5 TPSA 0-\u221e \u0172 Polar surface 20-140 \u0172 H-Bond Donors 0-\u221e H-bond donors 0-5 H-Bond Acceptors 0-\u221e H-bond acceptors 0-10 Rotatable Bonds 0-\u221e Flexibility 0-10 QED 0-1 Drug-likeness > 0.5 Model Selection Guide \u00b6 Model Best For Pros Cons Random Forest General purpose Easy, robust, feature importance Slower prediction SVM Small datasets Good generalization Slow training, needs scaling Gaussian Process Active learning Uncertainty estimates Very slow for large data Gradient Boosting Best performance Highest accuracy Prone to overfitting Common RDKit Functions \u00b6 # Molecule creation mol = Chem . MolFromSmiles ( \"CCO\" ) # Validation is_valid = mol is not None # Canonical SMILES canonical = Chem . MolToSmiles ( mol ) # Add/remove hydrogens mol_h = Chem . AddHs ( mol ) mol_no_h = Chem . RemoveHs ( mol ) # 3D coordinates AllChem . EmbedMolecule ( mol ) # Descriptors mw = Descriptors . MolWt ( mol ) logp = Descriptors . MolLogP ( mol ) tpsa = Descriptors . TPSA ( mol ) # Fingerprints fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) # Similarity similarity = DataStructs . TanimotoSimilarity ( fp1 , fp2 ) # Substructure search pattern = Chem . MolFromSmarts ( \"[OH]\" ) has_match = mol . HasSubstructMatch ( pattern ) # Visualization img = Draw . MolToImage ( mol , size = ( 300 , 300 ))","title":"ML for Molecular Systems"},{"location":"day1/#day__1__foundations__of__machine__learning__for__molecular__systems","text":"","title":"Day 1: Foundations of Machine Learning for Molecular Systems"},{"location":"day1/#course__overview","text":"Welcome to the Machine Learning and Deep Learning for Biomolecular Systems and Material Science course. This intensive 5-day program will equip you with the knowledge and skills to apply cutting-edge ML techniques to molecular design, property prediction, and materials discovery.","title":"Course Overview"},{"location":"day1/#learning__objectives","text":"By the end of Day 1, you will: - Understand the fundamental ML concepts relevant to molecular sciences - Learn multiple molecular representation methods and their trade-offs - Implement basic ML models for molecular property prediction - Work with chemical databases and molecular descriptors - Understand the unique challenges of applying ML to chemistry","title":"Learning Objectives"},{"location":"day1/#1__introduction__to__ml__in__molecular__sciences","text":"","title":"1. Introduction to ML in Molecular Sciences"},{"location":"day1/#11__why__machine__learning__for__molecules","text":"The chemical space is vast\u2014estimates suggest there are 10^60 possible drug-like molecules, far more than atoms in the universe. Traditional approaches to drug discovery and materials design involve: - Synthesizing and testing compounds one by one (expensive, slow) - Running quantum mechanical calculations for each molecule (computationally expensive) - Trial-and-error experimentation (low success rate) Machine learning has revolutionized our ability to: Predict Properties Without Experiments - Calculate solubility, toxicity, binding affinity computationally - Screen millions of compounds in silico before synthesis - Reduce time from years to weeks Discover Structure-Property Relationships - Identify which molecular features drive desired properties - Understand mechanisms of action - Transfer knowledge across molecular families Navigate Chemical Space Efficiently - Explore 10^60 possible molecules intelligently - Focus experimental resources on most promising candidates - Find novel scaffolds outside known chemistry Accelerate Discovery Pipelines - Traditional drug discovery: 10-15 years, $2.6B per drug - ML-assisted discovery: Potentially 2-3x faster, significantly cheaper - Example: Insilico Medicine designed a novel drug candidate in 46 days","title":"1.1 Why Machine Learning for Molecules?"},{"location":"day1/#12__success__stories","text":"COVID-19 Drug Repurposing - ML models screened 6,000+ FDA-approved drugs against SARS-CoV-2 - Identified Baricitinib (arthritis drug) as potential treatment - Approved by FDA for COVID-19 treatment in 2020 Antibiotic Discovery - ML identified Halicin, a novel antibiotic - Effective against drug-resistant bacteria - Different from existing antibiotics (discovered through ML, not traditional chemistry) Materials Science - ML accelerated discovery of solid electrolytes for batteries - Predicted thermal conductivity of materials 1000x faster than simulations - Identified new photovoltaic materials","title":"1.2 Success Stories"},{"location":"day1/#13__key__challenges__in__molecular__ml","text":"","title":"1.3 Key Challenges in Molecular ML"},{"location":"day1/#high__dimensionality","text":"Molecules exist in complex, high-dimensional spaces: - 3D coordinates for each atom - Electronic structure information - Conformational flexibility - Quantum mechanical properties Solution : Learn compact representations that capture essential features","title":"High Dimensionality"},{"location":"day1/#data__scarcity","text":"Unlike computer vision (millions of labeled images), molecular datasets are small: - Typical drug dataset: 1,000 - 100,000 compounds - Experimental measurements are expensive and time-consuming - Many properties are difficult to measure accurately Solution : Transfer learning, data augmentation, semi-supervised learning","title":"Data Scarcity"},{"location":"day1/#physical__constraints","text":"Models must respect fundamental laws: - Conservation of energy - Valence rules (atoms have specific bonding patterns) - Symmetries (rotation, translation, permutation) - Quantum mechanical principles Solution : Physics-informed neural networks, equivariant architectures","title":"Physical Constraints"},{"location":"day1/#interpretability__requirements","text":"Black-box predictions aren\u2019t enough in science: - Need to understand WHY predictions work - Identify key molecular features - Generate hypotheses for experiments - Build trust with domain experts Solution : Attention mechanisms, feature importance analysis, explainable AI","title":"Interpretability Requirements"},{"location":"day1/#distribution__shift","text":"Models trained on one chemical space may fail on another: - Different molecular scaffolds - Novel functional groups - Extreme property values Solution : Domain adaptation, uncertainty quantification, active learning","title":"Distribution Shift"},{"location":"day1/#2__molecular__representations","text":"The choice of molecular representation is crucial\u2014it determines what information is available to the model and how efficiently it can learn.","title":"2. Molecular Representations"},{"location":"day1/#21__smiles__simplified__molecular__input__line__entry__system","text":"SMILES is a text-based notation that represents molecular structure as a string.","title":"2.1 SMILES (Simplified Molecular Input Line Entry System)"},{"location":"day1/#basic__smiles__syntax","text":"Simple Molecules : Methane: C Ethanol: CCO Benzene: c1ccccc1 (lowercase = aromatic) Water: O Branches : Isobutane: CC(C)C \u2514\u2500 branch in parentheses Double and Triple Bonds : Ethene: C=C Ethyne: C#C CO2: O=C=O Rings : Cyclohexane: C1CCCCC1 \u2514\u2500 matching numbers close ring Naphthalene: c1ccc2ccccc2c1 \u2514\u2500 fused rings Stereochemistry : (R)-Alanine: N[C@@H](C)C(=O)O \u2514\u2500 @ indicates chirality","title":"Basic SMILES Syntax"},{"location":"day1/#working__with__smiles__in__python","text":"from rdkit import Chem from rdkit.Chem import Draw import matplotlib.pyplot as plt # Create molecule from SMILES smiles = \"CC(=O)Oc1ccccc1C(=O)O\" # Aspirin mol = Chem . MolFromSmiles ( smiles ) # Check validity if mol is None : print ( \"Invalid SMILES!\" ) else : print ( f \"Valid molecule with { mol . GetNumAtoms () } atoms\" ) # Visualize img = Draw . MolToImage ( mol , size = ( 300 , 300 )) plt . imshow ( img ) plt . axis ( 'off' ) plt . title ( 'Aspirin' ) #plt.show() plt . savefig ( 'aspirine.png' , dpi = 300 , bbox_inches = 'tight' ) plt . close () # Get canonical SMILES (standardized form) canonical_smiles = Chem . MolToSmiles ( mol ) print ( f \"Canonical SMILES: { canonical_smiles } \" ) # Generate randomized SMILES (useful for data augmentation) for i in range ( 5 ): random_smiles = Chem . MolToSmiles ( mol , doRandom = True ) print ( f \"Random SMILES { i + 1 } : { random_smiles } \" )","title":"Working with SMILES in Python"},{"location":"day1/#advantages__of__smiles","text":"Compact : Short strings for complex molecules Human-readable : Chemists can interpret them Widely used : Most databases provide SMILES Easy to store : Plain text format","title":"Advantages of SMILES"},{"location":"day1/#limitations__of__smiles","text":"Not unique : Same molecule can have multiple SMILES representations # All represent ethanol: smiles_variants = [ \"CCO\" , \"OCC\" , \"C(O)C\" ] # Solution: Use canonical SMILES No 3D information : Only connectivity, not geometry # Both are C3H8O but different 3D shapes: propanol = \"CCCO\" # Linear isopropanol = \"CC(O)C\" # Branched Sequence-based : Hard to capture graph structure directly Fragile : Single character error invalidates entire SMILES valid = \"CCO\" invalid = \"C CO\" # Space breaks it","title":"Limitations of SMILES"},{"location":"day1/#22__selfies__self-referencing__embedded__strings","text":"SELFIES is an alternative to SMILES that guarantees 100% valid molecules. import selfies as sf from rdkit import Chem # Convert SMILES to SELFIES smiles = \"CCO\" selfies_str = sf . encoder ( smiles ) print ( f \"SMILES: { smiles } \" ) print ( f \"SELFIES: { selfies_str } \" ) # Convert SELFIES back to SMILES smiles_back = sf . decoder ( selfies_str ) print ( f \"Back to SMILES: { smiles_back } \" ) # Verify it's a valid molecule mol = Chem . MolFromSmiles ( smiles_back ) print ( f \"Valid molecule: { mol is not None } \" )","title":"2.2 SELFIES (Self-Referencing Embedded Strings)"},{"location":"day1/#22__molecular__fingerprints","text":"Fingerprints are fixed-length binary or count vectors that encode molecular structure.","title":"2.2 Molecular Fingerprints"},{"location":"day1/#morgan__fingerprints__ecfp__-__extended__connectivity__fingerprints","text":"Morgan fingerprints capture circular neighborhoods around each atom. Algorithm : 1. Initialize each atom with a unique identifier based on properties 2. For each radius (0, 1, 2, \u2026), update atom identifiers based on neighbors 3. Hash identifiers to fixed-length bit vector from rdkit import Chem from rdkit.Chem import AllChem from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator import numpy as np import warnings # Create molecule mol = Chem . MolFromSmiles ( \"CCO\" ) # NEW METHOD: Using MorganGenerator morgan_gen = GetMorganGenerator ( radius = 2 , fpSize = 2048 ) # Generate fingerprint as bit vector morgan_fp = morgan_gen . GetFingerprint ( mol ) # Convert to numpy array fp_array = np . zeros (( 2048 ,)) from rdkit import DataStructs DataStructs . ConvertToNumpyArray ( morgan_fp , fp_array ) print ( f \"Fingerprint shape: { fp_array . shape } \" ) print ( f \"Number of set bits: { fp_array . sum () } \" ) # Morgan fingerprint with counts (count version) morgan_gen_count = GetMorganGenerator ( radius = 2 , fpSize = 2048 , countSimulation = True ) morgan_count = morgan_gen_count . GetFingerprint ( mol ) print ( f \"Count fingerprint generated\" ) Visualization of Circular Neighborhoods : # Visualize which atoms contribute to which bits from rdkit.Chem import Draw #info = {} #fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048, bitInfo=info) info = {} fp_old = AllChem . GetMorganFingerprintAsBitVect ( mol , radius = 2 , nBits = 2048 , bitInfo = info ) # Show atom environments for specific bits for bit_id in list ( info . keys ())[: 5 ]: # First 5 bits print ( f \"Bit { bit_id } : { info [ bit_id ] } \" ) # atom_ids = [atom_id for atom_id, radius in info[bit_id]] # img = Draw.DrawMorganBit(mol, bit_id, info) # Uncomment if you want to visualize Parameters : - Radius : Larger radius captures more context - Radius 1 (ECFP2): Immediate neighbors - Radius 2 (ECFP4): Common choice, balances local and broader context - Radius 3 (ECFP6): Larger substructures nBits : Fingerprint length 1024: Fast, but more collisions 2048: Common default 4096: More unique features, slower","title":"Morgan Fingerprints (ECFP - Extended Connectivity Fingerprints)"},{"location":"day1/#maccs__keys","text":"166 predefined structural keys based on common molecular features. from rdkit import Chem from rdkit.Chem import MACCSkeys import numpy as np maccs = MACCSkeys . GenMACCSKeys ( mol ) print ( f \"MACCS keys length: { len ( maccs ) } \" ) # Each bit represents specific structural feature: # Bit 1: Contains isotope # Bit 44: C-O bond # Bit 79: Aromatic ring # etc. # Convert to numpy maccs_array = np . array ( list ( maccs . ToBitString ()), dtype = int ) Advantages : - Interpretable: Each bit has defined meaning - Compact: Only 166 bits - Good for similarity searching Limitations : - Fixed features: Can\u2019t capture novel patterns - Less flexible than Morgan fingerprints","title":"MACCS Keys"},{"location":"day1/#rdkit__fingerprints","text":"Topological fingerprints based on molecular paths. from rdkit import Chem from rdkit.Chem import RDKFingerprint rdkit_fp = RDKFingerprint ( mol , fpSize = 2048 , maxPath = 7 ) # maxPath: maximum path length to consider","title":"RDKit Fingerprints"},{"location":"day1/#atom__pair__and__topological__torsion__fingerprints","text":"Encode distances between atom pairs or torsion angles. from rdkit.Chem import rdMolDescriptors # Atom pairs atom_pairs = rdMolDescriptors . GetHashedAtomPairFingerprintAsBitVect ( mol , nBits = 2048 ) # Topological torsions (4 consecutive atoms) torsions = rdMolDescriptors . GetHashedTopologicalTorsionFingerprintAsBitVect ( mol , nBits = 2048 )","title":"Atom Pair and Topological Torsion Fingerprints"},{"location":"day1/#comparing__molecules__with__fingerprints","text":"from rdkit import DataStructs mol1 = Chem . MolFromSmiles ( \"CCO\" ) mol2 = Chem . MolFromSmiles ( \"CCCO\" ) mol3 = Chem . MolFromSmiles ( \"c1ccccc1\" ) fp1 = AllChem . GetMorganFingerprintAsBitVect ( mol1 , 2 , 2048 ) fp2 = AllChem . GetMorganFingerprintAsBitVect ( mol2 , 2 , 2048 ) fp3 = AllChem . GetMorganFingerprintAsBitVect ( mol3 , 2 , 2048 ) # Tanimoto similarity (Jaccard index for binary vectors) sim_12 = DataStructs . TanimotoSimilarity ( fp1 , fp2 ) sim_13 = DataStructs . TanimotoSimilarity ( fp1 , fp3 ) print ( f \"Similarity(ethanol, propanol): { sim_12 : .3f } \" ) # High (similar structures) print ( f \"Similarity(ethanol, benzene): { sim_13 : .3f } \" ) # Low (different structures) # Other similarity metrics dice = DataStructs . DiceSimilarity ( fp1 , fp2 ) cosine = DataStructs . CosineSimilarity ( fp1 , fp2 )","title":"Comparing Molecules with Fingerprints"},{"location":"day1/#23__molecular__descriptors","text":"Numerical features that capture molecular properties.","title":"2.3 Molecular Descriptors"},{"location":"day1/#types__of__descriptors","text":"1. Physical Descriptors from rdkit.Chem import Descriptors , Crippen mol = Chem . MolFromSmiles ( \"CC(=O)Oc1ccccc1C(=O)O\" ) # Aspirin # Molecular weight mw = Descriptors . MolWt ( mol ) print ( f \"Molecular Weight: { mw : .2f } g/mol\" ) # Lipophilicity (logP - octanol/water partition coefficient) logp = Crippen . MolLogP ( mol ) print ( f \"LogP: { logp : .2f } \" ) # LogP > 5: Too lipophilic (Lipinski's Rule of Five) # Polar Surface Area tpsa = Descriptors . TPSA ( mol ) print ( f \"TPSA: { tpsa : .2f } \u0172\" ) # TPSA < 140: Likely to cross blood-brain barrier # Molar Refractivity mr = Crippen . MolMR ( mol ) print ( f \"Molar Refractivity: { mr : .2f } \" ) 2. Structural Descriptors # Hydrogen bond donors and acceptors h_donors = Descriptors . NumHDonors ( mol ) h_acceptors = Descriptors . NumHAcceptors ( mol ) print ( f \"H-Bond Donors: { h_donors } \" ) print ( f \"H-Bond Acceptors: { h_acceptors } \" ) # Rotatable bonds (flexibility) rot_bonds = Descriptors . NumRotatableBonds ( mol ) print ( f \"Rotatable Bonds: { rot_bonds } \" ) # Ring information num_rings = Descriptors . RingCount ( mol ) aromatic_rings = Descriptors . NumAromaticRings ( mol ) print ( f \"Total Rings: { num_rings } , Aromatic: { aromatic_rings } \" ) # Fraction of sp3 carbons (saturation) frac_sp3 = Descriptors . FractionCSP3 ( mol ) print ( f \"Fraction Csp3: { frac_sp3 : .2f } \" ) 3. Topological Descriptors from rdkit.Chem import GraphDescriptors # Balaban J index (molecular branching) balaban = GraphDescriptors . BalabanJ ( mol ) # Bertz complexity index bertz = GraphDescriptors . BertzCT ( mol ) # Chi indices (connectivity) chi0 = GraphDescriptors . Chi0 ( mol ) chi1 = GraphDescriptors . Chi1 ( mol ) 4. 3D Descriptors from rdkit.Chem import AllChem , Descriptors3D # Generate 3D coordinates mol_3d = Chem . AddHs ( mol ) AllChem . EmbedMolecule ( mol_3d , randomSeed = 42 ) AllChem . MMFFOptimizeMolecule ( mol_3d ) # 3D descriptors asphericity = Descriptors3D . Asphericity ( mol_3d ) eccentricity = Descriptors3D . Eccentricity ( mol_3d ) inertial_shape = Descriptors3D . InertialShapeFactor ( mol_3d ) radius_of_gyration = Descriptors3D . RadiusOfGyration ( mol_3d ) print ( f \"Radius of Gyration: { radius_of_gyration : .2f } \u0172\" )","title":"Types of Descriptors"},{"location":"day1/#drug-likeness__metrics","text":"Lipinski\u2019s Rule of Five def lipinski_rule_of_five ( mol ): \"\"\" Predicts if molecule is drug-like Rules: - MW <= 500 - LogP <= 5 - H-bond donors <= 5 - H-bond acceptors <= 10 \"\"\" mw = Descriptors . MolWt ( mol ) logp = Crippen . MolLogP ( mol ) hbd = Descriptors . NumHDonors ( mol ) hba = Descriptors . NumHAcceptors ( mol ) violations = 0 if mw > 500 : violations += 1 if logp > 5 : violations += 1 if hbd > 5 : violations += 1 if hba > 10 : violations += 1 return violations <= 1 # Allow 1 violation is_druglike = lipinski_rule_of_five ( mol ) print ( f \"Passes Lipinski's Rule: { is_druglike } \" ) QED (Quantitative Estimate of Drug-likeness) from rdkit.Chem import QED qed_score = QED . qed ( mol ) print ( f \"QED Score: { qed_score : .3f } \" ) # Range: [0, 1], higher is more drug-like # Based on 8 molecular properties Synthetic Accessibility Score from rdkit.Chem import RDConfig import sys sys . path . append ( f ' { RDConfig . RDContribDir } /SA_Score' ) import sascorer sa_score = sascorer . calculateScore ( mol ) print ( f \"SA Score: { sa_score : .2f } \" ) # Range: [1, 10] # 1: Easy to synthesize # 10: Difficult to synthesize","title":"Drug-Likeness Metrics"},{"location":"day1/#creating__feature__vectors","text":"def calculate_molecular_descriptors ( smiles ): \"\"\" Comprehensive descriptor calculation \"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None # Add hydrogens for accurate calculations mol = Chem . AddHs ( mol ) descriptors = { # Physical 'MW' : Descriptors . MolWt ( mol ), 'LogP' : Crippen . MolLogP ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'MolMR' : Crippen . MolMR ( mol ), # Structural 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumHeteroatoms' : Descriptors . NumHeteroatoms ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'NumSaturatedRings' : Descriptors . NumSaturatedRings ( mol ), 'NumAliphaticRings' : Descriptors . NumAliphaticRings ( mol ), 'RingCount' : Descriptors . RingCount ( mol ), # Complexity 'BertzCT' : GraphDescriptors . BertzCT ( mol ), 'NumBridgeheadAtoms' : Descriptors . NumBridgeheadAtoms ( mol ), 'NumSpiroAtoms' : Descriptors . NumSpiroAtoms ( mol ), # Electronic 'LabuteASA' : Descriptors . LabuteASA ( mol ), 'PEOE_VSA1' : Descriptors . PEOE_VSA1 ( mol ), # Counts 'NumCarbon' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 6 ]), 'NumNitrogen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 7 ]), 'NumOxygen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 8 ]), 'NumHalogens' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () in [ 9 , 17 , 35 , 53 ]]), # Saturation 'FractionCsp3' : Descriptors . FractionCSP3 ( mol ), # Drug-likeness 'QED' : QED . qed ( mol ), } return descriptors # Example usage smiles_list = [ \"CCO\" , \"CC(=O)Oc1ccccc1C(=O)O\" , \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\" ] import pandas as pd descriptor_list = [ calculate_molecular_descriptors ( s ) for s in smiles_list ] df_descriptors = pd . DataFrame ( descriptor_list ) df_descriptors [ 'SMILES' ] = smiles_list print ( df_descriptors )","title":"Creating Feature Vectors"},{"location":"day1/#24__graph__representations","text":"Molecules as graphs where atoms are nodes and bonds are edges.","title":"2.4 Graph Representations"},{"location":"day1/#graph__structure","text":"import networkx as nx def mol_to_graph ( smiles ): \"\"\"Convert molecule to NetworkX graph\"\"\" mol = Chem . MolFromSmiles ( smiles ) # Create graph G = nx . Graph () # Add nodes (atoms) for atom in mol . GetAtoms (): G . add_node ( atom . GetIdx (), atomic_num = atom . GetAtomicNum (), symbol = atom . GetSymbol (), degree = atom . GetDegree (), formal_charge = atom . GetFormalCharge (), num_h = atom . GetTotalNumHs (), hybridization = str ( atom . GetHybridization ()), is_aromatic = atom . GetIsAromatic () ) # Add edges (bonds) for bond in mol . GetBonds (): G . add_edge ( bond . GetBeginAtomIdx (), bond . GetEndAtomIdx (), bond_type = str ( bond . GetBondType ()), is_conjugated = bond . GetIsConjugated (), is_aromatic = bond . GetIsAromatic () ) return G # Example G = mol_to_graph ( \"CCO\" ) print ( f \"Nodes: { G . number_of_nodes () } \" ) print ( f \"Edges: { G . number_of_edges () } \" ) print ( f \"Node features: { G . nodes [ 0 ] } \" )","title":"Graph Structure"},{"location":"day1/#adjacency__matrix__representation","text":"def get_adjacency_matrix ( smiles , max_atoms = 50 ): \"\"\"Get adjacency matrix with padding\"\"\" mol = Chem . MolFromSmiles ( smiles ) num_atoms = mol . GetNumAtoms () # Initialize matrix adj_matrix = np . zeros (( max_atoms , max_atoms )) # Fill adjacency matrix for bond in mol . GetBonds (): i = bond . GetBeginAtomIdx () j = bond . GetEndAtomIdx () adj_matrix [ i , j ] = 1 adj_matrix [ j , i ] = 1 # Symmetric return adj_matrix , num_atoms adj , n_atoms = get_adjacency_matrix ( \"CCO\" ) print ( f \"Adjacency matrix shape: { adj . shape } \" ) print ( f \"Actual atoms: { n_atoms } \" )","title":"Adjacency Matrix Representation"},{"location":"day1/#node__and__edge__features","text":"def get_node_features ( atom ): \"\"\"Extract features for a single atom\"\"\" return np . array ([ atom . GetAtomicNum (), # Atomic number atom . GetDegree (), # Number of bonds atom . GetFormalCharge (), # Charge atom . GetNumRadicalElectrons (), # Radicals atom . GetHybridization () . real , # sp, sp2, sp3 atom . GetIsAromatic (), # Aromaticity atom . GetTotalNumHs (), # Hydrogens ]) def get_edge_features ( bond ): \"\"\"Extract features for a single bond\"\"\" bond_type_map = { Chem . rdchem . BondType . SINGLE : 1 , Chem . rdchem . BondType . DOUBLE : 2 , Chem . rdchem . BondType . TRIPLE : 3 , Chem . rdchem . BondType . AROMATIC : 4 , } return np . array ([ bond_type_map . get ( bond . GetBondType (), 0 ), bond . GetIsConjugated (), bond . GetIsAromatic (), ]) Advantages of Graph Representations : - Natural for molecules (atoms connected by bonds) - Permutation invariant (atom order doesn\u2019t matter) - Captures topology and local structure - Enables Graph Neural Networks (Day 3) Limitations : - More complex to implement - Computationally expensive for large molecules - Requires specialized neural network architectures Hasta aca","title":"Node and Edge Features"},{"location":"day1/#3__traditional__machine__learning__methods","text":"Before deep learning, these methods were (and still are) workhorses of molecular ML.","title":"3. Traditional Machine Learning Methods"},{"location":"day1/#31__feature__engineering__principles","text":"Domain Knowledge is Key : - Choose descriptors relevant to property being predicted - For solubility: polarity, surface area, H-bond capacity - For toxicity: reactive functional groups, lipophilicity Feature Scaling : from sklearn.preprocessing import StandardScaler , MinMaxScaler , RobustScaler # Standardization (zero mean, unit variance) scaler = StandardScaler () X_scaled = scaler . fit_transform ( X ) # Good for: Most ML algorithms, assumes normal distribution # Min-Max scaling (range [0, 1]) scaler = MinMaxScaler () X_scaled = scaler . fit_transform ( X ) # Good for: Neural networks, when you need bounded values # Robust scaling (uses median and IQR) scaler = RobustScaler () X_scaled = scaler . fit_transform ( X ) # Good for: Data with outliers Feature Selection : from sklearn.feature_selection import ( VarianceThreshold , SelectKBest , f_regression , RFE , SelectFromModel ) from sklearn.ensemble import RandomForestRegressor # Remove low-variance features selector = VarianceThreshold ( threshold = 0.01 ) X_high_var = selector . fit_transform ( X ) # Univariate feature selection selector = SelectKBest ( score_func = f_regression , k = 10 ) X_selected = selector . fit_transform ( X , y ) # Recursive Feature Elimination model = RandomForestRegressor () rfe = RFE ( model , n_features_to_select = 10 ) X_rfe = rfe . fit_transform ( X , y ) # Feature importance from model model = RandomForestRegressor () model . fit ( X , y ) selector = SelectFromModel ( model , prefit = True , threshold = 'median' ) X_important = selector . transform ( X )","title":"3.1 Feature Engineering Principles"},{"location":"day1/#32__random__forests","text":"Ensemble of decision trees, each trained on random subset of data and features.","title":"3.2 Random Forests"},{"location":"day1/#how__it__works","text":"Bootstrap Sampling : Create N random subsets of training data (with replacement) Random Feature Selection : At each split, consider random subset of features Tree Building : Build deep trees without pruning Prediction : Average predictions from all trees (regression) or vote (classification) from sklearn.ensemble import RandomForestRegressor , RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np # Regression example rf_reg = RandomForestRegressor ( n_estimators = 100 , # Number of trees max_depth = None , # Grow trees fully min_samples_split = 2 , # Minimum samples to split node min_samples_leaf = 1 , # Minimum samples in leaf max_features = 'sqrt' , # Features to consider at each split bootstrap = True , # Use bootstrap sampling random_state = 42 , n_jobs =- 1 # Use all CPUs ) # Train rf_reg . fit ( X_train , y_train ) # Predict y_pred = rf_reg . predict ( X_test ) # Cross-validation cv_scores = cross_val_score ( rf_reg , X , y , cv = 5 , scoring = 'r2' ) print ( f \"Cross-validation R\u00b2: { cv_scores . mean () : .3f } \u00b1 { cv_scores . std () : .3f } \" ) # Feature importance importances = rf_reg . feature_importances_ indices = np . argsort ( importances )[:: - 1 ] print ( \"Feature ranking:\" ) for i , idx in enumerate ( indices [: 10 ]): print ( f \" { i + 1 } . Feature { idx } : { importances [ idx ] : .4f } \" ) # Visualize feature importance import matplotlib.pyplot as plt plt . figure ( figsize = ( 10 , 6 )) plt . bar ( range ( len ( importances )), importances [ indices ]) plt . xlabel ( 'Features' ) plt . ylabel ( 'Importance' ) plt . title ( 'Random Forest Feature Importance' ) plt . tight_layout () plt . show () Advantages : - Handles non-linear relationships - Robust to outliers - Provides feature importance - Little hyperparameter tuning needed - Resistant to overfitting (with enough trees) Limitations : - Can be slow for very large datasets - Not great for extrapolation - Black-box model (hard to interpret individual predictions) Hyperparameter Tuning : from sklearn.model_selection import RandomizedSearchCV param_dist = { 'n_estimators' : [ 100 , 200 , 500 , 1000 ], 'max_depth' : [ 10 , 20 , 30 , None ], 'min_samples_split' : [ 2 , 5 , 10 ], 'min_samples_leaf' : [ 1 , 2 , 4 ], 'max_features' : [ 'sqrt' , 'log2' , 0.5 ] } random_search = RandomizedSearchCV ( RandomForestRegressor ( random_state = 42 ), param_distributions = param_dist , n_iter = 50 , cv = 5 , scoring = 'r2' , random_state = 42 , n_jobs =- 1 ) random_search . fit ( X_train , y_train ) print ( f \"Best parameters: { random_search . best_params_ } \" ) print ( f \"Best score: { random_search . best_score_ : .3f } \" )","title":"How it Works"},{"location":"day1/#33__model__evaluation__metrics","text":"","title":"3.3 Model Evaluation Metrics"},{"location":"day1/#regression__metrics","text":"from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score # Mean Absolute Error mae = mean_absolute_error ( y_true , y_pred ) print ( f \"MAE: { mae : .3f } \" ) # Root Mean Squared Error rmse = np . sqrt ( mean_squared_error ( y_true , y_pred )) print ( f \"RMSE: { rmse : .3f } \" ) # R\u00b2 Score r2 = r2_score ( y_true , y_pred ) print ( f \"R\u00b2: { r2 : .3f } \" )","title":"Regression Metrics"},{"location":"day1/#4__working__with__chemical__databases","text":"","title":"4. Working with Chemical Databases"},{"location":"day1/#41__public__databases","text":"","title":"4.1 Public Databases"},{"location":"day1/#pubchem","text":"import pubchempy as pcp # Search by name results = pcp . get_compounds ( 'aspirin' , 'name' ) compound = results [ 0 ] print ( f \"IUPAC Name: { compound . iupac_name } \" ) print ( f \"SMILES: { compound . isomeric_smiles } \" ) print ( f \"Molecular Formula: { compound . molecular_formula } \" ) print ( f \"Molecular Weight: { compound . molecular_weight } \" ) # Get properties properties = pcp . get_properties ( [ 'MolecularWeight' , 'XLogP' , 'TPSA' , 'Complexity' ], 'aspirin' , 'name' ) print ( properties )","title":"PubChem"},{"location":"day1/#chembl","text":"from chembl_webresource_client.new_client import new_client # Target search target = new_client . target targets = target . filter ( target_synonym__icontains = 'EGFR' ) for t in targets [: 5 ]: print ( f \" { t [ 'pref_name' ] } : { t [ 'target_chembl_id' ] } \" ) # Activity search activity = new_client . activity activities = activity . filter ( target_chembl_id = 'CHEMBL203' , standard_type = 'IC50' , standard_relation = '=' , pchembl_value__isnull = False ) # Convert to DataFrame import pandas as pd data = [] for act in activities [: 1000 ]: data . append ({ 'molecule_chembl_id' : act [ 'molecule_chembl_id' ], 'smiles' : act [ 'canonical_smiles' ], 'ic50' : act [ 'standard_value' ], 'pchembl_value' : act [ 'pchembl_value' ] }) df = pd . DataFrame ( data ) print ( df . head ())","title":"ChEMBL"},{"location":"day1/#42__data__preprocessing","text":"","title":"4.2 Data Preprocessing"},{"location":"day1/#molecular__standardization","text":"from rdkit.Chem import SaltRemover def standardize_molecule ( smiles ): \"\"\"Standardize molecular representation\"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None # Remove salts remover = SaltRemover . SaltRemover () mol = remover . StripMol ( mol ) # Get canonical SMILES canonical_smiles = Chem . MolToSmiles ( mol ) return canonical_smiles # Process dataset smiles_list = [ \"CC(=O)O.Na\" , \"CCO\" , \"[NH3+]CC[O-]\" ] standardized = [ standardize_molecule ( s ) for s in smiles_list ] print ( standardized )","title":"Molecular Standardization"},{"location":"day1/#traintest__splitting","text":"from sklearn.model_selection import train_test_split # Random split X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 42 ) # Scaffold split for molecules from rdkit.Chem.Scaffolds import MurckoScaffold from collections import defaultdict def scaffold_split ( smiles_list , test_size = 0.2 ): \"\"\"Split by molecular scaffolds\"\"\" scaffolds = defaultdict ( list ) for idx , smiles in enumerate ( smiles_list ): mol = Chem . MolFromSmiles ( smiles ) scaffold = MurckoScaffold . MurckoScaffoldSmiles ( mol = mol , includeChirality = False ) scaffolds [ scaffold ] . append ( idx ) # Distribute scaffolds scaffold_sets = sorted ( list ( scaffolds . values ()), key = len , reverse = True ) n_total = len ( smiles_list ) n_test = int ( n_total * test_size ) train_idx , test_idx = [], [] train_count = 0 for scaffold_set in scaffold_sets : if train_count + len ( scaffold_set ) <= n_total - n_test : train_idx . extend ( scaffold_set ) train_count += len ( scaffold_set ) else : test_idx . extend ( scaffold_set ) return train_idx , test_idx","title":"Train/Test Splitting"},{"location":"day1/#5__practical__exercise__solubility__prediction","text":"","title":"5. Practical Exercise: Solubility Prediction"},{"location":"day1/#complete__workflow","text":"import pandas as pd import numpy as np from rdkit import Chem from rdkit.Chem import Descriptors , AllChem from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error import matplotlib.pyplot as plt # Step 1: Load Data url = \"https://raw.githubusercontent.com/deepchem/deepchem/master/datasets/delaney-processed.csv\" df = pd . read_csv ( url ) print ( f \"Dataset size: { len ( df ) } \" ) # Step 2: Calculate Descriptors def calculate_descriptors ( smiles ): mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None return { 'MW' : Descriptors . MolWt ( mol ), 'LogP' : Descriptors . MolLogP ( mol ), 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'NumRings' : Descriptors . RingCount ( mol ), 'MolMR' : Descriptors . MolMR ( mol ), 'FractionCsp3' : Descriptors . FractionCsp3 ( mol ) } desc_list = [ calculate_descriptors ( s ) for s in df [ 'smiles' ]] X_desc = pd . DataFrame ([ d for d in desc_list if d is not None ]) y = df [ 'measured log solubility in mols per litre' ] . values [: len ( X_desc )] # Step 3: Train-Test Split X_train , X_test , y_train , y_test = train_test_split ( X_desc , y , test_size = 0.2 , random_state = 42 ) # Step 4: Scale Features scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_test_scaled = scaler . transform ( X_test ) # Step 5: Train Model model = RandomForestRegressor ( n_estimators = 100 , random_state = 42 ) model . fit ( X_train_scaled , y_train ) # Step 6: Evaluate y_pred = model . predict ( X_test_scaled ) rmse = np . sqrt ( mean_squared_error ( y_test , y_pred )) mae = mean_absolute_error ( y_test , y_pred ) r2 = r2_score ( y_test , y_pred ) print ( f \" \\n Results:\" ) print ( f \"RMSE: { rmse : .3f } \" ) print ( f \"MAE: { mae : .3f } \" ) print ( f \"R\u00b2: { r2 : .3f } \" ) # Step 7: Visualize plt . figure ( figsize = ( 8 , 8 )) plt . scatter ( y_test , y_pred , alpha = 0.5 ) plt . plot ([ y_test . min (), y_test . max ()], [ y_test . min (), y_test . max ()], 'r--' , lw = 2 ) plt . xlabel ( 'True Solubility (log M)' ) plt . ylabel ( 'Predicted Solubility (log M)' ) plt . title ( f 'Predictions (R\u00b2 = { r2 : .3f } )' ) plt . axis ( 'equal' ) plt . tight_layout () plt . show () # Step 8: Feature Importance importances = model . feature_importances_ feature_importance_df = pd . DataFrame ({ 'feature' : X_desc . columns , 'importance' : importances }) . sort_values ( 'importance' , ascending = False ) print ( \" \\n Top 5 Most Important Features:\" ) print ( feature_importance_df . head ()) Expected Results : - Test RMSE: ~0.7-0.9 log units - R\u00b2: ~0.75-0.85 - Key features: LogP, molecular weight, polar surface area","title":"Complete Workflow"},{"location":"day1/#6__example__with__selfies","text":"import numpy as np import selfies as sf from rdkit import Chem from rdkit.Chem import Descriptors , Crippen # Convert SELFIES to molecule selfies_str = sf . encoder ( \"CC(=O)Oc1ccccc1C(=O)O\" ) # Aspirin in SELFIES print ( f \"SELFIES representation: { selfies_str } \" ) smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) # Molecular weight mw = Descriptors . MolWt ( mol ) print ( f \"Molecular Weight: { mw : .2f } g/mol\" ) # Lipophilicity (logP - octanol/water partition coefficient) logp = Crippen . MolLogP ( mol ) print ( f \"LogP: { logp : .2f } \" ) # LogP > 5: Too lipophilic (Lipinski's Rule of Five) # Polar Surface Area tpsa = Descriptors . TPSA ( mol ) print ( f \"TPSA: { tpsa : .2f } \u0172\" ) # TPSA < 140: Likely to cross blood-brain barrier # Molar Refractivity mr = Crippen . MolMR ( mol ) print ( f \"Molar Refractivity: { mr : .2f } \" ) #### # Hydrogen bond donors and acceptors h_donors = Descriptors . NumHDonors ( mol ) h_acceptors = Descriptors . NumHAcceptors ( mol ) print ( f \"H-Bond Donors: { h_donors } \" ) print ( f \"H-Bond Acceptors: { h_acceptors } \" ) # Rotatable bonds (flexibility) rot_bonds = Descriptors . NumRotatableBonds ( mol ) print ( f \"Rotatable Bonds: { rot_bonds } \" ) # Ring information num_rings = Descriptors . RingCount ( mol ) aromatic_rings = Descriptors . NumAromaticRings ( mol ) print ( f \"Total Rings: { num_rings } , Aromatic: { aromatic_rings } \" ) # Fraction of sp3 carbons (saturation) frac_sp3 = Descriptors . FractionCSP3 ( mol ) print ( f \"Fraction Csp3: { frac_sp3 : .2f } \" ) #### from rdkit.Chem import GraphDescriptors # Balaban J index (molecular branching) balaban = GraphDescriptors . BalabanJ ( mol ) # Bertz complexity index bertz = GraphDescriptors . BertzCT ( mol ) # Chi indices (connectivity) chi0 = GraphDescriptors . Chi0 ( mol ) chi1 = GraphDescriptors . Chi1 ( mol ) #### from rdkit.Chem import AllChem , Descriptors3D # Generate 3D coordinates mol_3d = Chem . AddHs ( mol ) AllChem . EmbedMolecule ( mol_3d , randomSeed = 42 ) AllChem . MMFFOptimizeMolecule ( mol_3d ) # 3D descriptors asphericity = Descriptors3D . Asphericity ( mol_3d ) eccentricity = Descriptors3D . Eccentricity ( mol_3d ) inertial_shape = Descriptors3D . InertialShapeFactor ( mol_3d ) radius_of_gyration = Descriptors3D . RadiusOfGyration ( mol_3d ) print ( f \"Radius of Gyration: { radius_of_gyration : .2f } \u0172\" ) #### def lipinski_rule_of_five ( mol ): \"\"\" Predicts if molecule is drug-like Rules: - MW <= 500 - LogP <= 5 - H-bond donors <= 5 - H-bond acceptors <= 10 \"\"\" mw = Descriptors . MolWt ( mol ) logp = Crippen . MolLogP ( mol ) hbd = Descriptors . NumHDonors ( mol ) hba = Descriptors . NumHAcceptors ( mol ) violations = 0 if mw > 500 : violations += 1 if logp > 5 : violations += 1 if hbd > 5 : violations += 1 if hba > 10 : violations += 1 return violations <= 1 # Allow 1 violation is_druglike = lipinski_rule_of_five ( mol ) print ( f \"Passes Lipinski's Rule: { is_druglike } \" ) #### from rdkit.Chem import QED qed_score = QED . qed ( mol ) print ( f \"QED Score: { qed_score : .3f } \" ) # Range: [0, 1], higher is more drug-like # Based on 8 molecular properties #### from rdkit.Chem import RDConfig import sys sys . path . append ( f ' { RDConfig . RDContribDir } /SA_Score' ) import sascorer sa_score = sascorer . calculateScore ( mol ) print ( f \"SA Score: { sa_score : .2f } \" ) # Range: [1, 10] # 1: Easy to synthesize # 10: Difficult to synthesize #### def calculate_molecular_descriptors ( selfies_str ): \"\"\" Comprehensive descriptor calculation from SELFIES \"\"\" # Convert SELFIES to SMILES then to molecule smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None # Add hydrogens for accurate calculations mol = Chem . AddHs ( mol ) descriptors = { # Physical 'MW' : Descriptors . MolWt ( mol ), 'LogP' : Crippen . MolLogP ( mol ), 'TPSA' : Descriptors . TPSA ( mol ), 'MolMR' : Crippen . MolMR ( mol ), # Structural 'NumHDonors' : Descriptors . NumHDonors ( mol ), 'NumHAcceptors' : Descriptors . NumHAcceptors ( mol ), 'NumRotatableBonds' : Descriptors . NumRotatableBonds ( mol ), 'NumHeteroatoms' : Descriptors . NumHeteroatoms ( mol ), 'NumAromaticRings' : Descriptors . NumAromaticRings ( mol ), 'NumSaturatedRings' : Descriptors . NumSaturatedRings ( mol ), 'NumAliphaticRings' : Descriptors . NumAliphaticRings ( mol ), 'RingCount' : Descriptors . RingCount ( mol ), # Complexity 'BertzCT' : GraphDescriptors . BertzCT ( mol ), 'NumBridgeheadAtoms' : Descriptors . NumBridgeheadAtoms ( mol ), 'NumSpiroAtoms' : Descriptors . NumSpiroAtoms ( mol ), # Electronic 'LabuteASA' : Descriptors . LabuteASA ( mol ), 'PEOE_VSA1' : Descriptors . PEOE_VSA1 ( mol ), # Counts 'NumCarbon' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 6 ]), 'NumNitrogen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 7 ]), 'NumOxygen' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () == 8 ]), 'NumHalogens' : len ([ atom for atom in mol . GetAtoms () if atom . GetAtomicNum () in [ 9 , 17 , 35 , 53 ]]), # Saturation 'FractionCsp3' : Descriptors . FractionCSP3 ( mol ), # Drug-likeness 'QED' : QED . qed ( mol ), } return descriptors # Example usage - Convert SMILES to SELFIES first smiles_list = [ \"CCO\" , \"CC(=O)Oc1ccccc1C(=O)O\" , \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\" ] selfies_list = [ sf . encoder ( s ) for s in smiles_list ] print ( \" \\n SELFIES representations:\" ) for smiles , selfies_str in zip ( smiles_list , selfies_list ): print ( f \"SMILES: { smiles } \" ) print ( f \"SELFIES: { selfies_str } \" ) print () import pandas as pd descriptor_list = [ calculate_molecular_descriptors ( s ) for s in selfies_list ] df_descriptors = pd . DataFrame ( descriptor_list ) df_descriptors [ 'SELFIES' ] = selfies_list df_descriptors [ 'SMILES' ] = smiles_list # Optional: keep original SMILES for reference print ( df_descriptors ) #### import networkx as nx def mol_to_graph ( selfies_str ): \"\"\"Convert SELFIES molecule to NetworkX graph\"\"\" smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) # Create graph G = nx . Graph () # Add nodes (atoms) for atom in mol . GetAtoms (): G . add_node ( atom . GetIdx (), atomic_num = atom . GetAtomicNum (), symbol = atom . GetSymbol (), degree = atom . GetDegree (), formal_charge = atom . GetFormalCharge (), num_h = atom . GetTotalNumHs (), hybridization = str ( atom . GetHybridization ()), is_aromatic = atom . GetIsAromatic () ) # Add edges (bonds) for bond in mol . GetBonds (): G . add_edge ( bond . GetBeginAtomIdx (), bond . GetEndAtomIdx (), bond_type = str ( bond . GetBondType ()), is_conjugated = bond . GetIsConjugated (), is_aromatic = bond . GetIsAromatic () ) return G # Example selfies_ethanol = sf . encoder ( \"CCO\" ) G = mol_to_graph ( selfies_ethanol ) print ( f \" \\n Graph from SELFIES: { selfies_ethanol } \" ) print ( f \"Nodes: { G . number_of_nodes () } \" ) print ( f \"Edges: { G . number_of_edges () } \" ) print ( f \"Node features: { G . nodes [ 0 ] } \" ) #### def get_adjacency_matrix ( selfies_str , max_atoms = 50 ): \"\"\"Get adjacency matrix with padding from SELFIES\"\"\" smiles = sf . decoder ( selfies_str ) mol = Chem . MolFromSmiles ( smiles ) num_atoms = mol . GetNumAtoms () # Initialize matrix adj_matrix = np . zeros (( max_atoms , max_atoms )) # Fill adjacency matrix for bond in mol . GetBonds (): i = bond . GetBeginAtomIdx () j = bond . GetEndAtomIdx () adj_matrix [ i , j ] = 1 adj_matrix [ j , i ] = 1 # Symmetric return adj_matrix , num_atoms selfies_ethanol = sf . encoder ( \"CCO\" ) adj , n_atoms = get_adjacency_matrix ( selfies_ethanol ) print ( f \" \\n Adjacency matrix from SELFIES: { selfies_ethanol } \" ) print ( f \"Adjacency matrix shape: { adj . shape } \" ) print ( f \"Actual atoms: { n_atoms } \" )","title":"6. Example with SELFIES"},{"location":"day1/#7__key__takeaways","text":"","title":"7. Key Takeaways"},{"location":"day1/#molecular__representations","text":"SMILES : Compact text representation, requires careful handling Fingerprints : Fixed-length vectors, good for similarity and ML Descriptors : Interpretable features, require domain knowledge Graphs : Natural representation, enables GNNs (Day 3)","title":"Molecular Representations"},{"location":"day1/#traditional__ml__methods","text":"Random Forests : Robust baseline, handles non-linearity, provides feature importance SVMs : Effective in high dimensions, requires scaling Gaussian Processes : Provides uncertainty, excellent for active learning Gradient Boosting : Often best performance, requires careful tuning","title":"Traditional ML Methods"},{"location":"day1/#best__practices","text":"Always validate molecular structures Use scaffold-based splits for realistic evaluation Scale features appropriately for each algorithm Compare multiple representations (descriptors vs fingerprints) Report multiple metrics (RMSE, MAE, R\u00b2) Analyze feature importance for insights Check for data leakage in preprocessing","title":"Best Practices"},{"location":"day1/#common__pitfalls","text":"Using random splits instead of scaffold splits Forgetting to scale features for SVMs Not handling invalid SMILES Overfitting due to small datasets Ignoring uncertainty in predictions","title":"Common Pitfalls"},{"location":"day1/#8__resources__and__further__reading","text":"","title":"8. Resources and Further Reading"},{"location":"day1/#software__libraries","text":"RDKit : Cheminformatics toolkit - https://www.rdkit.org/ scikit-learn : Machine learning library - https://scikit-learn.org/ Pandas : Data manipulation - https://pandas.pydata.org/ Matplotlib : Visualization","title":"Software Libraries"},{"location":"day1/#databases","text":"PubChem : https://pubchem.ncbi.nlm.nih.gov/ ChEMBL : https://www.ebi.ac.uk/chembl/ ZINC : https://zinc.docking.org/ Materials Project : https://materialsproject.org/ QM9 : http://quantum-machine.org/datasets/","title":"Databases"},{"location":"day1/#papers","text":"\u201cMolecular descriptors for chemoinformatics\u201d - Todeschini & Consonni \u201cMachine Learning in Materials Informatics\u201d - Butler et al., 2018 \u201cGuidelines for ML predictive models in biomedical research\u201d - Luo et al., 2016 \u201cDeep Learning for Molecular Design\u201d - Elton et al., 2019","title":"Papers"},{"location":"day1/#tutorials","text":"RDKit Cookbook - https://www.rdkit.org/docs/Cookbook.html Scikit-learn User Guide - https://scikit-learn.org/stable/user_guide.html DeepChem Tutorials - https://deepchem.io/","title":"Tutorials"},{"location":"day1/#homework__assignment","text":"Data Exploration Download QM9 dataset Calculate descriptors for 1000 random molecules Visualize descriptor distributions and correlations Classification Model Build a model to predict if a molecule has dipole moment > 2 Debye Use both descriptors and fingerprints Report precision, recall, and AUC Model Comparison Compare Random Forest, SVM, and Gradient Boosting Use 5-fold cross-validation Create visualizations comparing performance Feature Analysis Identify the top 5 most important features Explain why these features are relevant Visualize feature importance Advanced Challenge Implement scaffold-based splitting Compare performance with random split Discuss implications for model generalization Prepare Questions Review neural networks basics (Day 0) Think about limitations of traditional ML for molecules Prepare questions for Day 2 on deep learning","title":"Homework Assignment"},{"location":"day1/#appendix__quick__reference__tables","text":"","title":"Appendix: Quick Reference Tables"},{"location":"day1/#molecular__descriptor__ranges","text":"Descriptor Range Interpretation Drug-like Range Molecular Weight 0-\u221e Size 150-500 g/mol LogP -\u221e to +\u221e Lipophilicity 0-5 TPSA 0-\u221e \u0172 Polar surface 20-140 \u0172 H-Bond Donors 0-\u221e H-bond donors 0-5 H-Bond Acceptors 0-\u221e H-bond acceptors 0-10 Rotatable Bonds 0-\u221e Flexibility 0-10 QED 0-1 Drug-likeness > 0.5","title":"Molecular Descriptor Ranges"},{"location":"day1/#model__selection__guide","text":"Model Best For Pros Cons Random Forest General purpose Easy, robust, feature importance Slower prediction SVM Small datasets Good generalization Slow training, needs scaling Gaussian Process Active learning Uncertainty estimates Very slow for large data Gradient Boosting Best performance Highest accuracy Prone to overfitting","title":"Model Selection Guide"},{"location":"day1/#common__rdkit__functions","text":"# Molecule creation mol = Chem . MolFromSmiles ( \"CCO\" ) # Validation is_valid = mol is not None # Canonical SMILES canonical = Chem . MolToSmiles ( mol ) # Add/remove hydrogens mol_h = Chem . AddHs ( mol ) mol_no_h = Chem . RemoveHs ( mol ) # 3D coordinates AllChem . EmbedMolecule ( mol ) # Descriptors mw = Descriptors . MolWt ( mol ) logp = Descriptors . MolLogP ( mol ) tpsa = Descriptors . TPSA ( mol ) # Fingerprints fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) # Similarity similarity = DataStructs . TanimotoSimilarity ( fp1 , fp2 ) # Substructure search pattern = Chem . MolFromSmarts ( \"[OH]\" ) has_match = mol . HasSubstructMatch ( pattern ) # Visualization img = Draw . MolToImage ( mol , size = ( 300 , 300 ))","title":"Common RDKit Functions"},{"location":"day2/","text":"Day 2: Deep Learning for Molecular Systems \u00b6 Machine Learning for Molecular Systems - Advanced Course Course Module 2: Neural Networks and Deep Learning Applications Table of Contents \u00b6 Deep Learning Fundamentals Feedforward Neural Networks Multi-Task Learning Convolutional Neural Networks Recurrent Neural Networks Transfer Learning Complete Practical Exercise Model Interpretation Best Practices Key Takeaways Resources Homework Assignment Appendix 1. Deep Learning Fundamentals \u00b6 1.1 Why Deep Learning for Molecules? \u00b6 Deep learning has revolutionized molecular property prediction by automatically learning complex representations from raw molecular data. Unlike traditional machine learning approaches that rely on hand-crafted descriptors, deep learning models can: Key Advantages: Automatic Feature Learning : Neural networks learn hierarchical representations directly from molecular structures (SMILES, graphs, 3D coordinates) without manual feature engineering Complex Pattern Recognition : Capture non-linear relationships and subtle structural patterns that affect molecular properties Transfer Learning : Pre-trained models on large chemical databases can be fine-tuned for specific tasks with limited data End-to-End Learning : Direct mapping from molecular representation to property prediction in a single unified model Multi-Task Learning : Simultaneously predict multiple properties, sharing learned representations across tasks Detailed Examples: Example 1: Solubility Prediction Traditional ML approach requires calculating molecular descriptors (LogP, molecular weight, H-bond donors/acceptors). Deep learning models can learn these patterns directly from SMILES strings: Input: \"CC(=O)OC1=CC=CC=C1C(=O)O\" (Aspirin) Model learns: aromatic rings \u2192 non-polar regions carboxyl groups \u2192 polar regions overall balance \u2192 solubility estimate Output: LogS = -1.5 (moderate solubility) Example 2: Toxicity Prediction Deep learning excels at identifying toxic substructures (toxicophores) without explicit rules: Model automatically learns: - Aromatic amines \u2192 potential carcinogens - Epoxides \u2192 DNA reactivity - Nitro groups \u2192 mutagenicity risk - Combination patterns \u2192 synergistic effects Example 3: Drug-Likeness Rather than using Lipinski\u2019s Rule of Five, neural networks learn implicit drug-likeness: Training on FDA-approved drugs, the model learns: - Optimal molecular weight ranges - Hydrogen bonding patterns - Lipophilicity balance - Metabolic stability indicators - Blood-brain barrier permeability 1.2 Neural Network Basics \u00b6 A neural network consists of interconnected layers of artificial neurons that transform input data through learned weights and biases. Architecture Components: Input Layer : Receives molecular features (fingerprints, descriptors, or embeddings) - Size determined by feature dimension - No activation function applied Hidden Layers : Transform inputs through non-linear operations - Each neuron computes: z = \u03a3(w_i \u00d7 x_i) + b - Applies activation function: a = f(z) - Multiple layers enable hierarchical feature learning Output Layer : Produces final predictions - Regression: Single neuron with linear activation - Binary classification: Single neuron with sigmoid activation - Multi-class: Multiple neurons with softmax activation Mathematical Foundation: For a single neuron: Input: x = [x\u2081, x\u2082, ..., x\u2099] Weights: w = [w\u2081, w\u2082, ..., w\u2099] Bias: b Linear transformation: z = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b = w^T x + b Activation: a = f(z) For a layer: Z^[l] = W^[l] \u00d7 A^[l-1] + b^[l] A^[l] = f(Z^[l]) Where: - l = layer index - W^[l] = weight matrix for layer l - A^[l-1] = activations from previous layer - b^[l] = bias vector for layer l Forward Propagation Example: # Simple 3-layer network import numpy as np def forward_propagation ( X , parameters ): \"\"\" X: input features (n_features, m_samples) parameters: dictionary containing W1, b1, W2, b2, W3, b3 \"\"\" # Layer 1: Input \u2192 Hidden (128 neurons) Z1 = np . dot ( parameters [ 'W1' ], X ) + parameters [ 'b1' ] A1 = relu ( Z1 ) # Layer 2: Hidden \u2192 Hidden (64 neurons) Z2 = np . dot ( parameters [ 'W2' ], A1 ) + parameters [ 'b2' ] A2 = relu ( Z2 ) # Layer 3: Hidden \u2192 Output (1 neuron for regression) Z3 = np . dot ( parameters [ 'W3' ], A2 ) + parameters [ 'b3' ] A3 = Z3 # Linear activation for regression cache = { 'Z1' : Z1 , 'A1' : A1 , 'Z2' : Z2 , 'A2' : A2 , 'Z3' : Z3 , 'A3' : A3 } return A3 , cache 1.3 Activation Functions \u00b6 Activation functions introduce non-linearity, enabling neural networks to learn complex patterns. ReLU (Rectified Linear Unit) f(x) = max(0, x) f'(x) = 1 if x > 0, else 0 Advantages: - Computationally efficient - Helps mitigate vanishing gradient problem - Induces sparsity (many neurons output 0) - Default choice for hidden layers Disadvantages: - Dead ReLU problem (neurons stuck at 0) - Not zero-centered def relu ( x ): return np . maximum ( 0 , x ) def relu_derivative ( x ): return ( x > 0 ) . astype ( float ) Visualization Concept: | / | / | / | / ___|/_________ 0 Leaky ReLU f(x) = max(0.01x, x) Advantages: - Prevents dead neurons - Small gradient for negative values def leaky_relu ( x , alpha = 0.01 ): return np . where ( x > 0 , x , alpha * x ) Sigmoid f(x) = 1 / (1 + e^(-x)) f'(x) = f(x) \u00d7 (1 - f(x)) Use Cases: - Binary classification output layer - Gate mechanisms in LSTM/GRU Disadvantages: - Vanishing gradient problem - Not zero-centered - Computationally expensive def sigmoid ( x ): return 1 / ( 1 + np . exp ( - x )) def sigmoid_derivative ( x ): s = sigmoid ( x ) return s * ( 1 - s ) Tanh (Hyperbolic Tangent) f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) f'(x) = 1 - f(x)\u00b2 Advantages: - Zero-centered (better than sigmoid) - Stronger gradients than sigmoid def tanh ( x ): return np . tanh ( x ) def tanh_derivative ( x ): return 1 - np . tanh ( x ) ** 2 Softmax (Multi-Class Output) f(x_i) = e^(x_i) / \u03a3(e^(x_j)) Properties: - Outputs sum to 1 (probability distribution) - Used for multi-class classification def softmax ( x ): exp_x = np . exp ( x - np . max ( x , axis = 0 , keepdims = True )) # Numerical stability return exp_x / np . sum ( exp_x , axis = 0 , keepdims = True ) Activation Function Selection Guide: Layer Type Recommended Activation Reason Hidden layers (general) ReLU Fast, effective, prevents vanishing gradients Hidden layers (negative values important) Leaky ReLU / ELU Allows negative activations Output (regression) Linear Unrestricted output range Output (binary classification) Sigmoid Output in [0, 1] range Output (multi-class) Softmax Probability distribution Recurrent networks Tanh Zero-centered, bounded 1.4 Loss Functions and Backpropagation \u00b6 Loss Functions quantify how well the model\u2019s predictions match the true values. Mean Squared Error (MSE) - Regression L(y, \u0177) = (1/n) \u00d7 \u03a3(y_i - \u0177_i)\u00b2 Characteristics: - Penalizes large errors more heavily - Sensitive to outliers - Smooth gradient def mse_loss ( y_true , y_pred ): return np . mean (( y_true - y_pred ) ** 2 ) def mse_derivative ( y_true , y_pred ): return 2 * ( y_pred - y_true ) / y_true . shape [ 0 ] Mean Absolute Error (MAE) - Robust Regression L(y, \u0177) = (1/n) \u00d7 \u03a3|y_i - \u0177_i| Advantages: - Less sensitive to outliers - All errors weighted equally def mae_loss ( y_true , y_pred ): return np . mean ( np . abs ( y_true - y_pred )) Binary Cross-Entropy - Binary Classification L(y, \u0177) = -(1/n) \u00d7 \u03a3[y_i \u00d7 log(\u0177_i) + (1-y_i) \u00d7 log(1-\u0177_i)] Used when: - Predicting probability of single class - Output: sigmoid activation def binary_crossentropy ( y_true , y_pred ): epsilon = 1e-15 # Prevent log(0) y_pred = np . clip ( y_pred , epsilon , 1 - epsilon ) return - np . mean ( y_true * np . log ( y_pred ) + ( 1 - y_true ) * np . log ( 1 - y_pred )) Categorical Cross-Entropy - Multi-Class Classification L(y, \u0177) = -(1/n) \u00d7 \u03a3 \u03a3 y_ij \u00d7 log(\u0177_ij) Used when: - Multiple mutually exclusive classes - Output: softmax activation def categorical_crossentropy ( y_true , y_pred ): epsilon = 1e-15 y_pred = np . clip ( y_pred , epsilon , 1 - epsilon ) return - np . sum ( y_true * np . log ( y_pred )) / y_true . shape [ 0 ] Backpropagation Algorithm Backpropagation computes gradients of the loss with respect to all parameters using the chain rule. Algorithm Steps: Forward Pass : Compute predictions and loss Output Layer Gradient : \u2202L/\u2202a^[L] Backward Pass : Compute gradients layer by layer Parameter Update : Update weights and biases Mathematical Derivation: For layer l: dZ^[l] = dA^[l] \u2299 f'(Z^[l]) dW^[l] = (1/m) \u00d7 dZ^[l] \u00d7 A^[l-1]^T db^[l] = (1/m) \u00d7 \u03a3 dZ^[l] dA^[l-1] = W^[l]^T \u00d7 dZ^[l] Where: - \u2299 represents element-wise multiplication - f' is the derivative of activation function - m is the number of training examples Complete Backpropagation Implementation: def backward_propagation ( X , Y , cache , parameters ): \"\"\" Implements backpropagation for 3-layer network Args: X: input features Y: true labels cache: forward propagation cache parameters: model parameters (W1, b1, W2, b2, W3, b3) Returns: gradients: dictionary containing dW1, db1, dW2, db2, dW3, db3 \"\"\" m = X . shape [ 1 ] # Retrieve cached values A1 , A2 , A3 = cache [ 'A1' ], cache [ 'A2' ], cache [ 'A3' ] Z1 , Z2 = cache [ 'Z1' ], cache [ 'Z2' ] # Output layer (Layer 3) dZ3 = A3 - Y # For MSE loss with linear activation dW3 = ( 1 / m ) * np . dot ( dZ3 , A2 . T ) db3 = ( 1 / m ) * np . sum ( dZ3 , axis = 1 , keepdims = True ) # Hidden layer 2 dA2 = np . dot ( parameters [ 'W3' ] . T , dZ3 ) dZ2 = dA2 * relu_derivative ( Z2 ) dW2 = ( 1 / m ) * np . dot ( dZ2 , A1 . T ) db2 = ( 1 / m ) * np . sum ( dZ2 , axis = 1 , keepdims = True ) # Hidden layer 1 dA1 = np . dot ( parameters [ 'W2' ] . T , dZ2 ) dZ1 = dA1 * relu_derivative ( Z1 ) dW1 = ( 1 / m ) * np . dot ( dZ1 , X . T ) db1 = ( 1 / m ) * np . sum ( dZ1 , axis = 1 , keepdims = True ) gradients = { 'dW1' : dW1 , 'db1' : db1 , 'dW2' : dW2 , 'db2' : db2 , 'dW3' : dW3 , 'db3' : db3 } return gradients 1.5 Optimization Algorithms \u00b6 Optimization algorithms update model parameters to minimize the loss function. Gradient Descent (Batch) Updates parameters using the entire dataset: W := W - \u03b1 \u00d7 \u2202L/\u2202W b := b - \u03b1 \u00d7 \u2202L/\u2202b Where \u03b1 is the learning rate Pros : Stable convergence, exact gradient Cons : Slow for large datasets, memory intensive def gradient_descent ( parameters , gradients , learning_rate ): \"\"\" Update parameters using batch gradient descent \"\"\" for key in parameters . keys (): parameters [ key ] -= learning_rate * gradients [ 'd' + key ] return parameters Stochastic Gradient Descent (SGD) Updates parameters using one sample at a time: W := W - \u03b1 \u00d7 \u2202L_i/\u2202W (for single sample i) Pros : Fast updates, can escape local minima Cons : Noisy updates, unstable convergence Mini-Batch Gradient Descent Compromise between batch and stochastic (typically 32-256 samples): def mini_batch_gradient_descent ( X , Y , parameters , batch_size = 32 , learning_rate = 0.01 ): \"\"\" Mini-batch gradient descent implementation \"\"\" m = X . shape [ 1 ] num_batches = m // batch_size for i in range ( num_batches ): # Get mini-batch start = i * batch_size end = start + batch_size X_batch = X [:, start : end ] Y_batch = Y [:, start : end ] # Forward propagation A3 , cache = forward_propagation ( X_batch , parameters ) # Backward propagation gradients = backward_propagation ( X_batch , Y_batch , cache , parameters ) # Update parameters parameters = gradient_descent ( parameters , gradients , learning_rate ) return parameters Momentum Accelerates SGD by accumulating velocity in relevant direction: v := \u03b2 \u00d7 v + (1-\u03b2) \u00d7 \u2202L/\u2202W W := W - \u03b1 \u00d7 v Common \u03b2 values: 0.9, 0.99 Benefits: - Smooths out oscillations - Faster convergence - Better navigation of ravines def momentum_optimizer ( parameters , gradients , velocity , beta = 0.9 , learning_rate = 0.01 ): \"\"\" Parameters update with momentum \"\"\" for key in parameters . keys (): # Update velocity velocity [ 'v' + key ] = beta * velocity [ 'v' + key ] + ( 1 - beta ) * gradients [ 'd' + key ] # Update parameters parameters [ key ] -= learning_rate * velocity [ 'v' + key ] return parameters , velocity RMSprop (Root Mean Square Propagation) Adapts learning rate per parameter based on recent gradients: s := \u03b2 \u00d7 s + (1-\u03b2) \u00d7 (\u2202L/\u2202W)\u00b2 W := W - \u03b1 \u00d7 (\u2202L/\u2202W) / \u221a(s + \u03b5) Where \u03b5 is a small constant for numerical stability (typically 1e-8) Benefits: - Adaptive learning rates - Works well for non-stationary objectives - Good for RNNs def rmsprop_optimizer ( parameters , gradients , cache , beta = 0.999 , learning_rate = 0.001 , epsilon = 1e-8 ): \"\"\" RMSprop optimization \"\"\" for key in parameters . keys (): # Update cache (squared gradients) cache [ 's' + key ] = beta * cache [ 's' + key ] + ( 1 - beta ) * gradients [ 'd' + key ] ** 2 # Update parameters parameters [ key ] -= learning_rate * gradients [ 'd' + key ] / ( np . sqrt ( cache [ 's' + key ]) + epsilon ) return parameters , cache Adam (Adaptive Moment Estimation) Combines momentum and RMSprop: m := \u03b2\u2081 \u00d7 m + (1-\u03b2\u2081) \u00d7 \u2202L/\u2202W (momentum) v := \u03b2\u2082 \u00d7 v + (1-\u03b2\u2082) \u00d7 (\u2202L/\u2202W)\u00b2 (RMSprop) m\u0302 := m / (1-\u03b2\u2081^t) (bias correction) v\u0302 := v / (1-\u03b2\u2082^t) (bias correction) W := W - \u03b1 \u00d7 m\u0302 / (\u221av\u0302 + \u03b5) Common values: \u03b2\u2081=0.9, \u03b2\u2082=0.999, \u03b5=1e-8 Benefits: - Most popular optimizer for deep learning - Works well with sparse gradients - Combines benefits of momentum and RMSprop - Bias correction for initialization def adam_optimizer ( parameters , gradients , adam_cache , t , beta1 = 0.9 , beta2 = 0.999 , learning_rate = 0.001 , epsilon = 1e-8 ): \"\"\" Adam optimization with bias correction Args: t: iteration number (for bias correction) \"\"\" for key in parameters . keys (): # Update momentum adam_cache [ 'm' + key ] = beta1 * adam_cache [ 'm' + key ] + ( 1 - beta1 ) * gradients [ 'd' + key ] # Update RMSprop adam_cache [ 'v' + key ] = beta2 * adam_cache [ 'v' + key ] + ( 1 - beta2 ) * gradients [ 'd' + key ] ** 2 # Bias correction m_corrected = adam_cache [ 'm' + key ] / ( 1 - beta1 ** t ) v_corrected = adam_cache [ 'v' + key ] / ( 1 - beta2 ** t ) # Update parameters parameters [ key ] -= learning_rate * m_corrected / ( np . sqrt ( v_corrected ) + epsilon ) return parameters , adam_cache AdamW (Adam with Weight Decay) Adam with decoupled weight decay regularization: W := W - \u03b1 \u00d7 (m\u0302 / (\u221av\u0302 + \u03b5) + \u03bb \u00d7 W) Where \u03bb is the weight decay coefficient Benefits: - Better generalization than Adam - Proper weight decay implementation - State-of-the-art for many tasks # Using PyTorch implementation import torch.optim as optim optimizer = optim . AdamW ( model . parameters (), lr = 0.001 , weight_decay = 0.01 ) Optimizer Selection Guide: Scenario Recommended Optimizer Learning Rate General purpose / starting point Adam 1e-3 Large dataset, need speed SGD with momentum 1e-2 (with decay) RNNs / sequence models Adam or RMSprop 1e-3 to 1e-4 Fine-tuning pretrained models AdamW 1e-5 to 1e-4 Non-stationary problems RMSprop 1e-3 When overfitting occurs AdamW or SGD Lower rates Learning Rate Scheduling: # Learning rate decay def lr_decay ( initial_lr , epoch , decay_rate = 0.95 ): return initial_lr * ( decay_rate ** epoch ) # Step decay def step_decay ( initial_lr , epoch , drop = 0.5 , epochs_drop = 10 ): return initial_lr * ( drop ** np . floor ( epoch / epochs_drop )) # Cosine annealing def cosine_annealing ( initial_lr , epoch , total_epochs ): return initial_lr * 0.5 * ( 1 + np . cos ( np . pi * epoch / total_epochs )) 2. Feedforward Neural Networks \u00b6 2.1 Architecture Design \u00b6 A feedforward neural network (FNN) is the simplest type of artificial neural network where information moves in only one direction\u2014forward\u2014from input to output. Design Principles: Layer Size Guidelines: - Input layer : Size = number of molecular features (e.g., 2048 for Morgan fingerprints) - Hidden layers : Start with 128-256 neurons, gradually decrease - Output layer : - Regression: 1 neuron - Binary classification: 1 neuron - Multi-class: Number of classes Architecture Patterns: Pattern 1: Pyramid Structure (Recommended for most molecular tasks) Input (2048) \u2192 Hidden1 (512) \u2192 Hidden2 (256) \u2192 Hidden3 (128) \u2192 Output (1) - Progressively reduces dimensionality - Learns hierarchical features Pattern 2: Hourglass Structure Input (2048) \u2192 Hidden1 (256) \u2192 Hidden2 (128) \u2192 Hidden3 (256) \u2192 Output (1) - Creates bottleneck representation - Useful for learning compressed features Pattern 3: Constant Width Input (2048) \u2192 Hidden1 (256) \u2192 Hidden2 (256) \u2192 Hidden3 (256) \u2192 Output (1) - Maintains capacity throughout - Good for complex non-linear mappings Depth vs Width Trade-off: Architecture Pros Cons Best For Deep & Narrow (5+ layers, 64-128 neurons) Hierarchical features, fewer parameters Harder to train, vanishing gradients Complex patterns, large datasets Shallow & Wide (2-3 layers, 512+ neurons) Easier to train, stable More parameters, less hierarchical Simple patterns, small datasets Balanced (3-4 layers, 128-256 neurons) Good trade-off - General purpose, recommended starting point Complete Architecture Implementation: import torch import torch.nn as nn import torch.nn.functional as F class MolecularFNN ( nn . Module ): \"\"\" Feedforward Neural Network for molecular property prediction \"\"\" def __init__ ( self , input_dim = 2048 , hidden_dims = [ 512 , 256 , 128 ], output_dim = 1 , dropout_rate = 0.3 ): \"\"\" Args: input_dim: Size of input features (e.g., fingerprint length) hidden_dims: List of hidden layer sizes output_dim: Number of output neurons (1 for regression) dropout_rate: Dropout probability for regularization \"\"\" super ( MolecularFNN , self ) . __init__ () # Build layers dynamically layers = [] prev_dim = input_dim for hidden_dim in hidden_dims : layers . append ( nn . Linear ( prev_dim , hidden_dim )) layers . append ( nn . BatchNorm1d ( hidden_dim )) # Batch normalization layers . append ( nn . ReLU ()) layers . append ( nn . Dropout ( dropout_rate )) prev_dim = hidden_dim # Output layer layers . append ( nn . Linear ( prev_dim , output_dim )) self . network = nn . Sequential ( * layers ) # Initialize weights self . _initialize_weights () def _initialize_weights ( self ): \"\"\"He initialization for ReLU networks\"\"\" for m in self . modules (): if isinstance ( m , nn . Linear ): nn . init . kaiming_normal_ ( m . weight , mode = 'fan_in' , nonlinearity = 'relu' ) if m . bias is not None : nn . init . constant_ ( m . bias , 0 ) def forward ( self , x ): \"\"\" Forward pass Args: x: Input tensor of shape (batch_size, input_dim) Returns: Output predictions of shape (batch_size, output_dim) \"\"\" return self . network ( x ) def get_embeddings ( self , x ): \"\"\" Extract learned representations from second-to-last layer Useful for visualization and transfer learning \"\"\" for layer in self . network [: - 1 ]: # All layers except final x = layer ( x ) return x # Example instantiation model = MolecularFNN ( input_dim = 2048 , # Morgan fingerprint size hidden_dims = [ 512 , 256 , 128 ], output_dim = 1 , # Single regression output dropout_rate = 0.3 ) print ( model ) print ( f \" \\n Total parameters: { sum ( p . numel () for p in model . parameters ()) : , } \" ) 2.2 Full Training Example \u00b6 Complete Training Pipeline: import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error from rdkit import Chem from rdkit.Chem import AllChem import pandas as pd from tqdm import tqdm # ============================================================================ # 1. DATA PREPARATION # ============================================================================ class MolecularDataset ( Dataset ): \"\"\"Custom Dataset for molecular data\"\"\" def __init__ ( self , smiles_list , labels , radius = 2 , n_bits = 2048 ): \"\"\" Args: smiles_list: List of SMILES strings labels: Array of target values radius: Morgan fingerprint radius n_bits: Fingerprint length \"\"\" self . fingerprints = [] self . labels = [] for smiles , label in zip ( smiles_list , labels ): mol = Chem . MolFromSmiles ( smiles ) if mol is not None : # Generate Morgan fingerprint fp = AllChem . GetMorganFingerprintAsBitVect ( mol , radius , nBits = n_bits ) self . fingerprints . append ( np . array ( fp )) self . labels . append ( label ) self . fingerprints = torch . FloatTensor ( np . array ( self . fingerprints )) self . labels = torch . FloatTensor ( np . array ( self . labels )) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . fingerprints ) def __getitem__ ( self , idx ): return self . fingerprints [ idx ], self . labels [ idx ] # ============================================================================ # 2. TRAINING FUNCTION # ============================================================================ def train_epoch ( model , train_loader , criterion , optimizer , device ): \"\"\"Train for one epoch\"\"\" model . train () total_loss = 0 for batch_x , batch_y in train_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) # Forward pass optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) # Backward pass loss . backward () optimizer . step () total_loss += loss . item () return total_loss / len ( train_loader ) # ============================================================================ # 3. VALIDATION FUNCTION # ============================================================================ def validate ( model , val_loader , criterion , device ): \"\"\"Validate the model\"\"\" model . eval () total_loss = 0 all_predictions = [] all_labels = [] with torch . no_grad (): for batch_x , batch_y in val_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) total_loss += loss . item () all_predictions . extend ( predictions . cpu () . numpy ()) all_labels . extend ( batch_y . cpu () . numpy ()) avg_loss = total_loss / len ( val_loader ) # Calculate metrics all_predictions = np . array ( all_predictions ) . flatten () all_labels = np . array ( all_labels ) . flatten () rmse = np . sqrt ( mean_squared_error ( all_labels , all_predictions )) mae = mean_absolute_error ( all_labels , all_predictions ) r2 = r2_score ( all_labels , all_predictions ) return avg_loss , rmse , mae , r2 # ============================================================================ # 4. COMPLETE TRAINING PIPELINE # ============================================================================ def train_molecular_model ( smiles_train , y_train , smiles_val , y_val , config = None ): \"\"\" Complete training pipeline for molecular property prediction Args: smiles_train: Training SMILES y_train: Training labels smiles_val: Validation SMILES y_val: Validation labels config: Configuration dictionary Returns: trained_model: Best model history: Training history \"\"\" # Default configuration if config is None : config = { 'input_dim' : 2048 , 'hidden_dims' : [ 512 , 256 , 128 ], 'output_dim' : 1 , 'dropout_rate' : 0.3 , 'learning_rate' : 0.001 , 'batch_size' : 64 , 'epochs' : 100 , 'patience' : 15 , 'device' : 'cuda' if torch . cuda . is_available () else 'cpu' } device = torch . device ( config [ 'device' ]) print ( f \"Using device: { device } \" ) # Create datasets print ( \"Creating datasets...\" ) train_dataset = MolecularDataset ( smiles_train , y_train ) val_dataset = MolecularDataset ( smiles_val , y_val ) # Create data loaders train_loader = DataLoader ( train_dataset , batch_size = config [ 'batch_size' ], shuffle = True , num_workers = 0 ) val_loader = DataLoader ( val_dataset , batch_size = config [ 'batch_size' ], shuffle = False , num_workers = 0 ) # Initialize model model = MolecularFNN ( input_dim = config [ 'input_dim' ], hidden_dims = config [ 'hidden_dims' ], output_dim = config [ 'output_dim' ], dropout_rate = config [ 'dropout_rate' ] ) . to ( device ) # Loss and optimizer criterion = nn . MSELoss () optimizer = optim . Adam ( model . parameters (), lr = config [ 'learning_rate' ]) # Learning rate scheduler scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = 'min' , factor = 0.5 , patience = 5 , verbose = True ) # Training history history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : [], 'val_mae' : [], 'val_r2' : [] } # Early stopping best_val_loss = float ( 'inf' ) patience_counter = 0 best_model_state = None # Training loop print ( \" \\n Starting training...\" ) for epoch in range ( config [ 'epochs' ]): # Train train_loss = train_epoch ( model , train_loader , criterion , optimizer , device ) # Validate val_loss , val_rmse , val_mae , val_r2 = validate ( model , val_loader , criterion , device ) # Learning rate scheduling scheduler . step ( val_loss ) # Save history history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_rmse' ] . append ( val_rmse ) history [ 'val_mae' ] . append ( val_mae ) history [ 'val_r2' ] . append ( val_r2 ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } / { config [ 'epochs' ] } \" ) print ( f \" Train Loss: { train_loss : .4f } \" ) print ( f \" Val Loss: { val_loss : .4f } , RMSE: { val_rmse : .4f } , \" f \"MAE: { val_mae : .4f } , R\u00b2: { val_r2 : .4f } \" ) # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 best_model_state = model . state_dict () . copy () else : patience_counter += 1 if patience_counter >= config [ 'patience' ]: print ( f \" \\n Early stopping at epoch { epoch + 1 } \" ) break # Load best model model . load_state_dict ( best_model_state ) print ( \" \\n Training complete!\" ) print ( f \"Best validation loss: { best_val_loss : .4f } \" ) return model , history # ============================================================================ # 5. EXAMPLE USAGE # ============================================================================ # Generate synthetic data for demonstration def generate_synthetic_data ( n_samples = 1000 ): \"\"\"Generate synthetic molecular data\"\"\" np . random . seed ( 42 ) # Simple SMILES for demonstration (in practice, use real dataset) base_smiles = [ 'CCO' , 'CC(C)O' , 'CCCO' , 'CC(C)CO' , 'CCCCO' ] smiles_list = np . random . choice ( base_smiles , n_samples ) # Synthetic labels (in practice, use real property values) labels = np . random . randn ( n_samples ) * 2 + 5 return smiles_list , labels # Generate data smiles , labels = generate_synthetic_data ( 1000 ) # Split data smiles_train , smiles_temp , y_train , y_temp = train_test_split ( smiles , labels , test_size = 0.3 , random_state = 42 ) smiles_val , smiles_test , y_val , y_test = train_test_split ( smiles_temp , y_temp , test_size = 0.5 , random_state = 42 ) # Train model model , history = train_molecular_model ( smiles_train , y_train , smiles_val , y_val ) # Visualize training history import matplotlib.pyplot as plt plt . figure ( figsize = ( 12 , 4 )) plt . subplot ( 1 , 3 , 1 ) plt . plot ( history [ 'train_loss' ], label = 'Train Loss' ) plt . plot ( history [ 'val_loss' ], label = 'Val Loss' ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . title ( 'Training and Validation Loss' ) plt . subplot ( 1 , 3 , 2 ) plt . plot ( history [ 'val_rmse' ], label = 'RMSE' ) plt . plot ( history [ 'val_mae' ], label = 'MAE' ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Error' ) plt . legend () plt . title ( 'Validation Errors' ) plt . subplot ( 1 , 3 , 3 ) plt . plot ( history [ 'val_r2' ]) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'R\u00b2 Score' ) plt . title ( 'Validation R\u00b2' ) plt . tight_layout () plt . savefig ( 'training_history.png' , dpi = 150 , bbox_inches = 'tight' ) plt . show () 2.3 Best Practices and Tips \u00b6 1. Data Preprocessing Normalize inputs : Scale features to similar ranges from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_val_scaled = scaler . transform ( X_val ) Handle missing values : Remove or impute before training Remove duplicates : Ensure no data leakage between train/val/test sets 2. Architecture Selection Start simple : Begin with 2-3 hidden layers Increase gradually : Add complexity only if needed Monitor overfitting : If train loss << val loss, model too complex 3. Hyperparameter Tuning Priority Priority Hyperparameter Impact Typical Range HIGH Learning rate Critical for convergence 1e-4 to 1e-2 HIGH Batch size Memory and convergence speed 32, 64, 128, 256 MEDIUM Number of layers Model capacity 2-5 MEDIUM Neurons per layer Model capacity 64, 128, 256, 512 MEDIUM Dropout rate Regularization 0.1-0.5 LOW Optimizer Usually Adam works Adam, AdamW LOW Activation function ReLU usually best ReLU, Leaky ReLU 4. Regularization Techniques # L2 Regularization (Weight Decay) optimizer = optim . Adam ( model . parameters (), lr = 0.001 , weight_decay = 1e-5 ) # Dropout (already in model architecture) nn . Dropout ( p = 0.3 ) # Batch Normalization (already in model architecture) nn . BatchNorm1d ( hidden_dim ) # Early Stopping (implemented in training loop) if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 else : patience_counter += 1 5. Debugging Checklist Problem : Loss is NaN Solution : Reduce learning rate, check for invalid inputs, add gradient clipping torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 1.0 ) Problem : Loss not decreasing Solution : Increase learning rate, check data preprocessing, verify labels Problem : Training loss decreasing but validation loss increasing Solution : Overfitting - increase dropout, add L2 regularization, reduce model size Problem : Both losses high and not improving Solution : Underfitting - increase model capacity, decrease regularization, train longer 6. Monitoring Training # Use TensorBoard for real-time monitoring from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter ( 'runs/experiment_1' ) # During training loop writer . add_scalar ( 'Loss/train' , train_loss , epoch ) writer . add_scalar ( 'Loss/val' , val_loss , epoch ) writer . add_scalar ( 'Metrics/RMSE' , val_rmse , epoch ) writer . add_scalar ( 'Metrics/R2' , val_r2 , epoch ) # View with: tensorboard --logdir=runs 7. Model Saving and Loading # Save complete model state torch . save ({ 'epoch' : epoch , 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (), 'loss' : best_val_loss , 'history' : history }, 'best_model.pth' ) # Load model checkpoint = torch . load ( 'best_model.pth' ) model . load_state_dict ( checkpoint [ 'model_state_dict' ]) optimizer . load_state_dict ( checkpoint [ 'optimizer_state_dict' ]) 8. Tips for Molecular Data Use appropriate fingerprints : Morgan (most common), MACCS, RDKit fingerprints Consider molecular size : Normalize by molecular weight or atom count if relevant Handle invalid SMILES : Filter out molecules that RDKit cannot parse Augmentation : Consider SMILES enumeration for data augmentation # SMILES enumeration for augmentation from rdkit import Chem def enumerate_smiles ( smiles , n_variants = 5 ): \"\"\"Generate different SMILES representations of same molecule\"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return [ smiles ] variants = [] for _ in range ( n_variants ): variants . append ( Chem . MolToSmiles ( mol , doRandom = True )) return list ( set ( variants )) 3. Multi-Task Learning \u00b6 3.1 Why and When to Use Multi-Task Learning \u00b6 Multi-task learning (MTL) is a machine learning paradigm where a model simultaneously learns multiple related tasks, sharing representations between tasks. Key Benefits: Improved Generalization : Shared representations act as implicit regularization Data Efficiency : Tasks with more data help tasks with less data Related Features : Capture common patterns across molecular properties Transfer Learning : Learn general molecular representations Computational Efficiency : One model predicts multiple properties When to Use MTL: Ideal Scenarios: - Related Properties : Predicting solubility, LogP, and permeability (all related to molecular polarity) - Limited Data : Some tasks have abundant data, others have scarce data - Shared Features : Tasks depend on similar molecular features - Multiple Endpoints : Drug discovery (ADMET properties all relevant) Not Recommended: - Unrelated Tasks : Predicting solubility and catalytic activity (different mechanisms) - Conflicting Objectives : Tasks requiring opposite feature representations - Single Task is Sufficient : When only one property matters Examples in Drug Discovery: Task Group 1: ADMET Properties \u251c\u2500 Absorption (Caco-2 permeability) \u251c\u2500 Distribution (BBB permeability, plasma protein binding) \u251c\u2500 Metabolism (CYP450 inhibition, metabolic stability) \u251c\u2500 Excretion (clearance, half-life) \u2514\u2500 Toxicity (hERG inhibition, hepatotoxicity) Task Group 2: Physical Properties \u251c\u2500 Solubility (aqueous, LogS) \u251c\u2500 Lipophilicity (LogP, LogD) \u2514\u2500 Permeability (PAMPA, Caco-2) Task Group 3: Biological Activity \u251c\u2500 Target binding (IC50, Ki) \u251c\u2500 Cell viability (CC50) \u2514\u2500 Selectivity across targets 3.2 Multi-Task Architecture \u00b6 Hard Parameter Sharing (Most Common) All tasks share hidden layers, separate output heads: Input (Molecular Features) | Shared Hidden Layers / | \\ Task 1 Task 2 Task 3 Output Output Output Complete Implementation: import torch import torch.nn as nn class MultiTaskMolecularModel ( nn . Module ): \"\"\" Multi-task neural network for molecular property prediction \"\"\" def __init__ ( self , input_dim = 2048 , shared_dims = [ 512 , 256 ], task_configs = None , dropout_rate = 0.3 ): \"\"\" Args: input_dim: Size of input features shared_dims: List of shared hidden layer sizes task_configs: List of dicts with task specifications [{'name': 'task1', 'output_dim': 1, 'task_type': 'regression'}, ...] dropout_rate: Dropout probability \"\"\" super ( MultiTaskMolecularModel , self ) . __init__ () if task_configs is None : task_configs = [ { 'name' : 'solubility' , 'output_dim' : 1 , 'task_type' : 'regression' }, { 'name' : 'toxicity' , 'output_dim' : 1 , 'task_type' : 'classification' } ] self . task_configs = task_configs self . task_names = [ task [ 'name' ] for task in task_configs ] # Shared layers shared_layers = [] prev_dim = input_dim for hidden_dim in shared_dims : shared_layers . extend ([ nn . Linear ( prev_dim , hidden_dim ), nn . BatchNorm1d ( hidden_dim ), nn . ReLU (), nn . Dropout ( dropout_rate ) ]) prev_dim = hidden_dim self . shared_network = nn . Sequential ( * shared_layers ) # Task-specific heads self . task_heads = nn . ModuleDict () for task in task_configs : task_name = task [ 'name' ] output_dim = task [ 'output_dim' ] # Task-specific layers (2 layers) task_head = nn . Sequential ( nn . Linear ( prev_dim , 128 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 128 , output_dim ) ) self . task_heads [ task_name ] = task_head def forward ( self , x ): \"\"\" Forward pass through shared network and all task heads Args: x: Input tensor (batch_size, input_dim) Returns: Dictionary of predictions for each task \"\"\" # Shared representations shared_features = self . shared_network ( x ) # Task-specific predictions outputs = {} for task_name in self . task_names : outputs [ task_name ] = self . task_heads [ task_name ]( shared_features ) return outputs def get_shared_features ( self , x ): \"\"\"Extract shared representations for visualization or transfer learning\"\"\" return self . shared_network ( x ) # Example instantiation task_configs = [ { 'name' : 'solubility' , 'output_dim' : 1 , 'task_type' : 'regression' }, { 'name' : 'bbb_permeability' , 'output_dim' : 1 , 'task_type' : 'regression' }, { 'name' : 'toxicity' , 'output_dim' : 1 , 'task_type' : 'classification' }, { 'name' : 'cyp450_inhibition' , 'output_dim' : 5 , 'task_type' : 'multi-class' } ] model = MultiTaskMolecularModel ( input_dim = 2048 , shared_dims = [ 512 , 256 ], task_configs = task_configs , dropout_rate = 0.3 ) print ( model ) Soft Parameter Sharing (Advanced) Each task has its own network, but networks are constrained to be similar: class SoftParameterSharingModel ( nn . Module ): \"\"\" Soft parameter sharing: separate networks with similarity constraints \"\"\" def __init__ ( self , input_dim , hidden_dims , num_tasks ): super ( SoftParameterSharingModel , self ) . __init__ () # Create separate networks for each task self . task_networks = nn . ModuleList ([ self . _build_network ( input_dim , hidden_dims ) for _ in range ( num_tasks ) ]) def _build_network ( self , input_dim , hidden_dims ): layers = [] prev_dim = input_dim for hidden_dim in hidden_dims : layers . extend ([ nn . Linear ( prev_dim , hidden_dim ), nn . ReLU () ]) prev_dim = hidden_dim layers . append ( nn . Linear ( prev_dim , 1 )) return nn . Sequential ( * layers ) def forward ( self , x ): outputs = [] for network in self . task_networks : outputs . append ( network ( x )) return torch . cat ( outputs , dim = 1 ) def compute_similarity_loss ( self , lambda_reg = 0.01 ): \"\"\" Regularization term to keep task networks similar \"\"\" similarity_loss = 0 num_tasks = len ( self . task_networks ) for i in range ( num_tasks ): for j in range ( i + 1 , num_tasks ): # L2 distance between parameters for p1 , p2 in zip ( self . task_networks [ i ] . parameters (), self . task_networks [ j ] . parameters ()): similarity_loss += torch . norm ( p1 - p2 , p = 2 ) return lambda_reg * similarity_loss 3.3 Training Strategies \u00b6 Multi-Task Loss Function: def compute_multitask_loss ( outputs , labels , task_configs , task_weights = None ): \"\"\" Compute weighted combination of task-specific losses Args: outputs: Dict of predictions {task_name: predictions} labels: Dict of true labels {task_name: labels} task_configs: List of task configurations task_weights: Dict of loss weights {task_name: weight} Returns: total_loss: Combined loss task_losses: Dict of individual task losses \"\"\" if task_weights is None : task_weights = { task [ 'name' ]: 1.0 for task in task_configs } task_losses = {} total_loss = 0 for task in task_configs : task_name = task [ 'name' ] task_type = task [ 'task_type' ] # Skip if no labels for this task in batch if task_name not in labels or labels [ task_name ] is None : continue pred = outputs [ task_name ] true = labels [ task_name ] # Select appropriate loss function if task_type == 'regression' : loss = nn . MSELoss ()( pred , true ) elif task_type == 'classification' : loss = nn . BCEWithLogitsLoss ()( pred , true ) elif task_type == 'multi-class' : loss = nn . CrossEntropyLoss ()( pred , true ) else : raise ValueError ( f \"Unknown task type: { task_type } \" ) task_losses [ task_name ] = loss . item () total_loss += task_weights [ task_name ] * loss return total_loss , task_losses Complete Training Loop: def train_multitask_model ( model , train_loader , val_loader , task_configs , num_epochs = 100 , device = 'cuda' ): \"\"\" Complete multi-task training pipeline \"\"\" model = model . to ( device ) optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , patience = 5 ) # Initialize task weights (can be learned or fixed) task_weights = { task [ 'name' ]: 1.0 for task in task_configs } history = { 'train_loss' : [], 'val_loss' : [], 'task_losses' : {}} best_val_loss = float ( 'inf' ) for epoch in range ( num_epochs ): # Training model . train () train_loss = 0 train_task_losses = { task [ 'name' ]: 0 for task in task_configs } for batch_x , batch_labels in train_loader : batch_x = batch_x . to ( device ) batch_labels = { k : v . to ( device ) if v is not None else None for k , v in batch_labels . items ()} optimizer . zero_grad () outputs = model ( batch_x ) loss , task_losses = compute_multitask_loss ( outputs , batch_labels , task_configs , task_weights ) loss . backward () optimizer . step () train_loss += loss . item () for task_name , task_loss in task_losses . items (): train_task_losses [ task_name ] += task_loss # Validation model . eval () val_loss = 0 val_task_losses = { task [ 'name' ]: 0 for task in task_configs } with torch . no_grad (): for batch_x , batch_labels in val_loader : batch_x = batch_x . to ( device ) batch_labels = { k : v . to ( device ) if v is not None else None for k , v in batch_labels . items ()} outputs = model ( batch_x ) loss , task_losses = compute_multitask_loss ( outputs , batch_labels , task_configs , task_weights ) val_loss += loss . item () for task_name , task_loss in task_losses . items (): val_task_losses [ task_name ] += task_loss # Average losses train_loss /= len ( train_loader ) val_loss /= len ( val_loader ) # Learning rate scheduling scheduler . step ( val_loss ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } / { num_epochs } \" ) print ( f \" Train Loss: { train_loss : .4f } , Val Loss: { val_loss : .4f } \" ) for task in task_configs : task_name = task [ 'name' ] print ( f \" { task_name } : { val_task_losses [ task_name ] / len ( val_loader ) : .4f } \" ) # Save best model if val_loss < best_val_loss : best_val_loss = val_loss torch . save ( model . state_dict (), 'best_multitask_model.pth' ) return model , history 3.4 Task Balancing Techniques \u00b6 Balancing multiple tasks is crucial for effective multi-task learning. 1. Manual Weight Tuning # Simple fixed weights task_weights = { 'solubility' : 1.0 , 'bbb_permeability' : 2.0 , # Prioritize this task 'toxicity' : 1.5 , 'cyp450_inhibition' : 1.0 } 2. Uncertainty Weighting (Kendall et al., 2018) Learns task weights based on homoscedastic uncertainty: class MultiTaskLossWithUncertainty ( nn . Module ): \"\"\" Automatically learns task weights based on uncertainty \"\"\" def __init__ ( self , num_tasks ): super ( MultiTaskLossWithUncertainty , self ) . __init__ () # Log variance for each task (learned parameters) self . log_vars = nn . Parameter ( torch . zeros ( num_tasks )) def forward ( self , losses ): \"\"\" Args: losses: List of individual task losses Returns: Weighted total loss \"\"\" total_loss = 0 for i , loss in enumerate ( losses ): precision = torch . exp ( - self . log_vars [ i ]) total_loss += precision * loss + self . log_vars [ i ] return total_loss # Usage in training uncertainty_loss = MultiTaskLossWithUncertainty ( num_tasks = 4 ) optimizer = optim . Adam ( list ( model . parameters ()) + list ( uncertainty_loss . parameters ())) 3. Gradient Normalization (GradNorm) Balances tasks by normalizing gradient magnitudes: def compute_gradnorm_weights ( model , losses , alpha = 1.5 ): \"\"\" Compute task weights using GradNorm algorithm Args: model: Neural network model losses: List of task losses alpha: Hyperparameter for balancing (default: 1.5) Returns: Updated task weights \"\"\" # Get gradients for last shared layer shared_params = list ( model . shared_network . parameters ())[ - 1 ] gradients = [] for loss in losses : grad = torch . autograd . grad ( loss , shared_params , retain_graph = True )[ 0 ] gradients . append ( torch . norm ( grad )) # Compute inverse training rate loss_ratios = [ loss / losses [ 0 ] for loss in losses ] mean_loss_ratio = sum ( loss_ratios ) / len ( loss_ratios ) # Compute target gradients target_grads = [ mean_loss_ratio * ( ratio ** alpha ) for ratio in loss_ratios ] # Compute weights weights = [ target / grad for target , grad in zip ( target_grads , gradients )] # Normalize weights = [ w / sum ( weights ) * len ( weights ) for w in weights ] return weights 4. Dynamic Task Prioritization Adjust weights during training based on task performance: class DynamicTaskWeighting : \"\"\" Dynamically adjust task weights during training \"\"\" def __init__ ( self , num_tasks , initial_weights = None ): if initial_weights is None : self . weights = [ 1.0 ] * num_tasks else : self . weights = initial_weights self . loss_history = [[] for _ in range ( num_tasks )] def update_weights ( self , epoch , task_losses , strategy = 'inverse_performance' ): \"\"\" Update weights based on task performance Strategies: - 'inverse_performance': Higher weight for worse-performing tasks - 'uncertainty': Higher weight for high-variance tasks - 'curriculum': Gradually increase difficulty \"\"\" for i , loss in enumerate ( task_losses ): self . loss_history [ i ] . append ( loss ) if epoch < 5 : # Wait for some history return self . weights if strategy == 'inverse_performance' : # Give more weight to tasks with higher recent loss recent_losses = [ np . mean ( history [ - 5 :]) for history in self . loss_history ] self . weights = [ loss / sum ( recent_losses ) * len ( recent_losses ) for loss in recent_losses ] elif strategy == 'uncertainty' : # Give more weight to high-variance tasks variances = [ np . var ( history [ - 10 :]) for history in self . loss_history ] self . weights = [ var / sum ( variances ) * len ( variances ) for var in variances ] return self . weights 3.5 Performance Comparison \u00b6 Evaluation Metrics for Multi-Task Learning: def evaluate_multitask_model ( model , test_loader , task_configs , device = 'cuda' ): \"\"\" Comprehensive evaluation of multi-task model \"\"\" model . eval () model = model . to ( device ) # Store predictions and labels predictions = { task [ 'name' ]: [] for task in task_configs } true_labels = { task [ 'name' ]: [] for task in task_configs } with torch . no_grad (): for batch_x , batch_labels in test_loader : batch_x = batch_x . to ( device ) outputs = model ( batch_x ) for task in task_configs : task_name = task [ 'name' ] if task_name in batch_labels and batch_labels [ task_name ] is not None : predictions [ task_name ] . extend ( outputs [ task_name ] . cpu () . numpy () ) true_labels [ task_name ] . extend ( batch_labels [ task_name ] . cpu () . numpy () ) # Compute metrics for each task results = {} for task in task_configs : task_name = task [ 'name' ] task_type = task [ 'task_type' ] pred = np . array ( predictions [ task_name ]) . flatten () true = np . array ( true_labels [ task_name ]) . flatten () if task_type == 'regression' : from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score results [ task_name ] = { 'RMSE' : np . sqrt ( mean_squared_error ( true , pred )), 'MAE' : mean_absolute_error ( true , pred ), 'R2' : r2_score ( true , pred ) } elif task_type == 'classification' : from sklearn.metrics import roc_auc_score , accuracy_score , f1_score pred_binary = ( pred > 0 ) . astype ( int ) results [ task_name ] = { 'ROC-AUC' : roc_auc_score ( true , pred ), 'Accuracy' : accuracy_score ( true , pred_binary ), 'F1' : f1_score ( true , pred_binary ) } return results # Compare with single-task models def compare_single_vs_multitask ( smiles_data , labels_dict , task_configs ): \"\"\" Train single-task models and compare with multi-task model \"\"\" results = { 'single_task' : {}, 'multi_task' : {}} # Train single-task models for task in task_configs : task_name = task [ 'name' ] print ( f \" \\n Training single-task model for { task_name } ...\" ) single_model = MolecularFNN ( input_dim = 2048 , output_dim = 1 ) # Train single_model (code similar to previous examples) # ... results [ 'single_task' ][ task_name ] = evaluate_model ( single_model , test_data ) # Train multi-task model print ( \" \\n Training multi-task model...\" ) multitask_model = MultiTaskMolecularModel ( task_configs = task_configs ) # Train multitask_model # ... results [ 'multi_task' ] = evaluate_multitask_model ( multitask_model , test_data , task_configs ) # Print comparison print ( \" \\n \" + \"=\" * 60 ) print ( \"SINGLE-TASK vs MULTI-TASK COMPARISON\" ) print ( \"=\" * 60 ) for task in task_configs : task_name = task [ 'name' ] print ( f \" \\n { task_name . upper () } :\" ) print ( f \" Single-task RMSE: { results [ 'single_task' ][ task_name ][ 'RMSE' ] : .4f } \" ) print ( f \" Multi-task RMSE: { results [ 'multi_task' ][ task_name ][ 'RMSE' ] : .4f } \" ) improvement = (( results [ 'single_task' ][ task_name ][ 'RMSE' ] - results [ 'multi_task' ][ task_name ][ 'RMSE' ]) / results [ 'single_task' ][ task_name ][ 'RMSE' ] * 100 ) print ( f \" Improvement: { improvement : .2f } %\" ) return results Expected Results: Task Single-Task RMSE Multi-Task RMSE Improvement Solubility 0.85 0.72 15.3% BBB Permeability 0.92 0.79 14.1% Toxicity (AUC) 0.78 0.84 7.7% CYP450 Inhibition 0.88 0.81 8.0% Multi-task learning typically shows 10-20% improvement, especially for tasks with limited data. 4. Convolutional Neural Networks \u00b6 Convolutional Neural Networks (CNNs) excel at extracting local patterns and spatial hierarchies, making them suitable for sequence and image data representations of molecules. 4.1 1D CNNs for SMILES \u00b6 SMILES strings can be treated as sequences where local substructures (like functional groups) are important patterns. Architecture Overview: SMILES: \"CCO\" \u2192 Embedding \u2192 Conv1D layers \u2192 Pooling \u2192 Dense \u2192 Output Complete Implementation: import torch import torch.nn as nn import torch.nn.functional as F class SMILES_CNN ( nn . Module ): \"\"\" 1D Convolutional Neural Network for SMILES strings \"\"\" def __init__ ( self , vocab_size = 50 , embedding_dim = 128 , num_filters = 64 , filter_sizes = [ 3 , 5 , 7 ], hidden_dim = 256 , output_dim = 1 , dropout_rate = 0.3 ): \"\"\" Args: vocab_size: Size of character vocabulary (typically 40-60 for SMILES) embedding_dim: Dimension of character embeddings num_filters: Number of filters per filter size filter_sizes: List of filter sizes (kernel sizes) hidden_dim: Size of fully connected layer output_dim: Output size (1 for regression) dropout_rate: Dropout probability \"\"\" super ( SMILES_CNN , self ) . __init__ () # Character embedding layer self . embedding = nn . Embedding ( vocab_size , embedding_dim , padding_idx = 0 ) # Multiple convolutional layers with different kernel sizes self . convs = nn . ModuleList ([ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = fs ) for fs in filter_sizes ]) # Fully connected layers self . fc1 = nn . Linear ( len ( filter_sizes ) * num_filters , hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , output_dim ) # Regularization self . dropout = nn . Dropout ( dropout_rate ) self . batch_norm = nn . BatchNorm1d ( len ( filter_sizes ) * num_filters ) def forward ( self , x ): \"\"\" Args: x: Input tensor of shape (batch_size, max_length) Returns: Output predictions of shape (batch_size, output_dim) \"\"\" # Embedding: (batch_size, max_length) -> (batch_size, max_length, embedding_dim) embedded = self . embedding ( x ) # Transpose for Conv1d: (batch_size, embedding_dim, max_length) embedded = embedded . transpose ( 1 , 2 ) # Apply convolutions and max pooling conv_outputs = [] for conv in self . convs : # Convolution + ReLU: (batch_size, num_filters, length - kernel_size + 1) conv_out = F . relu ( conv ( embedded )) # Max pooling: (batch_size, num_filters, 1) pooled = F . max_pool1d ( conv_out , conv_out . size ( 2 )) # Flatten: (batch_size, num_filters) conv_outputs . append ( pooled . squeeze ( 2 )) # Concatenate all filter outputs: (batch_size, len(filter_sizes) * num_filters) concatenated = torch . cat ( conv_outputs , dim = 1 ) # Batch normalization normalized = self . batch_norm ( concatenated ) # Fully connected layers hidden = F . relu ( self . fc1 ( self . dropout ( normalized ))) output = self . fc2 ( self . dropout ( hidden )) return output # ============================================================================ # SMILES Tokenization # ============================================================================ class SMILESTokenizer : \"\"\" Tokenizer for SMILES strings \"\"\" def __init__ ( self ): # Common SMILES tokens self . tokens = [ 'C' , 'N' , 'O' , 'S' , 'F' , 'Cl' , 'Br' , 'I' , 'P' , # Atoms 'c' , 'n' , 'o' , 's' , # Aromatic atoms '=' , '#' , # Bonds '(' , ')' , # Branches '[' , ']' , # Atom properties '+' , '-' , # Charges '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , # Ring numbers '@' , '@@' , # Chirality 'H' , # Hydrogen '/' , ' \\\\ ' # Stereochemistry ] # Special tokens self . special_tokens = [ '<PAD>' , '<UNK>' , '<START>' , '<END>' ] # Create vocabulary self . vocab = self . special_tokens + self . tokens self . token_to_idx = { token : idx for idx , token in enumerate ( self . vocab )} self . idx_to_token = { idx : token for token , idx in self . token_to_idx . items ()} self . pad_idx = self . token_to_idx [ '<PAD>' ] self . unk_idx = self . token_to_idx [ '<UNK>' ] def tokenize ( self , smiles ): \"\"\" Tokenize SMILES string into characters \"\"\" tokens = [] i = 0 while i < len ( smiles ): # Check for two-character tokens (Cl, Br, @@) if i < len ( smiles ) - 1 : two_char = smiles [ i : i + 2 ] if two_char in self . tokens : tokens . append ( two_char ) i += 2 continue # Single character token token = smiles [ i ] tokens . append ( token if token in self . tokens else '<UNK>' ) i += 1 return tokens def encode ( self , smiles , max_length = 100 ): \"\"\" Convert SMILES to integer sequence \"\"\" tokens = self . tokenize ( smiles ) # Convert to indices indices = [ self . token_to_idx . get ( token , self . unk_idx ) for token in tokens ] # Pad or truncate if len ( indices ) < max_length : indices += [ self . pad_idx ] * ( max_length - len ( indices )) else : indices = indices [: max_length ] return indices def batch_encode ( self , smiles_list , max_length = 100 ): \"\"\" Encode batch of SMILES strings \"\"\" return [ self . encode ( smiles , max_length ) for smiles in smiles_list ] # ============================================================================ # Dataset and Training # ============================================================================ class SMILES_Dataset ( Dataset ): \"\"\" Dataset for SMILES strings \"\"\" def __init__ ( self , smiles_list , labels , tokenizer , max_length = 100 ): self . tokenizer = tokenizer self . encoded_smiles = [] self . labels = [] for smiles , label in zip ( smiles_list , labels ): try : encoded = torch . LongTensor ( tokenizer . encode ( smiles , max_length )) self . encoded_smiles . append ( encoded ) self . labels . append ( label ) except : continue self . labels = torch . FloatTensor ( self . labels ) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . encoded_smiles ) def __getitem__ ( self , idx ): return self . encoded_smiles [ idx ], self . labels [ idx ] # Example usage tokenizer = SMILESTokenizer () print ( f \"Vocabulary size: { len ( tokenizer . vocab ) } \" ) # Create model model = SMILES_CNN ( vocab_size = len ( tokenizer . vocab ), embedding_dim = 128 , num_filters = 64 , filter_sizes = [ 3 , 5 , 7 ], hidden_dim = 256 , output_dim = 1 , dropout_rate = 0.3 ) # Create dataset smiles_train = [ \"CCO\" , \"CC(C)O\" , \"CCCO\" , \"CC(C)CO\" ] # Example SMILES labels_train = [ 1.5 , 1.8 , 1.2 , 1.6 ] # Example labels train_dataset = SMILES_Dataset ( smiles_train , labels_train , tokenizer , max_length = 100 ) train_loader = DataLoader ( train_dataset , batch_size = 32 , shuffle = True ) # Training loop (similar to previous examples) optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) criterion = nn . MSELoss () for epoch in range ( 50 ): model . train () for batch_smiles , batch_labels in train_loader : optimizer . zero_grad () predictions = model ( batch_smiles ) loss = criterion ( predictions , batch_labels ) loss . backward () optimizer . step () Key Design Choices: Multiple Filter Sizes : Capture patterns of different lengths (3=short, 5=medium, 7=long functional groups) Embedding Layer : Learn distributed representations of SMILES characters Max Pooling : Extract most important features regardless of position Concatenation : Combine features from different filter sizes 4.2 2D CNNs for Molecular Images \u00b6 Molecules can be represented as 2D images (chemical structure diagrams) or as heatmaps of molecular properties. Image Representation Approaches: Chemical Structure Diagrams : Rendered 2D molecular structures 3D Conformer Projections : 2D projections of 3D molecular conformations Property Heatmaps : Grids showing electrostatic potential, electron density, etc. Implementation: import torch import torch.nn as nn import torchvision.models as models class Molecular2DCNN ( nn . Module ): \"\"\" 2D CNN for molecular images \"\"\" def __init__ ( self , num_classes = 1 , pretrained = False ): \"\"\" Args: num_classes: Number of output classes/values pretrained: Whether to use pretrained ImageNet weights \"\"\" super ( Molecular2DCNN , self ) . __init__ () # Option 1: Custom CNN architecture self . conv_layers = nn . Sequential ( # First conv block nn . Conv2d ( 3 , 32 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 32 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ), # Second conv block nn . Conv2d ( 32 , 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 64 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ), # Third conv block nn . Conv2d ( 64 , 128 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 128 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ), # Fourth conv block nn . Conv2d ( 128 , 256 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 256 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ) ) # Fully connected layers self . fc_layers = nn . Sequential ( nn . Linear ( 256 * 14 * 14 , 512 ), # Assuming 224x224 input nn . ReLU (), nn . Dropout ( 0.5 ), nn . Linear ( 512 , num_classes ) ) def forward ( self , x ): \"\"\" Args: x: Input images of shape (batch_size, 3, 224, 224) Returns: Output predictions of shape (batch_size, num_classes) \"\"\" x = self . conv_layers ( x ) x = x . view ( x . size ( 0 ), - 1 ) # Flatten x = self . fc_layers ( x ) return x class MolecularResNet ( nn . Module ): \"\"\" ResNet-based model for molecular images (transfer learning) \"\"\" def __init__ ( self , num_classes = 1 , pretrained = True ): super ( MolecularResNet , self ) . __init__ () # Load pretrained ResNet self . resnet = models . resnet50 ( pretrained = pretrained ) # Replace final layer num_features = self . resnet . fc . in_features self . resnet . fc = nn . Linear ( num_features , num_classes ) def forward ( self , x ): return self . resnet ( x ) # Generate molecular images from SMILES from rdkit import Chem from rdkit.Chem import Draw from PIL import Image import io import numpy as np def smiles_to_image ( smiles , size = ( 224 , 224 )): \"\"\" Convert SMILES to image Args: smiles: SMILES string size: Image size (width, height) Returns: numpy array of shape (3, height, width) \"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : # Return blank image if invalid SMILES return np . zeros (( 3 , size [ 1 ], size [ 0 ])) # Draw molecule img = Draw . MolToImage ( mol , size = size ) # Convert to numpy array img_array = np . array ( img ) # Convert to (C, H, W) format if len ( img_array . shape ) == 2 : # Grayscale img_array = np . stack ([ img_array ] * 3 ) else : # RGB img_array = img_array . transpose ( 2 , 0 , 1 ) return img_array / 255.0 # Normalize to [0, 1] class MolecularImageDataset ( Dataset ): \"\"\" Dataset for molecular images \"\"\" def __init__ ( self , smiles_list , labels , transform = None , size = ( 224 , 224 )): self . images = [] self . labels = [] for smiles , label in zip ( smiles_list , labels ): img = smiles_to_image ( smiles , size ) self . images . append ( torch . FloatTensor ( img )) self . labels . append ( label ) self . labels = torch . FloatTensor ( self . labels ) . reshape ( - 1 , 1 ) self . transform = transform def __len__ ( self ): return len ( self . images ) def __getitem__ ( self , idx ): image = self . images [ idx ] if self . transform : image = self . transform ( image ) return image , self . labels [ idx ] 4.3 When to Use Each Approach \u00b6 Comparison Table: Approach Best For Pros Cons 1D CNN on SMILES - Sequence patterns - Functional groups - Large datasets - Fast training - No molecular rendering - Handles variable length - Limited spatial info - SMILES representation bias 2D CNN on Images - Spatial relationships - Stereochemistry - Visual patterns - Captures 2D structure - Transfer learning from ImageNet - Interpretable - Slow image generation - Fixed size input - Loss of 3D info Graph Neural Networks - Atom/bond relationships - 3D structure - Small molecules - Natural molecular representation - Permutation invariant - Interpretable - More complex implementation - (Covered in Day 3) Decision Guide: START \u251c\u2500 Need fast training? \u2192 1D CNN on SMILES \u251c\u2500 Have molecular images? \u2192 2D CNN \u251c\u2500 3D structure important? \u2192 GNN (Day 3) \u251c\u2500 Sequence patterns important? \u2192 1D CNN or RNN \u2514\u2500 Spatial relationships important? \u2192 2D CNN or GNN Example Use Cases: Use 1D CNN on SMILES: - Toxicity prediction (functional group patterns) - Synthetic accessibility (molecular complexity) - Quick property screening Use 2D CNN on Images: - Structure-activity relationship visualization - Similarity search with visual features - Transfer learning from chemical structure databases Hybrid Approach: class HybridMolecularModel ( nn . Module ): \"\"\" Combine 1D CNN (SMILES) and 2D CNN (images) for robust predictions \"\"\" def __init__ ( self , vocab_size , embedding_dim ): super ( HybridMolecularModel , self ) . __init__ () # 1D CNN branch for SMILES self . smiles_cnn = SMILES_CNN ( vocab_size , embedding_dim ) # 2D CNN branch for images self . image_cnn = Molecular2DCNN () # Fusion layer self . fusion = nn . Linear ( 2 , 1 ) # Combine predictions def forward ( self , smiles , images ): \"\"\" Args: smiles: Encoded SMILES (batch_size, max_length) images: Molecular images (batch_size, 3, 224, 224) Returns: Combined predictions \"\"\" smiles_pred = self . smiles_cnn ( smiles ) image_pred = self . image_cnn ( images ) # Concatenate and fuse combined = torch . cat ([ smiles_pred , image_pred ], dim = 1 ) final_pred = self . fusion ( combined ) return final_pred 5. Recurrent Neural Networks \u00b6 Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states that capture information from previous time steps. They are ideal for SMILES strings where the order of tokens matters. 5.1 LSTM for SMILES Sequences \u00b6 Long Short-Term Memory (LSTM) networks address the vanishing gradient problem in traditional RNNs through gating mechanisms. LSTM Architecture: Input \u2192 Embedding \u2192 LSTM layers \u2192 Final hidden state \u2192 Dense \u2192 Output Complete Implementation: import torch import torch.nn as nn class SMILES_LSTM ( nn . Module ): \"\"\" LSTM model for SMILES-based molecular property prediction \"\"\" def __init__ ( self , vocab_size = 50 , embedding_dim = 128 , hidden_dim = 256 , num_layers = 2 , output_dim = 1 , dropout_rate = 0.3 , bidirectional = True ): \"\"\" Args: vocab_size: Size of SMILES vocabulary embedding_dim: Dimension of character embeddings hidden_dim: Size of LSTM hidden state num_layers: Number of stacked LSTM layers output_dim: Output size (1 for regression) dropout_rate: Dropout between LSTM layers bidirectional: Whether to use bidirectional LSTM \"\"\" super ( SMILES_LSTM , self ) . __init__ () self . hidden_dim = hidden_dim self . num_layers = num_layers self . bidirectional = bidirectional # Embedding layer self . embedding = nn . Embedding ( vocab_size , embedding_dim , padding_idx = 0 ) # LSTM layers self . lstm = nn . LSTM ( input_size = embedding_dim , hidden_size = hidden_dim , num_layers = num_layers , batch_first = True , dropout = dropout_rate if num_layers > 1 else 0 , bidirectional = bidirectional ) # Fully connected layers lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim self . fc = nn . Sequential ( nn . Linear ( lstm_output_dim , 128 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 128 , output_dim ) ) def forward ( self , x ): \"\"\" Args: x: Input tensor of shape (batch_size, sequence_length) Returns: Output predictions of shape (batch_size, output_dim) \"\"\" # Embedding: (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim) embedded = self . embedding ( x ) # LSTM: (batch_size, seq_length, hidden_dim * num_directions) lstm_out , ( hidden , cell ) = self . lstm ( embedded ) # Use final hidden state if self . bidirectional : # Concatenate forward and backward final hidden states hidden_forward = hidden [ - 2 , :, :] hidden_backward = hidden [ - 1 , :, :] final_hidden = torch . cat ([ hidden_forward , hidden_backward ], dim = 1 ) else : final_hidden = hidden [ - 1 , :, :] # Fully connected layers output = self . fc ( final_hidden ) return output # Create model model = SMILES_LSTM ( vocab_size = 50 , embedding_dim = 128 , hidden_dim = 256 , num_layers = 2 , output_dim = 1 , dropout_rate = 0.3 , bidirectional = True ) print ( model ) print ( f \"Total parameters: { sum ( p . numel () for p in model . parameters ()) : , } \" ) LSTM Internals: # Manual LSTM cell implementation for understanding class LSTMCell ( nn . Module ): \"\"\" Single LSTM cell showing internal gates \"\"\" def __init__ ( self , input_size , hidden_size ): super ( LSTMCell , self ) . __init__ () # Gates: forget, input, output, candidate self . W_f = nn . Linear ( input_size + hidden_size , hidden_size ) # Forget gate self . W_i = nn . Linear ( input_size + hidden_size , hidden_size ) # Input gate self . W_o = nn . Linear ( input_size + hidden_size , hidden_size ) # Output gate self . W_c = nn . Linear ( input_size + hidden_size , hidden_size ) # Candidate def forward ( self , x_t , h_prev , c_prev ): \"\"\" Args: x_t: Input at time t (batch_size, input_size) h_prev: Previous hidden state (batch_size, hidden_size) c_prev: Previous cell state (batch_size, hidden_size) Returns: h_t: New hidden state c_t: New cell state \"\"\" # Concatenate input and previous hidden state combined = torch . cat ([ x_t , h_prev ], dim = 1 ) # Forget gate: decides what to forget from cell state f_t = torch . sigmoid ( self . W_f ( combined )) # Input gate: decides what new information to store i_t = torch . sigmoid ( self . W_i ( combined )) # Candidate: new candidate values for cell state c_tilde = torch . tanh ( self . W_c ( combined )) # Update cell state c_t = f_t * c_prev + i_t * c_tilde # Output gate: decides what to output from cell state o_t = torch . sigmoid ( self . W_o ( combined )) # Update hidden state h_t = o_t * torch . tanh ( c_t ) return h_t , c_t Attention Mechanism for LSTM: class SMILES_LSTM_Attention ( nn . Module ): \"\"\" LSTM with attention mechanism for SMILES \"\"\" def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim ): super ( SMILES_LSTM_Attention , self ) . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . lstm = nn . LSTM ( embedding_dim , hidden_dim , batch_first = True , bidirectional = True ) # Attention layer self . attention = nn . Linear ( hidden_dim * 2 , 1 ) # Output layer self . fc = nn . Linear ( hidden_dim * 2 , output_dim ) def forward ( self , x ): # Embedding and LSTM embedded = self . embedding ( x ) lstm_out , _ = self . lstm ( embedded ) # (batch, seq_len, hidden*2) # Attention weights attention_scores = self . attention ( lstm_out ) # (batch, seq_len, 1) attention_weights = torch . softmax ( attention_scores , dim = 1 ) # Weighted sum of LSTM outputs context = torch . sum ( attention_weights * lstm_out , dim = 1 ) # (batch, hidden*2) # Output output = self . fc ( context ) return output , attention_weights 5.2 GRU Alternative \u00b6 Gated Recurrent Unit (GRU) is a simpler alternative to LSTM with fewer parameters. GRU vs LSTM: Feature LSTM GRU Gates 3 (input, forget, output) 2 (reset, update) Cell state Separate (c_t and h_t) Unified (h_t only) Parameters More Fewer (~25% less) Training speed Slower Faster Performance Slightly better on complex tasks Comparable on most tasks Memory Higher Lower GRU Implementation: class SMILES_GRU ( nn . Module ): \"\"\" GRU model for SMILES-based molecular property prediction \"\"\" def __init__ ( self , vocab_size = 50 , embedding_dim = 128 , hidden_dim = 256 , num_layers = 2 , output_dim = 1 , dropout_rate = 0.3 , bidirectional = True ): super ( SMILES_GRU , self ) . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim , padding_idx = 0 ) # GRU layers (API similar to LSTM) self . gru = nn . GRU ( input_size = embedding_dim , hidden_size = hidden_dim , num_layers = num_layers , batch_first = True , dropout = dropout_rate if num_layers > 1 else 0 , bidirectional = bidirectional ) gru_output_dim = hidden_dim * 2 if bidirectional else hidden_dim self . fc = nn . Sequential ( nn . Linear ( gru_output_dim , 128 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 128 , output_dim ) ) def forward ( self , x ): embedded = self . embedding ( x ) gru_out , hidden = self . gru ( embedded ) # Use final hidden state if self . gru . bidirectional : hidden_forward = hidden [ - 2 , :, :] hidden_backward = hidden [ - 1 , :, :] final_hidden = torch . cat ([ hidden_forward , hidden_backward ], dim = 1 ) else : final_hidden = hidden [ - 1 , :, :] output = self . fc ( final_hidden ) return output GRU Internals: class GRUCell ( nn . Module ): \"\"\" Single GRU cell for understanding \"\"\" def __init__ ( self , input_size , hidden_size ): super ( GRUCell , self ) . __init__ () # Gates: reset and update self . W_r = nn . Linear ( input_size + hidden_size , hidden_size ) # Reset gate self . W_z = nn . Linear ( input_size + hidden_size , hidden_size ) # Update gate self . W_h = nn . Linear ( input_size + hidden_size , hidden_size ) # Candidate def forward ( self , x_t , h_prev ): \"\"\" Args: x_t: Input at time t h_prev: Previous hidden state Returns: h_t: New hidden state \"\"\" combined = torch . cat ([ x_t , h_prev ], dim = 1 ) # Reset gate: decides how much past to forget r_t = torch . sigmoid ( self . W_r ( combined )) # Update gate: decides how much to update z_t = torch . sigmoid ( self . W_z ( combined )) # Candidate: new candidate hidden state combined_reset = torch . cat ([ x_t , r_t * h_prev ], dim = 1 ) h_tilde = torch . tanh ( self . W_h ( combined_reset )) # Final hidden state: interpolation between prev and candidate h_t = ( 1 - z_t ) * h_prev + z_t * h_tilde return h_t 5.3 Comparison with CNNs \u00b6 Performance Comparison: def compare_cnn_lstm_gru ( smiles_train , y_train , smiles_val , y_val ): \"\"\" Compare CNN, LSTM, and GRU on same dataset \"\"\" tokenizer = SMILESTokenizer () # Prepare data train_dataset = SMILES_Dataset ( smiles_train , y_train , tokenizer ) val_dataset = SMILES_Dataset ( smiles_val , y_val , tokenizer ) train_loader = DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = 64 ) results = {} # Models to compare models = { '1D CNN' : SMILES_CNN ( vocab_size = len ( tokenizer . vocab )), 'LSTM' : SMILES_LSTM ( vocab_size = len ( tokenizer . vocab )), 'GRU' : SMILES_GRU ( vocab_size = len ( tokenizer . vocab )), 'LSTM+Attention' : SMILES_LSTM_Attention ( vocab_size = len ( tokenizer . vocab )) } for name , model in models . items (): print ( f \" \\n Training { name } ...\" ) optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) criterion = nn . MSELoss () # Training loop train_model ( model , train_loader , val_loader , criterion , optimizer , epochs = 50 ) # Evaluate val_metrics = evaluate_model ( model , val_loader , criterion ) results [ name ] = { 'RMSE' : val_metrics [ 'rmse' ], 'R2' : val_metrics [ 'r2' ], 'Parameters' : sum ( p . numel () for p in model . parameters ()), 'Training time' : val_metrics [ 'time' ] } # Print comparison print ( \" \\n \" + \"=\" * 80 ) print ( \"MODEL COMPARISON\" ) print ( \"=\" * 80 ) print ( f \" { 'Model' : <20 } { 'RMSE' : <10 } { 'R2' : <10 } { 'Parameters' : <15 } { 'Time (s)' : <10 } \" ) print ( \"-\" * 80 ) for name , metrics in results . items (): print ( f \" { name : <20 } { metrics [ 'RMSE' ] : <10.4f } { metrics [ 'R2' ] : <10.4f } \" f \" { metrics [ 'Parameters' ] : <15, } { metrics [ 'Training time' ] : <10.1f } \" ) return results Typical Results: Model RMSE R\u00b2 Parameters Training Time Inference Speed 1D CNN 0.72 0.85 1.2M Fast (1x) Very Fast LSTM 0.68 0.87 2.5M Slow (3x) Slow GRU 0.69 0.86 1.9M Medium (2x) Medium LSTM+Attention 0.65 0.89 2.8M Slowest (3.5x) Slow Recommendations: Use CNN when: Speed is priority, local patterns important, large datasets Use LSTM when: Long-range dependencies matter, sequential information crucial Use GRU when: Want RNN benefits with fewer parameters, faster training Use LSTM+Attention when: Need interpretability, best performance, sufficient data 6. Transfer Learning \u00b6 Transfer learning leverages knowledge learned from one task (usually on large datasets) to improve performance on another related task (often with limited data). 6.1 Why Transfer Learning for Molecules \u00b6 Challenges in Molecular Machine Learning: Limited Labeled Data : Experimental measurements are expensive Example: Only ~10K molecules with measured BBB permeability Contrast with millions of images in ImageNet Data Imbalance : Some properties measured more than others Solubility: ~100K datapoints Metabolic stability: ~1K datapoints Related Tasks : Many molecular properties share underlying features Lipophilicity and permeability both depend on polarity Multiple ADMET properties relate to molecular shape Benefits of Transfer Learning: Data Efficiency : Achieve good performance with 10-100x less labeled data Faster Convergence : Pre-trained models converge in fewer epochs Better Generalization : Pre-learned features capture general molecular patterns Domain Adaptation : Adapt models trained on one molecule type to another Example Scenario: Problem: Predict BBB permeability (only 5,000 labeled molecules) Solution 1 (From Scratch): \u251c\u2500 Train model on 5,000 BBB molecules \u251c\u2500 Performance: R\u00b2 = 0.65 \u2514\u2500 Training time: 50 epochs Solution 2 (Transfer Learning): \u251c\u2500 Pre-train on 1M molecules (solubility + LogP + toxicity) \u251c\u2500 Fine-tune on 5,000 BBB molecules \u251c\u2500 Performance: R\u00b2 = 0.82 (+26%) \u2514\u2500 Training time: 10 epochs (5x faster) 6.2 Pre-Training Strategies \u00b6 1. Self-Supervised Pre-training with Autoencoders Learn molecular representations by reconstructing input: class MolecularAutoencoder ( nn . Module ): \"\"\" Autoencoder for learning molecular representations \"\"\" def __init__ ( self , input_dim = 2048 , encoding_dim = 256 ): super ( MolecularAutoencoder , self ) . __init__ () # Encoder self . encoder = nn . Sequential ( nn . Linear ( input_dim , 1024 ), nn . ReLU (), nn . BatchNorm1d ( 1024 ), nn . Linear ( 1024 , 512 ), nn . ReLU (), nn . BatchNorm1d ( 512 ), nn . Linear ( 512 , encoding_dim ), nn . ReLU () ) # Decoder self . decoder = nn . Sequential ( nn . Linear ( encoding_dim , 512 ), nn . ReLU (), nn . BatchNorm1d ( 512 ), nn . Linear ( 512 , 1024 ), nn . ReLU (), nn . BatchNorm1d ( 1024 ), nn . Linear ( 1024 , input_dim ), nn . Sigmoid () # Output in [0, 1] for fingerprints ) def forward ( self , x ): encoding = self . encoder ( x ) reconstruction = self . decoder ( encoding ) return reconstruction def encode ( self , x ): \"\"\"Extract learned representations\"\"\" return self . encoder ( x ) # Pre-training on large unlabeled dataset def pretrain_autoencoder ( smiles_list , num_epochs = 100 ): \"\"\" Pre-train autoencoder on large molecular dataset \"\"\" # Generate fingerprints fingerprints = [] for smiles in smiles_list : mol = Chem . MolFromSmiles ( smiles ) if mol : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) fingerprints . append ( np . array ( fp )) fingerprints = torch . FloatTensor ( np . array ( fingerprints )) dataset = TensorDataset ( fingerprints , fingerprints ) # Input = target dataloader = DataLoader ( dataset , batch_size = 256 , shuffle = True ) # Create and train autoencoder autoencoder = MolecularAutoencoder ( input_dim = 2048 , encoding_dim = 256 ) optimizer = optim . Adam ( autoencoder . parameters (), lr = 0.001 ) criterion = nn . MSELoss () for epoch in range ( num_epochs ): total_loss = 0 for batch_x , _ in dataloader : optimizer . zero_grad () reconstruction = autoencoder ( batch_x ) loss = criterion ( reconstruction , batch_x ) loss . backward () optimizer . step () total_loss += loss . item () if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } , Loss: { total_loss / len ( dataloader ) : .4f } \" ) return autoencoder # Use pre-trained encoder for downstream task class PretrainedMolecularModel ( nn . Module ): \"\"\" Use pre-trained encoder for property prediction \"\"\" def __init__ ( self , pretrained_encoder , output_dim = 1 , freeze_encoder = False ): super ( PretrainedMolecularModel , self ) . __init__ () self . encoder = pretrained_encoder . encoder # Freeze encoder weights if specified if freeze_encoder : for param in self . encoder . parameters (): param . requires_grad = False # Add prediction head self . prediction_head = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Dropout ( 0.3 ), nn . Linear ( 128 , output_dim ) ) def forward ( self , x ): features = self . encoder ( x ) predictions = self . prediction_head ( features ) return predictions 2. Multi-Task Pre-training Train on multiple related properties simultaneously: def pretrain_multitask ( smiles_list , properties_dict , task_configs ): \"\"\" Pre-train on multiple molecular properties Args: smiles_list: List of SMILES strings properties_dict: Dict of {property_name: values_array} task_configs: List of task configurations Returns: Pre-trained model \"\"\" # Create multi-task model model = MultiTaskMolecularModel ( input_dim = 2048 , shared_dims = [ 512 , 256 ], task_configs = task_configs ) # Prepare dataset fingerprints = [] labels = { task [ 'name' ]: [] for task in task_configs } for smiles in smiles_list : mol = Chem . MolFromSmiles ( smiles ) if mol : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) fingerprints . append ( np . array ( fp )) for task in task_configs : task_name = task [ 'name' ] labels [ task_name ] . append ( properties_dict [ task_name ]) # Train multi-task model # (Training code similar to Section 3) # ... return model 3. Masked Language Model (for SMILES) Similar to BERT, mask random tokens and predict them: class MaskedSMILESModel ( nn . Module ): \"\"\" BERT-like masked language model for SMILES \"\"\" def __init__ ( self , vocab_size , embedding_dim = 256 , hidden_dim = 512 , num_layers = 6 ): super ( MaskedSMILESModel , self ) . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) # Transformer encoder encoder_layer = nn . TransformerEncoderLayer ( d_model = embedding_dim , nhead = 8 , dim_feedforward = hidden_dim , dropout = 0.1 ) self . transformer = nn . TransformerEncoder ( encoder_layer , num_layers = num_layers ) # Prediction head for masked tokens self . mlm_head = nn . Linear ( embedding_dim , vocab_size ) def forward ( self , x , mask = None ): embedded = self . embedding ( x ) encoded = self . transformer ( embedded , src_key_padding_mask = mask ) predictions = self . mlm_head ( encoded ) return predictions def create_masked_data ( smiles_list , tokenizer , mask_prob = 0.15 ): \"\"\" Create masked SMILES for pre-training Args: smiles_list: List of SMILES strings tokenizer: SMILES tokenizer mask_prob: Probability of masking each token Returns: masked_inputs, targets \"\"\" masked_inputs = [] targets = [] for smiles in smiles_list : encoded = tokenizer . encode ( smiles ) masked = encoded . copy () for i in range ( len ( encoded )): if np . random . random () < mask_prob : masked [ i ] = tokenizer . token_to_idx [ '<MASK>' ] masked_inputs . append ( masked ) targets . append ( encoded ) return torch . LongTensor ( masked_inputs ), torch . LongTensor ( targets ) 6.3 Fine-Tuning Workflow \u00b6 Complete Fine-Tuning Pipeline: def fine_tune_pretrained_model ( pretrained_model , smiles_train , y_train , smiles_val , y_val , freeze_layers = True ): \"\"\" Fine-tune pre-trained model on downstream task Args: pretrained_model: Pre-trained neural network smiles_train, y_train: Training data for downstream task smiles_val, y_val: Validation data freeze_layers: Whether to freeze early layers Returns: fine_tuned_model, history \"\"\" # Create model with pre-trained weights model = PretrainedMolecularModel ( pretrained_encoder = pretrained_model , output_dim = 1 , freeze_encoder = freeze_layers ) # Prepare data train_dataset = MolecularDataset ( smiles_train , y_train ) val_dataset = MolecularDataset ( smiles_val , y_val ) train_loader = DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = 64 ) # Use lower learning rate for fine-tuning optimizer = optim . Adam ( model . parameters (), lr = 1e-4 ) # 10x smaller than from-scratch criterion = nn . MSELoss () # Training configuration num_epochs = 30 # Fewer epochs needed best_val_loss = float ( 'inf' ) patience = 10 patience_counter = 0 history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : []} for epoch in range ( num_epochs ): # Training model . train () train_loss = 0 for batch_x , batch_y in train_loader : optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) loss . backward () optimizer . step () train_loss += loss . item () # Validation model . eval () val_loss = 0 all_preds = [] all_labels = [] with torch . no_grad (): for batch_x , batch_y in val_loader : predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) val_loss += loss . item () all_preds . extend ( predictions . numpy ()) all_labels . extend ( batch_y . numpy ()) train_loss /= len ( train_loader ) val_loss /= len ( val_loader ) val_rmse = np . sqrt ( mean_squared_error ( all_labels , all_preds )) history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_rmse' ] . append ( val_rmse ) print ( f \"Epoch { epoch + 1 } / { num_epochs } : \" f \"Train Loss= { train_loss : .4f } , Val Loss= { val_loss : .4f } , RMSE= { val_rmse : .4f } \" ) # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 torch . save ( model . state_dict (), 'best_finetuned_model.pth' ) else : patience_counter += 1 if patience_counter >= patience : print ( f \"Early stopping at epoch { epoch + 1 } \" ) break # Load best model model . load_state_dict ( torch . load ( 'best_finetuned_model.pth' )) return model , history # Gradual unfreezing strategy def gradual_unfreezing ( model , train_loader , val_loader , num_phases = 3 ): \"\"\" Gradually unfreeze layers during fine-tuning Phase 1: Freeze all, train head only Phase 2: Unfreeze top encoder layers Phase 3: Unfreeze all layers \"\"\" all_layers = list ( model . encoder . children ()) criterion = nn . MSELoss () for phase in range ( num_phases ): print ( f \" \\n Phase { phase + 1 } / { num_phases } \" ) # Determine which layers to unfreeze if phase == 0 : # Freeze all encoder layers for param in model . encoder . parameters (): param . requires_grad = False elif phase == 1 : # Unfreeze last 1/3 of encoder layers unfreeze_from = len ( all_layers ) * 2 // 3 for i , layer in enumerate ( all_layers ): if i >= unfreeze_from : for param in layer . parameters (): param . requires_grad = True else : # Unfreeze all layers for param in model . encoder . parameters (): param . requires_grad = True # Use decreasing learning rate for each phase lr = 1e-3 / ( 10 ** phase ) optimizer = optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ()), lr = lr ) # Train for this phase for epoch in range ( 10 ): train_epoch ( model , train_loader , criterion , optimizer ) return model 6.4 Pre-Trained Models (ChemBERTa) \u00b6 Using ChemBERTa (Chemical BERT Architecture): from transformers import AutoTokenizer , AutoModel import torch class ChemBERTaFineTuner ( nn . Module ): \"\"\" Fine-tune ChemBERTa for molecular property prediction \"\"\" def __init__ ( self , output_dim = 1 , dropout_rate = 0.3 ): super ( ChemBERTaFineTuner , self ) . __init__ () # Load pre-trained ChemBERTa self . chemberta = AutoModel . from_pretrained ( \"seyonec/ChemBERTa-zinc-base-v1\" ) # Add prediction head hidden_size = self . chemberta . config . hidden_size self . prediction_head = nn . Sequential ( nn . Linear ( hidden_size , 256 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 256 , output_dim ) ) def forward ( self , input_ids , attention_mask ): \"\"\" Args: input_ids: Tokenized SMILES (batch_size, seq_length) attention_mask: Attention mask (batch_size, seq_length) Returns: Predictions (batch_size, output_dim) \"\"\" # Get ChemBERTa embeddings outputs = self . chemberta ( input_ids = input_ids , attention_mask = attention_mask ) # Use [CLS] token representation cls_embedding = outputs . last_hidden_state [:, 0 , :] # Prediction predictions = self . prediction_head ( cls_embedding ) return predictions # Usage example def use_chemberta ( smiles_list , labels ): \"\"\" Fine-tune ChemBERTa on your dataset \"\"\" # Load tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"seyonec/ChemBERTa-zinc-base-v1\" ) # Tokenize SMILES encoded = tokenizer ( smiles_list , padding = True , truncation = True , max_length = 512 , return_tensors = 'pt' ) # Create model model = ChemBERTaFineTuner ( output_dim = 1 ) # Training loop optimizer = optim . AdamW ( model . parameters (), lr = 2e-5 ) criterion = nn . MSELoss () model . train () for epoch in range ( 10 ): optimizer . zero_grad () predictions = model ( input_ids = encoded [ 'input_ids' ], attention_mask = encoded [ 'attention_mask' ] ) loss = criterion ( predictions . squeeze (), labels ) loss . backward () optimizer . step () print ( f \"Epoch { epoch + 1 } , Loss: { loss . item () : .4f } \" ) return model 6.5 Results and Comparisons \u00b6 Comparison Study: def compare_transfer_learning_strategies ( smiles_train , y_train , smiles_val , y_val ): \"\"\" Compare different transfer learning approaches \"\"\" results = {} # Strategy 1: Train from scratch (baseline) print ( \" \\n 1. Training from scratch...\" ) scratch_model = MolecularFNN ( input_dim = 2048 ) scratch_history = train_model ( scratch_model , smiles_train , y_train , smiles_val , y_val ) results [ 'From Scratch' ] = evaluate_model ( scratch_model , smiles_val , y_val ) # Strategy 2: Pre-trained autoencoder print ( \" \\n 2. Pre-trained autoencoder + fine-tuning...\" ) # Pre-train on large unlabeled dataset (e.g., ZINC database) large_smiles = load_large_dataset () # Assume 1M molecules autoencoder = pretrain_autoencoder ( large_smiles , num_epochs = 100 ) transfer_model = PretrainedMolecularModel ( autoencoder , freeze_encoder = True ) transfer_history = fine_tune_pretrained_model ( transfer_model , smiles_train , y_train , smiles_val , y_val ) results [ 'Autoencoder Transfer' ] = evaluate_model ( transfer_model , smiles_val , y_val ) # Strategy 3: Multi-task pre-training print ( \" \\n 3. Multi-task pre-training + fine-tuning...\" ) multitask_model = pretrain_multitask ( large_smiles , properties_dict , task_configs ) multitask_transfer = fine_tune_pretrained_model ( multitask_model , smiles_train , y_train , smiles_val , y_val ) results [ 'Multi-Task Transfer' ] = evaluate_model ( multitask_transfer , smiles_val , y_val ) # Strategy 4: ChemBERTa print ( \" \\n 4. ChemBERTa fine-tuning...\" ) chemberta_model = ChemBERTaFineTuner () chemberta_history = fine_tune_chemberta ( chemberta_model , smiles_train , y_train , smiles_val , y_val ) results [ 'ChemBERTa' ] = evaluate_model ( chemberta_model , smiles_val , y_val ) # Print comparison print ( \" \\n \" + \"=\" * 70 ) print ( \"TRANSFER LEARNING COMPARISON\" ) print ( \"=\" * 70 ) print ( f \" { 'Strategy' : <30 } { 'RMSE' : <10 } { 'R\u00b2' : <10 } { 'Training Time' : <15 } \" ) print ( \"-\" * 70 ) for strategy , metrics in results . items (): print ( f \" { strategy : <30 } { metrics [ 'RMSE' ] : <10.4f } { metrics [ 'R\u00b2' ] : <10.4f } \" f \" { metrics [ 'time' ] : <15.1f } s\" ) return results Expected Results (BBB Permeability Example): Strategy RMSE R\u00b2 Data Required Training Time Improvement From Scratch 0.95 0.68 5,000 120 min Baseline Autoencoder Transfer 0.78 0.79 5,000 45 min +16% Multi-Task Transfer 0.72 0.82 5,000 35 min +20% ChemBERTa 0.65 0.87 5,000 25 min +28% ChemBERTa (Low Data) 0.82 0.75 500 15 min Still viable Key Insights: Transfer learning provides 15-30% improvement in performance Training time reduced by 60-80% Most effective when target task has limited data (<10K samples) ChemBERTa performs best due to massive pre-training on 77M molecules Multi-task pre-training excellent when related properties available 7. Complete Practical Exercise \u00b6 7.1 Problem: BBB Permeability Prediction \u00b6 Background: Blood-Brain Barrier (BBB) permeability is crucial for CNS drugs. We\u2019ll predict log(BB), the logarithm of the brain-to-blood concentration ratio. Dataset: - Training: 1,500 molecules - Validation: 300 molecules - Test: 200 molecules - Features: SMILES strings - Target: log(BB) values (continuous, range: -3 to 2) 7.2 Full Pipeline \u00b6 # ============================================================================ # COMPLETE BBB PERMEABILITY PREDICTION PIPELINE # ============================================================================ import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score from rdkit import Chem from rdkit.Chem import AllChem , Descriptors import matplotlib.pyplot as plt import seaborn as sns from tqdm import tqdm import time # Set random seeds for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 42 ) # ============================================================================ # 1. DATA LOADING AND PREPROCESSING # ============================================================================ def load_bbb_data ( filepath = 'bbb_permeability.csv' ): \"\"\" Load BBB permeability dataset Expected columns: SMILES, logBB \"\"\" try : df = pd . read_csv ( filepath ) except : # Generate synthetic data for demonstration print ( \"Generating synthetic BBB data...\" ) df = generate_synthetic_bbb_data ( 2000 ) # Remove invalid SMILES valid_indices = [] for idx , smiles in enumerate ( df [ 'SMILES' ]): mol = Chem . MolFromSmiles ( smiles ) if mol is not None : valid_indices . append ( idx ) df = df . iloc [ valid_indices ] . reset_index ( drop = True ) print ( f \"Loaded { len ( df ) } valid molecules\" ) print ( f \"logBB range: [ { df [ 'logBB' ] . min () : .2f } , { df [ 'logBB' ] . max () : .2f } ]\" ) return df def generate_synthetic_bbb_data ( n_samples = 2000 ): \"\"\" Generate synthetic BBB data for demonstration \"\"\" # Common drug-like SMILES templates templates = [ \"CCO\" , \"CC(C)O\" , \"CCCO\" , \"CC(C)CO\" , \"CCCCO\" , \"c1ccccc1\" , \"c1ccccc1C\" , \"c1ccccc1O\" , \"c1ccccc1N\" , \"CC(=O)O\" , \"CC(=O)N\" , \"CCNC\" , \"CCN(C)C\" , \"c1ccc(cc1)C(=O)O\" , \"c1ccc(cc1)N\" ] smiles_list = [] logbb_list = [] for _ in range ( n_samples ): # Random combination of templates smiles = np . random . choice ( templates ) # Calculate simple features for synthetic logBB mol = Chem . MolFromSmiles ( smiles ) if mol : mw = Descriptors . MolWt ( mol ) logp = Descriptors . MolLogP ( mol ) tpsa = Descriptors . TPSA ( mol ) # Synthetic logBB based on known correlations logbb = 0.1 * logp - 0.01 * tpsa - 0.003 * mw + np . random . normal ( 0 , 0.3 ) logbb = np . clip ( logbb , - 3 , 2 ) smiles_list . append ( smiles ) logbb_list . append ( logbb ) df = pd . DataFrame ({ 'SMILES' : smiles_list , 'logBB' : logbb_list }) return df # ============================================================================ # 2. FEATURE EXTRACTION # ============================================================================ def compute_molecular_fingerprints ( smiles_list , radius = 2 , n_bits = 2048 ): \"\"\" Compute Morgan fingerprints for molecules \"\"\" fingerprints = [] for smiles in tqdm ( smiles_list , desc = \"Computing fingerprints\" ): mol = Chem . MolFromSmiles ( smiles ) if mol : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , radius , nBits = n_bits ) fingerprints . append ( np . array ( fp )) else : fingerprints . append ( np . zeros ( n_bits )) return np . array ( fingerprints ) def compute_molecular_descriptors ( smiles_list ): \"\"\" Compute RDKit molecular descriptors \"\"\" descriptors_list = [] descriptor_functions = [ Descriptors . MolWt , Descriptors . MolLogP , Descriptors . NumHDonors , Descriptors . NumHAcceptors , Descriptors . TPSA , Descriptors . NumRotatableBonds , Descriptors . NumAromaticRings , Descriptors . FractionCsp3 ] for smiles in tqdm ( smiles_list , desc = \"Computing descriptors\" ): mol = Chem . MolFromSmiles ( smiles ) if mol : desc = [ func ( mol ) for func in descriptor_functions ] descriptors_list . append ( desc ) else : descriptors_list . append ([ 0 ] * len ( descriptor_functions )) return np . array ( descriptors_list ) # ============================================================================ # 3. DEEP LEARNING MODEL # ============================================================================ class BBBPermeabilityModel ( nn . Module ): \"\"\" Neural network for BBB permeability prediction \"\"\" def __init__ ( self , input_dim = 2048 ): super ( BBBPermeabilityModel , self ) . __init__ () self . network = nn . Sequential ( nn . Linear ( input_dim , 512 ), nn . BatchNorm1d ( 512 ), nn . ReLU (), nn . Dropout ( 0.3 ), nn . Linear ( 512 , 256 ), nn . BatchNorm1d ( 256 ), nn . ReLU (), nn . Dropout ( 0.3 ), nn . Linear ( 256 , 128 ), nn . BatchNorm1d ( 128 ), nn . ReLU (), nn . Dropout ( 0.2 ), nn . Linear ( 128 , 1 ) ) def forward ( self , x ): return self . network ( x ) class BBBDataset ( Dataset ): \"\"\"Dataset for BBB permeability\"\"\" def __init__ ( self , features , labels ): self . features = torch . FloatTensor ( features ) self . labels = torch . FloatTensor ( labels ) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . features ) def __getitem__ ( self , idx ): return self . features [ idx ], self . labels [ idx ] # ============================================================================ # 4. TRAINING FUNCTION # ============================================================================ def train_deep_learning_model ( X_train , y_train , X_val , y_val , num_epochs = 100 , batch_size = 64 , lr = 0.001 ): \"\"\" Train deep learning model for BBB permeability \"\"\" device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) print ( f \"Using device: { device } \" ) # Create datasets and dataloaders train_dataset = BBBDataset ( X_train , y_train ) val_dataset = BBBDataset ( X_val , y_val ) train_loader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = batch_size ) # Initialize model model = BBBPermeabilityModel ( input_dim = X_train . shape [ 1 ]) . to ( device ) # Loss and optimizer criterion = nn . MSELoss () optimizer = optim . Adam ( model . parameters (), lr = lr ) scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , patience = 10 , factor = 0.5 ) # Training history history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : [], 'val_mae' : [], 'val_r2' : [] } best_val_loss = float ( 'inf' ) patience_counter = 0 patience = 20 start_time = time . time () # Training loop for epoch in range ( num_epochs ): # Training model . train () train_loss = 0 for batch_x , batch_y in train_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) loss . backward () optimizer . step () train_loss += loss . item () # Validation model . eval () val_loss = 0 all_preds = [] all_labels = [] with torch . no_grad (): for batch_x , batch_y in val_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) val_loss += loss . item () all_preds . extend ( predictions . cpu () . numpy ()) all_labels . extend ( batch_y . cpu () . numpy ()) # Calculate metrics train_loss /= len ( train_loader ) val_loss /= len ( val_loader ) all_preds = np . array ( all_preds ) . flatten () all_labels = np . array ( all_labels ) . flatten () val_rmse = np . sqrt ( mean_squared_error ( all_labels , all_preds )) val_mae = mean_absolute_error ( all_labels , all_preds ) val_r2 = r2_score ( all_labels , all_preds ) # Update history history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_rmse' ] . append ( val_rmse ) history [ 'val_mae' ] . append ( val_mae ) history [ 'val_r2' ] . append ( val_r2 ) # Learning rate scheduling scheduler . step ( val_loss ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } / { num_epochs } \" ) print ( f \" Train Loss: { train_loss : .4f } \" ) print ( f \" Val Loss: { val_loss : .4f } , RMSE: { val_rmse : .4f } , \" f \"MAE: { val_mae : .4f } , R\u00b2: { val_r2 : .4f } \" ) # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 torch . save ( model . state_dict (), 'best_bbb_model.pth' ) else : patience_counter += 1 if patience_counter >= patience : print ( f \" \\n Early stopping at epoch { epoch + 1 } \" ) break training_time = time . time () - start_time # Load best model model . load_state_dict ( torch . load ( 'best_bbb_model.pth' )) return model , history , training_time # ============================================================================ # 5. RANDOM FOREST BASELINE # ============================================================================ def train_random_forest_model ( X_train , y_train , X_val , y_val ): \"\"\" Train Random Forest baseline model \"\"\" start_time = time . time () # Train Random Forest rf_model = RandomForestRegressor ( n_estimators = 500 , max_depth = 20 , min_samples_split = 5 , min_samples_leaf = 2 , random_state = 42 , n_jobs =- 1 ) rf_model . fit ( X_train , y_train ) # Predictions train_pred = rf_model . predict ( X_train ) val_pred = rf_model . predict ( X_val ) # Metrics train_rmse = np . sqrt ( mean_squared_error ( y_train , train_pred )) val_rmse = np . sqrt ( mean_squared_error ( y_val , val_pred )) val_mae = mean_absolute_error ( y_val , val_pred ) val_r2 = r2_score ( y_val , val_pred ) training_time = time . time () - start_time print ( \" \\n Random Forest Results:\" ) print ( f \" Train RMSE: { train_rmse : .4f } \" ) print ( f \" Val RMSE: { val_rmse : .4f } \" ) print ( f \" Val MAE: { val_mae : .4f } \" ) print ( f \" Val R\u00b2: { val_r2 : .4f } \" ) print ( f \" Training time: { training_time : .2f } s\" ) return rf_model , { 'train_rmse' : train_rmse , 'val_rmse' : val_rmse , 'val_mae' : val_mae , 'val_r2' : val_r2 , 'time' : training_time } # ============================================================================ # 6. EVALUATION AND VISUALIZATION # ============================================================================ def evaluate_model ( model , X_test , y_test , model_type = 'dl' ): \"\"\" Comprehensive model evaluation \"\"\" if model_type == 'dl' : device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) model . eval () test_dataset = BBBDataset ( X_test , y_test ) test_loader = DataLoader ( test_dataset , batch_size = 64 ) predictions = [] with torch . no_grad (): for batch_x , _ in test_loader : batch_x = batch_x . to ( device ) pred = model ( batch_x ) predictions . extend ( pred . cpu () . numpy ()) predictions = np . array ( predictions ) . flatten () else : predictions = model . predict ( X_test ) # Calculate metrics rmse = np . sqrt ( mean_squared_error ( y_test , predictions )) mae = mean_absolute_error ( y_test , predictions ) r2 = r2_score ( y_test , predictions ) print ( f \" \\n Test Set Evaluation:\" ) print ( f \" RMSE: { rmse : .4f } \" ) print ( f \" MAE: { mae : .4f } \" ) print ( f \" R\u00b2: { r2 : .4f } \" ) return predictions , { 'rmse' : rmse , 'mae' : mae , 'r2' : r2 } def visualize_results ( y_true , y_pred_dl , y_pred_rf , history ): \"\"\" Create comprehensive visualization of results \"\"\" fig , axes = plt . subplots ( 2 , 3 , figsize = ( 18 , 12 )) # 1. Training history axes [ 0 , 0 ] . plot ( history [ 'train_loss' ], label = 'Train Loss' ) axes [ 0 , 0 ] . plot ( history [ 'val_loss' ], label = 'Val Loss' ) axes [ 0 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 0 ] . set_ylabel ( 'Loss' ) axes [ 0 , 0 ] . set_title ( 'Training History' ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( True ) # 2. Metrics evolution axes [ 0 , 1 ] . plot ( history [ 'val_rmse' ], label = 'RMSE' , color = 'red' ) axes [ 0 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 1 ] . set_ylabel ( 'RMSE' ) axes [ 0 , 1 ] . set_title ( 'Validation RMSE' ) axes [ 0 , 1 ] . grid ( True ) ax2 = axes [ 0 , 1 ] . twinx () ax2 . plot ( history [ 'val_r2' ], label = 'R\u00b2' , color = 'blue' ) ax2 . set_ylabel ( 'R\u00b2' ) # 3. R\u00b2 evolution axes [ 0 , 2 ] . plot ( history [ 'val_r2' ], color = 'green' ) axes [ 0 , 2 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 2 ] . set_ylabel ( 'R\u00b2 Score' ) axes [ 0 , 2 ] . set_title ( 'Validation R\u00b2' ) axes [ 0 , 2 ] . grid ( True ) # 4. DL predictions scatter plot axes [ 1 , 0 ] . scatter ( y_true , y_pred_dl , alpha = 0.5 ) axes [ 1 , 0 ] . plot ([ y_true . min (), y_true . max ()], [ y_true . min (), y_true . max ()], 'r--' , lw = 2 ) axes [ 1 , 0 ] . set_xlabel ( 'True logBB' ) axes [ 1 , 0 ] . set_ylabel ( 'Predicted logBB' ) axes [ 1 , 0 ] . set_title ( 'Deep Learning Predictions' ) axes [ 1 , 0 ] . grid ( True ) # 5. RF predictions scatter plot axes [ 1 , 1 ] . scatter ( y_true , y_pred_rf , alpha = 0.5 , color = 'orange' ) axes [ 1 , 1 ] . plot ([ y_true . min (), y_true . max ()], [ y_true . min (), y_true . max ()], 'r--' , lw = 2 ) axes [ 1 , 1 ] . set_xlabel ( 'True logBB' ) axes [ 1 , 1 ] . set_ylabel ( 'Predicted logBB' ) axes [ 1 , 1 ] . set_title ( 'Random Forest Predictions' ) axes [ 1 , 1 ] . grid ( True ) # 6. Residuals comparison residuals_dl = y_true - y_pred_dl residuals_rf = y_true - y_pred_rf axes [ 1 , 2 ] . hist ( residuals_dl , bins = 30 , alpha = 0.5 , label = 'Deep Learning' ) axes [ 1 , 2 ] . hist ( residuals_rf , bins = 30 , alpha = 0.5 , label = 'Random Forest' ) axes [ 1 , 2 ] . set_xlabel ( 'Residuals' ) axes [ 1 , 2 ] . set_ylabel ( 'Frequency' ) axes [ 1 , 2 ] . set_title ( 'Residuals Distribution' ) axes [ 1 , 2 ] . legend () axes [ 1 , 2 ] . grid ( True ) plt . tight_layout () plt . savefig ( 'bbb_prediction_results.png' , dpi = 150 , bbox_inches = 'tight' ) plt . show () # ============================================================================ # 7. MAIN EXECUTION # ============================================================================ def main (): \"\"\" Main execution function \"\"\" print ( \"=\" * 70 ) print ( \"BBB PERMEABILITY PREDICTION - COMPLETE PIPELINE\" ) print ( \"=\" * 70 ) # 1. Load data print ( \" \\n 1. Loading data...\" ) df = load_bbb_data () # 2. Split data print ( \" \\n 2. Splitting data...\" ) train_df , temp_df = train_test_split ( df , test_size = 0.3 , random_state = 42 ) val_df , test_df = train_test_split ( temp_df , test_size = 0.5 , random_state = 42 ) print ( f \" Train: { len ( train_df ) } molecules\" ) print ( f \" Val: { len ( val_df ) } molecules\" ) print ( f \" Test: { len ( test_df ) } molecules\" ) # 3. Extract features print ( \" \\n 3. Extracting features...\" ) X_train = compute_molecular_fingerprints ( train_df [ 'SMILES' ] . values ) X_val = compute_molecular_fingerprints ( val_df [ 'SMILES' ] . values ) X_test = compute_molecular_fingerprints ( test_df [ 'SMILES' ] . values ) y_train = train_df [ 'logBB' ] . values y_val = val_df [ 'logBB' ] . values y_test = test_df [ 'logBB' ] . values # 4. Train Deep Learning model print ( \" \\n 4. Training Deep Learning model...\" ) dl_model , history , dl_time = train_deep_learning_model ( X_train , y_train , X_val , y_val , num_epochs = 100 , batch_size = 64 , lr = 0.001 ) print ( f \" Training time: { dl_time : .2f } s\" ) # 5. Train Random Forest baseline print ( \" \\n 5. Training Random Forest baseline...\" ) rf_model , rf_results = train_random_forest_model ( X_train , y_train , X_val , y_val ) # 6. Evaluate on test set print ( \" \\n 6. Evaluating models on test set...\" ) print ( \" \\n Deep Learning Model:\" ) dl_predictions , dl_metrics = evaluate_model ( dl_model , X_test , y_test , 'dl' ) print ( \" \\n Random Forest Model:\" ) rf_predictions , rf_metrics = evaluate_model ( rf_model , X_test , y_test , 'rf' ) # 7. Compare results print ( \" \\n \" + \"=\" * 70 ) print ( \"FINAL COMPARISON\" ) print ( \"=\" * 70 ) print ( f \" { 'Metric' : <15 } { 'Deep Learning' : <20 } { 'Random Forest' : <20 } { 'Improvement' } \" ) print ( \"-\" * 70 ) print ( f \" { 'RMSE' : <15 } { dl_metrics [ 'rmse' ] : <20.4f } { rf_metrics [ 'rmse' ] : <20.4f } \" f \" { ( rf_metrics [ 'rmse' ] - dl_metrics [ 'rmse' ]) / rf_metrics [ 'rmse' ] * 100 : >6.1f } %\" ) print ( f \" { 'MAE' : <15 } { dl_metrics [ 'mae' ] : <20.4f } { rf_metrics [ 'mae' ] : <20.4f } \" f \" { ( rf_metrics [ 'mae' ] - dl_metrics [ 'mae' ]) / rf_metrics [ 'mae' ] * 100 : >6.1f } %\" ) print ( f \" { 'R\u00b2' : <15 } { dl_metrics [ 'r2' ] : <20.4f } { rf_metrics [ 'r2' ] : <20.4f } \" f \" { ( dl_metrics [ 'r2' ] - rf_metrics [ 'r2' ]) / rf_metrics [ 'r2' ] * 100 : >6.1f } %\" ) print ( f \" { 'Training Time' : <15 } { dl_time : <20.1f } s { rf_results [ 'time' ] : <20.1f } s\" ) # 8. Visualize results print ( \" \\n 8. Generating visualizations...\" ) visualize_results ( y_test , dl_predictions , rf_predictions , history ) print ( \" \\n \" + \"=\" * 70 ) print ( \"PIPELINE COMPLETE!\" ) print ( \"=\" * 70 ) return { 'dl_model' : dl_model , 'rf_model' : rf_model , 'dl_metrics' : dl_metrics , 'rf_metrics' : rf_metrics , 'history' : history } # Run the pipeline if __name__ == \"__main__\" : results = main () 7.3 Expected Output \u00b6 ====================================================================== BBB PERMEABILITY PREDICTION - COMPLETE PIPELINE ====================================================================== 1. Loading data... Loaded 2000 valid molecules logBB range: [-2.85, 1.92] 2. Splitting data... Train: 1400 molecules Val: 300 molecules Test: 300 molecules 3. Extracting features... Computing fingerprints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1400/1400 [00:05<00:00] Computing fingerprints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:01<00:00] Computing fingerprints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:01<00:00] 4. Training Deep Learning model... Using device: cuda Epoch 10/100 Train Loss: 0.3245 Val Loss: 0.3678, RMSE: 0.6065, MAE: 0.4532, R\u00b2: 0.7234 Epoch 20/100 Train Loss: 0.2156 Val Loss: 0.2987, RMSE: 0.5465, MAE: 0.4012, R\u00b2: 0.7789 ... (training continues) Early stopping at epoch 68 Training time: 142.35s 5. Training Random Forest baseline... Random Forest Results: Train RMSE: 0.1234 Val RMSE: 0.6234 Val MAE: 0.4689 Val R\u00b2: 0.7456 Training time: 56.78s 6. Evaluating models on test set... Deep Learning Model: Test Set Evaluation: RMSE: 0.5234 MAE: 0.3876 R\u00b2: 0.7923 Random Forest Model: Test Set Evaluation: RMSE: 0.5987 MAE: 0.4456 R\u00b2: 0.7534 ====================================================================== FINAL COMPARISON ====================================================================== Metric Deep Learning Random Forest Improvement ---------------------------------------------------------------------- RMSE 0.5234 0.5987 12.6% MAE 0.3876 0.4456 13.0% R\u00b2 0.7923 0.7534 5.2% Training Time 142.4s 56.8s 8. Generating visualizations... ====================================================================== PIPELINE COMPLETE! ====================================================================== 8. Model Interpretation \u00b6 Understanding what your model has learned is crucial for trust, debugging, and scientific insight. 8.1 Gradient-Based Importance \u00b6 Calculate feature importance by examining gradients: def compute_gradient_importance ( model , X , device = 'cuda' ): \"\"\" Compute feature importance using gradients Args: model: Trained neural network X: Input features (numpy array or tensor) Returns: importance_scores: Feature importance for each sample \"\"\" model . eval () if isinstance ( X , np . ndarray ): X = torch . FloatTensor ( X ) X = X . to ( device ) X . requires_grad = True # Forward pass output = model ( X ) # Compute gradients gradients = [] for i in range ( output . shape [ 0 ]): model . zero_grad () if X . grad is not None : X . grad . zero_ () output [ i ] . backward ( retain_graph = True ) gradients . append ( X . grad [ i ] . cpu () . detach () . numpy () . copy ()) gradients = np . array ( gradients ) # Importance = absolute gradient * input value importance = np . abs ( gradients ) * X . cpu () . detach () . numpy () return importance def visualize_feature_importance ( importance , feature_names = None , top_k = 20 ): \"\"\" Visualize feature importance \"\"\" # Average importance across samples avg_importance = np . mean ( importance , axis = 0 ) # Get top-k features top_indices = np . argsort ( avg_importance )[ - top_k :][:: - 1 ] top_importance = avg_importance [ top_indices ] if feature_names is not None : top_features = [ feature_names [ i ] for i in top_indices ] else : top_features = [ f \"Feature { i } \" for i in top_indices ] # Plot plt . figure ( figsize = ( 10 , 6 )) plt . barh ( range ( top_k ), top_importance ) plt . yticks ( range ( top_k ), top_features ) plt . xlabel ( 'Importance Score' ) plt . title ( f 'Top { top_k } Most Important Features' ) plt . tight_layout () plt . savefig ( 'feature_importance.png' , dpi = 150 ) plt . show () # Usage importance_scores = compute_gradient_importance ( model , X_test ) visualize_feature_importance ( importance_scores , top_k = 20 ) 8.2 Integrated Gradients \u00b6 More accurate attribution method that accounts for baseline: def integrated_gradients ( model , X , baseline = None , steps = 50 , device = 'cuda' ): \"\"\" Compute integrated gradients for feature attribution Args: model: Trained neural network X: Input features baseline: Baseline input (default: zero) steps: Number of integration steps Returns: attributions: Feature attributions \"\"\" model . eval () if isinstance ( X , np . ndarray ): X = torch . FloatTensor ( X ) X = X . to ( device ) if baseline is None : baseline = torch . zeros_like ( X ) else : baseline = torch . FloatTensor ( baseline ) . to ( device ) # Generate interpolated inputs alphas = torch . linspace ( 0 , 1 , steps ) . to ( device ) interpolated_inputs = [] for alpha in alphas : interpolated = baseline + alpha * ( X - baseline ) interpolated_inputs . append ( interpolated ) interpolated_inputs = torch . stack ( interpolated_inputs ) # Compute gradients for each interpolated input gradients = [] for interp_input in interpolated_inputs : interp_input . requires_grad = True output = model ( interp_input ) model . zero_grad () output . sum () . backward () gradients . append ( interp_input . grad . cpu () . detach ()) gradients = torch . stack ( gradients ) # Average gradients and multiply by input difference avg_gradients = torch . mean ( gradients , dim = 0 ) attributions = ( X . cpu () - baseline . cpu ()) * avg_gradients return attributions . numpy () def explain_prediction ( model , smiles , tokenizer , importance_method = 'integrated_gradients' ): \"\"\" Explain prediction for a single molecule \"\"\" # Encode SMILES mol = Chem . MolFromSmiles ( smiles ) fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) X = np . array ( fp ) . reshape ( 1 , - 1 ) # Get prediction model . eval () with torch . no_grad (): prediction = model ( torch . FloatTensor ( X )) . item () # Get importance if importance_method == 'integrated_gradients' : importance = integrated_gradients ( model , X ) else : importance = compute_gradient_importance ( model , X ) # Visualize fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) # Draw molecule img = Draw . MolToImage ( mol , size = ( 400 , 400 )) ax1 . imshow ( img ) ax1 . axis ( 'off' ) ax1 . set_title ( f 'Predicted logBB: { prediction : .2f } ' ) # Plot importance top_k = 50 top_indices = np . argsort ( np . abs ( importance [ 0 ]))[ - top_k :] ax2 . barh ( range ( top_k ), importance [ 0 , top_indices ]) ax2 . set_xlabel ( 'Attribution Score' ) ax2 . set_ylabel ( 'Fingerprint Bit' ) ax2 . set_title ( 'Feature Attributions (Integrated Gradients)' ) plt . tight_layout () plt . savefig ( f 'explanation_ { smiles [: 10 ] } .png' , dpi = 150 ) plt . show () return prediction , importance 8.3 Activation Maximization \u00b6 Find input patterns that maximally activate specific neurons: def activation_maximization ( model , layer_name , neuron_idx , input_shape = ( 1 , 2048 ), iterations = 1000 , lr = 0.1 ): \"\"\" Find input that maximally activates a specific neuron Args: model: Neural network layer_name: Name of layer to analyze neuron_idx: Index of neuron in that layer input_shape: Shape of input iterations: Number of optimization steps lr: Learning rate Returns: optimal_input: Input that maximizes activation \"\"\" model . eval () # Initialize random input optimal_input = torch . randn ( input_shape , requires_grad = True ) optimizer = optim . Adam ([ optimal_input ], lr = lr ) # Get layer activation = {} def get_activation ( name ): def hook ( model , input , output ): activation [ name ] = output return hook # Register hook for name , module in model . named_modules (): if name == layer_name : module . register_forward_hook ( get_activation ( layer_name )) break # Optimize for i in range ( iterations ): optimizer . zero_grad () _ = model ( optimal_input ) # Loss = negative activation (we want to maximize) loss = - activation [ layer_name ][ 0 , neuron_idx ] loss . backward () optimizer . step () # Clip to valid range [0, 1] for fingerprints with torch . no_grad (): optimal_input . clamp_ ( 0 , 1 ) if ( i + 1 ) % 100 == 0 : print ( f \"Iteration { i + 1 } , Activation: { - loss . item () : .4f } \" ) return optimal_input . detach () . numpy () # Usage optimal_fp = activation_maximization ( model , layer_name = 'network.4' , # Second hidden layer neuron_idx = 42 , iterations = 1000 ) print ( f \"Optimal fingerprint pattern: { optimal_fp [ 0 ][: 20 ] } ...\" ) print ( f \"Number of active bits: { np . sum ( optimal_fp > 0.5 ) } \" ) Attention Visualization for LSTM: def visualize_attention ( model , smiles , tokenizer ): \"\"\" Visualize attention weights for LSTM model with attention \"\"\" # Encode SMILES encoded = tokenizer . encode ( smiles ) X = torch . LongTensor ([ encoded ]) # Get prediction and attention weights model . eval () with torch . no_grad (): prediction , attention_weights = model ( X ) # Plot attention attention = attention_weights [ 0 ] . squeeze () . numpy () tokens = tokenizer . tokenize ( smiles ) plt . figure ( figsize = ( 12 , 4 )) plt . bar ( range ( len ( tokens )), attention [: len ( tokens )]) plt . xticks ( range ( len ( tokens )), tokens , rotation = 45 ) plt . xlabel ( 'SMILES Token' ) plt . ylabel ( 'Attention Weight' ) plt . title ( f 'Attention Weights for: { smiles } \\n Prediction: { prediction . item () : .2f } ' ) plt . tight_layout () plt . savefig ( 'attention_visualization.png' , dpi = 150 ) plt . show () 9. Best Practices \u00b6 9.1 Hyperparameter Tuning with Optuna \u00b6 Automated hyperparameter optimization: import optuna from optuna.visualization import plot_optimization_history , plot_param_importances def objective ( trial , X_train , y_train , X_val , y_val ): \"\"\" Objective function for Optuna hyperparameter optimization \"\"\" # Suggest hyperparameters config = { 'hidden_dim_1' : trial . suggest_int ( 'hidden_dim_1' , 256 , 1024 , step = 128 ), 'hidden_dim_2' : trial . suggest_int ( 'hidden_dim_2' , 128 , 512 , step = 64 ), 'hidden_dim_3' : trial . suggest_int ( 'hidden_dim_3' , 64 , 256 , step = 64 ), 'dropout_rate' : trial . suggest_float ( 'dropout_rate' , 0.1 , 0.5 ), 'learning_rate' : trial . suggest_loguniform ( 'learning_rate' , 1e-5 , 1e-2 ), 'batch_size' : trial . suggest_categorical ( 'batch_size' , [ 32 , 64 , 128 ]), 'weight_decay' : trial . suggest_loguniform ( 'weight_decay' , 1e-6 , 1e-3 ) } # Create model model = MolecularFNN ( input_dim = X_train . shape [ 1 ], hidden_dims = [ config [ 'hidden_dim_1' ], config [ 'hidden_dim_2' ], config [ 'hidden_dim_3' ]], output_dim = 1 , dropout_rate = config [ 'dropout_rate' ] ) # Create dataloaders train_dataset = BBBDataset ( X_train , y_train ) val_dataset = BBBDataset ( X_val , y_val ) train_loader = DataLoader ( train_dataset , batch_size = config [ 'batch_size' ], shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = config [ 'batch_size' ]) # Train optimizer = optim . Adam ( model . parameters (), lr = config [ 'learning_rate' ], weight_decay = config [ 'weight_decay' ]) criterion = nn . MSELoss () num_epochs = 50 # Reduced for faster tuning for epoch in range ( num_epochs ): # Training model . train () for batch_x , batch_y in train_loader : optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) loss . backward () optimizer . step () # Validation model . eval () val_loss = 0 with torch . no_grad (): for batch_x , batch_y in val_loader : predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) val_loss += loss . item () val_loss /= len ( val_loader ) # Report intermediate value for pruning trial . report ( val_loss , epoch ) # Handle pruning if trial . should_prune (): raise optuna . TrialPruned () return val_loss # Run optimization study = optuna . create_study ( direction = 'minimize' , pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 10 ) ) study . optimize ( lambda trial : objective ( trial , X_train , y_train , X_val , y_val ), n_trials = 50 , timeout = 7200 # 2 hours ) # Print results print ( \" \\n Best hyperparameters:\" ) for key , value in study . best_params . items (): print ( f \" { key } : { value } \" ) print ( f \" \\n Best validation loss: { study . best_value : .4f } \" ) # Visualize fig1 = plot_optimization_history ( study ) fig1 . write_image ( 'optimization_history.png' ) fig2 = plot_param_importances ( study ) fig2 . write_image ( 'param_importances.png' ) # Train final model with best parameters best_config = study . best_params final_model = MolecularFNN ( input_dim = X_train . shape [ 1 ], hidden_dims = [ best_config [ 'hidden_dim_1' ], best_config [ 'hidden_dim_2' ], best_config [ 'hidden_dim_3' ]], output_dim = 1 , dropout_rate = best_config [ 'dropout_rate' ] ) 9.2 Complete Debugging Checklist \u00b6 Pre-Training Checks: def pre_training_diagnostics ( model , train_loader , device = 'cpu' ): \"\"\" Run diagnostics before training \"\"\" print ( \"=\" * 70 ) print ( \"PRE-TRAINING DIAGNOSTICS\" ) print ( \"=\" * 70 ) model = model . to ( device ) # 1. Check data loading print ( \" \\n 1. Data Loading Check:\" ) try : batch_x , batch_y = next ( iter ( train_loader )) print ( f \" \u2713 Batch shapes: X= { batch_x . shape } , Y= { batch_y . shape } \" ) print ( f \" \u2713 X range: [ { batch_x . min () : .3f } , { batch_x . max () : .3f } ]\" ) print ( f \" \u2713 Y range: [ { batch_y . min () : .3f } , { batch_y . max () : .3f } ]\" ) print ( f \" \u2713 No NaN in X: { not torch . isnan ( batch_x ) . any () } \" ) print ( f \" \u2713 No NaN in Y: { not torch . isnan ( batch_y ) . any () } \" ) except Exception as e : print ( f \" \u2717 Error: { e } \" ) return False # 2. Check forward pass print ( \" \\n 2. Forward Pass Check:\" ) try : model . eval () with torch . no_grad (): batch_x = batch_x . to ( device ) output = model ( batch_x ) print ( f \" \u2713 Output shape: { output . shape } \" ) print ( f \" \u2713 Output range: [ { output . min () : .3f } , { output . max () : .3f } ]\" ) print ( f \" \u2713 No NaN in output: { not torch . isnan ( output ) . any () } \" ) except Exception as e : print ( f \" \u2717 Error: { e } \" ) return False # 3. Check backward pass print ( \" \\n 3. Backward Pass Check:\" ) try : model . train () batch_x = batch_x . to ( device ) batch_y = batch_y . to ( device ) criterion = nn . MSELoss () optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) optimizer . zero_grad () output = model ( batch_x ) loss = criterion ( output , batch_y ) loss . backward () print ( f \" \u2713 Loss value: { loss . item () : .4f } \" ) print ( f \" \u2713 Loss is finite: { torch . isfinite ( loss ) } \" ) # Check gradients has_gradients = False max_grad = 0 for name , param in model . named_parameters (): if param . grad is not None : has_gradients = True max_grad = max ( max_grad , param . grad . abs () . max () . item ()) print ( f \" \u2713 Gradients computed: { has_gradients } \" ) print ( f \" \u2713 Max gradient: { max_grad : .6f } \" ) optimizer . step () print ( f \" \u2713 Optimizer step successful\" ) except Exception as e : print ( f \" \u2717 Error: { e } \" ) return False # 4. Check model parameters print ( \" \\n 4. Model Parameters Check:\" ) total_params = sum ( p . numel () for p in model . parameters ()) trainable_params = sum ( p . numel () for p in model . parameters () if p . requires_grad ) print ( f \" \u2713 Total parameters: { total_params : , } \" ) print ( f \" \u2713 Trainable parameters: { trainable_params : , } \" ) # 5. Check learning rate print ( \" \\n 5. Optimizer Check:\" ) for param_group in optimizer . param_groups : print ( f \" \u2713 Learning rate: { param_group [ 'lr' ] } \" ) print ( \" \\n \" + \"=\" * 70 ) print ( \"ALL CHECKS PASSED - READY TO TRAIN\" ) print ( \"=\" * 70 ) return True # Run diagnostics if pre_training_diagnostics ( model , train_loader ): # Start training train_model ( ... ) 9.3 Common Issues and Solutions \u00b6 Comprehensive Troubleshooting Table: Issue Symptoms Possible Causes Solutions Loss is NaN Loss becomes NaN during training - Learning rate too high - Gradient explosion - Invalid inputs - Reduce learning rate by 10x - Add gradient clipping - Check for NaN/Inf in data - Use mixed precision training Loss not decreasing Loss stays constant or increases - Learning rate too low - Wrong loss function - Data not normalized - Model too simple - Increase learning rate - Verify loss function matches task - Normalize inputs - Increase model capacity Overfitting Train loss << Val loss - Model too complex - Too little data - Insufficient regularization - Add dropout (0.3-0.5) - Add L2 regularization - Use data augmentation - Reduce model size - Early stopping Underfitting Both losses high - Model too simple - Training time insufficient - Learning rate issues - Increase model capacity - Train longer - Tune learning rate - Remove excessive regularization Slow training Training takes too long - Batch size too small - Model too large - Inefficient data loading - Increase batch size - Use GPU - Enable data loader workers - Use mixed precision Unstable training Loss oscillates wildly - Learning rate too high - Batch size too small - Poor initialization - Reduce learning rate - Increase batch size - Use learning rate scheduler - Use batch normalization Poor test performance Test worse than validation - Data leakage - Different distribution - Overfitting to val set - Check train/val/test splits - Verify data preprocessing - Use stratified splitting Gradient vanishing Gradients become very small - Too many layers - Wrong activation - Poor initialization - Use ReLU/Leaky ReLU - Reduce number of layers - Use skip connections - Use batch normalization Out of memory CUDA out of memory - Batch size too large - Model too large - Gradient accumulation needed - Reduce batch size - Use gradient checkpointing - Use mixed precision - Clear cache regularly Debugging Code: def debug_training_step ( model , batch_x , batch_y , criterion , optimizer ): \"\"\" Detailed debugging of single training step \"\"\" print ( \" \\n \" + \"=\" * 70 ) print ( \"DEBUGGING TRAINING STEP\" ) print ( \"=\" * 70 ) # 1. Input check print ( \" \\n 1. Input Check:\" ) print ( f \" X shape: { batch_x . shape } , dtype: { batch_x . dtype } \" ) print ( f \" Y shape: { batch_y . shape } , dtype: { batch_y . dtype } \" ) print ( f \" X range: [ { batch_x . min () : .4f } , { batch_x . max () : .4f } ]\" ) print ( f \" Y range: [ { batch_y . min () : .4f } , { batch_y . max () : .4f } ]\" ) print ( f \" X has NaN: { torch . isnan ( batch_x ) . any () } \" ) print ( f \" Y has NaN: { torch . isnan ( batch_y ) . any () } \" ) # 2. Forward pass print ( \" \\n 2. Forward Pass:\" ) model . train () optimizer . zero_grad () output = model ( batch_x ) print ( f \" Output shape: { output . shape } \" ) print ( f \" Output range: [ { output . min () : .4f } , { output . max () : .4f } ]\" ) print ( f \" Output has NaN: { torch . isnan ( output ) . any () } \" ) # 3. Loss computation print ( \" \\n 3. Loss Computation:\" ) loss = criterion ( output , batch_y ) print ( f \" Loss value: { loss . item () : .4f } \" ) print ( f \" Loss is finite: { torch . isfinite ( loss ) } \" ) # 4. Backward pass print ( \" \\n 4. Backward Pass:\" ) loss . backward () # Check gradients print ( \" \\n Gradient Statistics:\" ) for name , param in model . named_parameters (): if param . grad is not None : grad_min = param . grad . min () . item () grad_max = param . grad . max () . item () grad_mean = param . grad . mean () . item () grad_norm = param . grad . norm () . item () print ( f \" { name } :\" ) print ( f \" Range: [ { grad_min : .6f } , { grad_max : .6f } ]\" ) print ( f \" Mean: { grad_mean : .6f } , Norm: { grad_norm : .6f } \" ) print ( f \" Has NaN: { torch . isnan ( param . grad ) . any () } \" ) # 5. Optimizer step print ( \" \\n 5. Optimizer Step:\" ) optimizer . step () print ( \" \u2713 Step completed\" ) # 6. Parameter update check print ( \" \\n 6. Parameter Update Check:\" ) new_output = model ( batch_x ) new_loss = criterion ( new_output , batch_y ) print ( f \" New loss: { new_loss . item () : .4f } \" ) print ( f \" Loss change: { new_loss . item () - loss . item () : .6f } \" ) print ( \" \\n \" + \"=\" * 70 ) # Usage: Run on first batch to debug batch_x , batch_y = next ( iter ( train_loader )) debug_training_step ( model , batch_x , batch_y , criterion , optimizer ) 10. Key Takeaways \u00b6 Core Concepts: Deep Learning Advantages for Molecules Automatic feature learning from raw molecular representations Captures complex non-linear relationships Enables transfer learning and multi-task learning State-of-the-art performance on many molecular property prediction tasks Model Selection Guidelines Feedforward NN : Best for general-purpose molecular property prediction with fingerprints 1D CNN : Excellent for SMILES when local patterns (functional groups) matter LSTM/GRU : Best for sequence modeling and when order matters in SMILES Multi-Task : Use when predicting multiple related properties Transfer Learning : Critical when data is limited (<5K samples) Architecture Best Practices Start simple (2-3 layers) and add complexity only if needed Use ReLU activation for hidden layers Apply batch normalization and dropout for regularization Use Adam optimizer with learning rate 1e-3 as default Implement early stopping (patience=15-20) Training Strategies Always split data into train/val/test (70/15/15) Normalize inputs for faster convergence Use learning rate scheduling (ReduceLROnPlateau) Monitor multiple metrics (RMSE, MAE, R\u00b2) Save best model based on validation loss Performance Optimization Deep learning typically provides 10-20% improvement over Random Forest Transfer learning can improve performance by 15-30% with limited data Multi-task learning helps when tasks are related Ensemble methods (combining multiple models) can add another 5-10% Hyperparameter Importance Ranking Learning rate (most critical) Batch size Number of layers and neurons Dropout rate Optimizer choice (Adam usually best) Common Pitfalls to Avoid Training without validation set Not normalizing inputs Ignoring overfitting signs Using too large learning rate Not checking for data leakage Forgetting to set model.eval() during inference Model Interpretation Matters Use gradient-based methods for feature importance Integrated gradients provide better attributions Attention mechanisms add interpretability Always validate interpretations with domain knowledge Production Considerations Save model architecture and weights separately Version control for models and data Monitor model performance degradation over time Implement confidence scoring for predictions Document training procedures and hyperparameters Research Directions Graph Neural Networks for molecular graphs (Day 3) 3D conformation-based models Active learning for efficient data collection Uncertainty quantification Explainable AI for drug discovery Performance Benchmarks (BBB Permeability): Approach R\u00b2 Score RMSE Training Time Data Required Random Forest 0.75 0.60 Fast (1 min) 1K+ Feedforward NN 0.79 0.52 Medium (15 min) 1K+ 1D CNN (SMILES) 0.82 0.48 Medium (20 min) 2K+ LSTM (SMILES) 0.84 0.45 Slow (45 min) 3K+ Transfer Learning 0.87 0.41 Fast (10 min) 500+ Multi-Task 0.86 0.42 Medium (25 min) 1K+ per task 11. Resources \u00b6 11.1 Essential Papers \u00b6 Deep Learning Fundamentals: 1. LeCun, Y., Bengio, Y., & Hinton, G. (2015). \u201cDeep learning.\u201d Nature , 521(7553), 436-444. 2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning . MIT Press. Molecular Deep Learning: 3. Wu, Z., et al. (2018). \u201cMoleculeNet: A benchmark for molecular machine learning.\u201d Chemical Science , 9(2), 513-530. 4. Wen, M., et al. (2020). \u201cDeep learning for molecular property prediction.\u201d arXiv preprint arXiv:2003.03167. 5. Goh, G. B., et al. (2017). \u201cDeep learning for computational chemistry.\u201d Journal of Computational Chemistry , 38(16), 1291-1307. Architecture-Specific: 6. G\u00f3mez-Bombarelli, R., et al. (2018). \u201cAutomatic chemical design using a data-driven continuous representation of molecules.\u201d ACS Central Science , 4(2), 268-276. 7. Zheng, S., et al. (2020). \u201cIdentifying structure\u2013property relationships through SMILES syntax analysis with self-attention mechanism.\u201d Journal of Chemical Information and Modeling , 59(2), 914-923. 8. Chithrananda, S., et al. (2020). \u201cChemBERTa: Large-scale self-supervised pretraining for molecular property prediction.\u201d arXiv preprint arXiv:2010.09885. Transfer Learning: 9. Hu, W., et al. (2020). \u201cStrategies for pre-training graph neural networks.\u201d ICLR 2020 . 10. Ramsundar, B., et al. (2015). \u201cMassively multitask networks for drug discovery.\u201d arXiv preprint arXiv:1502.02072. Model Interpretation: 11. Sundararajan, M., et al. (2017). \u201cAxiomatic attribution for deep networks.\u201d ICML 2017 . 12. Jim\u00e9nez-Luna, J., et al. (2020). \u201cDrug discovery with explainable artificial intelligence.\u201d Nature Machine Intelligence , 2(10), 573-584. 11.2 Software Libraries \u00b6 Deep Learning Frameworks: - PyTorch : https://pytorch.org/ (Recommended for research) - TensorFlow/Keras : https://www.tensorflow.org/ - JAX : https://github.com/google/jax (For advanced users) Molecular Machine Learning: - DeepChem : https://deepchem.io/ (Comprehensive molecular ML library) - RDKit : https://www.rdkit.org/ (Cheminformatics toolkit) - Mordred : https://github.com/mordred-descriptor/mordred (Molecular descriptors) - ChemProp : https://github.com/chemprop/chemprop (Message passing neural networks) Model Interpretation: - Captum : https://captum.ai/ (PyTorch interpretability) - SHAP : https://github.com/slundberg/shap (SHapley Additive exPlanations) - LIME : https://github.com/marcotcr/lime (Local interpretable model-agnostic explanations) Hyperparameter Tuning: - Optuna : https://optuna.org/ (Automated hyperparameter optimization) - Ray Tune : https://docs.ray.io/en/latest/tune/index.html (Scalable tuning) - Weights & Biases : https://wandb.ai/ (Experiment tracking) 11.3 Datasets \u00b6 Public Molecular Datasets: 1. MoleculeNet : Collection of datasets for molecular property prediction - http://moleculenet.ai/ ZINC Database : 230M purchasable compounds https://zinc.docking.org/ PubChem : 100M+ compounds with biological activities https://pubchem.ncbi.nlm.nih.gov/ ChEMBL : 2M+ bioactive molecules https://www.ebi.ac.uk/chembl/ Tox21 : Toxicity data for 12K compounds https://tripod.nih.gov/tox21/ BBBP (Blood-Brain Barrier Penetration) : 2K molecules Part of MoleculeNet ESOL (Solubility) : 1K molecules with measured solubility https://pubs.acs.org/doi/10.1021/ci034243x 11.4 Online Courses and Tutorials \u00b6 Deep Learning: 1. Deep Learning Specialization (Coursera - Andrew Ng) - https://www.coursera.org/specializations/deep-learning Fast.ai Practical Deep Learning https://course.fast.ai/ Molecular Machine Learning: 3. DeepChem Tutorials - https://deepchem.readthedocs.io/en/latest/get_started/tutorials.html Molecular Machine Learning (MIT) http://molecularml.github.io/ Machine Learning for Drug Discovery (Stanford) http://cs229.stanford.edu/proj2019aut/ 11.5 Useful Blogs and Articles \u00b6 Distill.pub : Interactive ML visualizations https://distill.pub/ Pat Walters\u2019 Blog : Practical cheminformatics http://practicalcheminformatics.blogspot.com/ Is Life Worth Living? : Deep learning for chemistry https://iwatobipen.wordpress.com/ DeepMind Research : Latest AI research https://deepmind.com/research 11.6 Community and Forums \u00b6 RDKit Discussions : https://github.com/rdkit/rdkit/discussions DeepChem Gitter : https://gitter.im/deepchem/Lobby r/MachineLearning : https://www.reddit.com/r/MachineLearning/ r/cheminformatics : https://www.reddit.com/r/cheminformatics/ 12. Homework Assignment \u00b6 Instructions \u00b6 Complete the following 10 exercises to reinforce your understanding of Day 2 concepts. Submit a Jupyter notebook with code, results, and brief explanations. Evaluation Criteria: - Code correctness and clarity (40%) - Results and analysis quality (30%) - Explanations and insights (20%) - Code documentation (10%) Submission Format: - Jupyter notebook (.ipynb) - Include all outputs and visualizations - Add markdown cells with explanations - Ensure code is reproducible Exercise 1: Implement a Custom Neural Network (10 points) \u00b6 Task: Build a feedforward neural network from scratch using only NumPy (no PyTorch/TensorFlow). Requirements: - Implement forward propagation - Implement backward propagation - Train on a simple molecular dataset (e.g., solubility) - Compare performance with PyTorch implementation Deliverables: - Complete implementation with comments - Training loss plot - Comparison table Exercise 2: Activation Function Comparison (8 points) \u00b6 Task: Compare different activation functions on molecular property prediction. Requirements: - Test: ReLU, Leaky ReLU, ELU, SELU, Tanh - Use same architecture (3 hidden layers) - Plot training curves for each - Analyze convergence speed and final performance Deliverables: - Training curves for all activations - Performance comparison table - Analysis of results (2-3 paragraphs) Exercise 3: Implement Multi-Task Learning (12 points) \u00b6 Task: Build a multi-task model to predict multiple molecular properties. Requirements: - Predict at least 3 properties (e.g., solubility, LogP, TPSA) - Implement task weighting (try 3 different strategies) - Compare with single-task models - Visualize shared representations (t-SNE) Deliverables: - Multi-task model implementation - Performance comparison - t-SNE visualization - Analysis of benefits Exercise 4: SMILES CNN Implementation (10 points) \u00b6 Task: Implement and optimize a 1D CNN for SMILES. Requirements: - Test different filter sizes (3, 5, 7, 9) - Test different numbers of filters (32, 64, 128) - Implement data augmentation (SMILES enumeration) - Compare with fingerprint-based model Deliverables: - CNN implementation - Hyperparameter search results - Performance comparison - Best model configuration Exercise 5: LSTM vs GRU Comparison (10 points) \u00b6 Task: Compare LSTM and GRU architectures for SMILES processing. Requirements: - Implement both LSTM and GRU - Add attention mechanism to both - Compare training time, memory usage, performance - Test on sequences of different lengths Deliverables: - Both implementations - Performance metrics table - Training time comparison - Recommendations Exercise 6: Transfer Learning Experiment (12 points) \u00b6 Task: Implement transfer learning for a low-data regime task. Requirements: - Pre-train on large dataset (e.g., solubility, 10K molecules) - Fine-tune on small dataset (BBB permeability, 500 molecules) - Try different freezing strategies - Compare with training from scratch Deliverables: - Pre-training code - Fine-tuning code - Learning curves comparison - Analysis of data efficiency Exercise 7: Model Interpretation (10 points) \u00b6 Task: Implement and compare interpretation methods. Requirements: - Implement gradient-based importance - Implement integrated gradients - Apply to 10 test molecules - Visualize important features - Validate interpretations Deliverables: - Implementation of both methods - Visualizations for 5 molecules - Comparison of methods - Validation with domain knowledge Exercise 8: Hyperparameter Tuning with Optuna (10 points) \u00b6 Task: Use Optuna to find optimal hyperparameters. Requirements: - Define search space for 6+ hyperparameters - Run at least 50 trials - Visualize optimization history - Analyze parameter importance - Train final model with best parameters Deliverables: - Optuna code - Optimization visualizations - Best hyperparameters - Final model performance Exercise 9: Complete Pipeline Development (15 points) \u00b6 Task: Build a complete end-to-end pipeline for a novel dataset. Requirements: - Choose a dataset from MoleculeNet (not used in class) - Data preprocessing and splitting - Feature engineering - Model selection (try 3+ architectures) - Hyperparameter tuning - Final evaluation and error analysis Deliverables: - Complete pipeline code - Detailed README - Results report (1-2 pages) - Error analysis Exercise 10: Research Paper Implementation (13 points) \u00b6 Task: Implement a model from a recent research paper. Suggested Papers: 1. \u201cMolecular graph convolutions: moving beyond fingerprints\u201d (Duvenaud et al., 2015) 2. \u201cAnalyzing learned molecular representations for property prediction\u201d (Yang et al., 2019) 3. \u201cSelf-Attention-Based Molecular Representation\u201d (Zheng et al., 2019) Requirements: - Implement core architecture from paper - Reproduce key results (within 5% of reported performance) - Apply to new dataset - Write implementation notes Deliverables: - Implementation code - Results comparison with paper - Application to new dataset - Implementation notes (1 page) Bonus Exercise: Ensemble Methods (+5 points) \u00b6 Task: Implement ensemble methods to improve predictions. Requirements: - Create ensemble of 5+ models (different architectures) - Try different ensemble strategies (averaging, stacking, voting) - Compare with individual models - Analyze diversity of predictions Deliverables: - Ensemble implementation - Performance comparison - Diversity analysis Submission Guidelines \u00b6 File Structure: homework_day2/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 exercise_1_custom_nn.ipynb \u251c\u2500\u2500 exercise_2_activation_comparison.ipynb \u251c\u2500\u2500 exercise_3_multitask.ipynb \u251c\u2500\u2500 exercise_4_smiles_cnn.ipynb \u251c\u2500\u2500 exercise_5_lstm_gru.ipynb \u251c\u2500\u2500 exercise_6_transfer_learning.ipynb \u251c\u2500\u2500 exercise_7_interpretation.ipynb \u251c\u2500\u2500 exercise_8_optuna.ipynb \u251c\u2500\u2500 exercise_9_complete_pipeline.ipynb \u251c\u2500\u2500 exercise_10_paper_implementation.ipynb \u251c\u2500\u2500 bonus_ensemble.ipynb (optional) \u251c\u2500\u2500 data/ (if applicable) \u2514\u2500\u2500 models/ (saved models) README.md should include: - Student name and ID - Brief description of each exercise - Key findings and insights - Challenges encountered - Total time spent Deadline: 7 days from course date Grading: Total 100 points (+ 5 bonus) 13. Appendix \u00b6 Appendix A: Complete Feedforward NN Template \u00b6 \"\"\" Complete Feedforward Neural Network Template For Molecular Property Prediction This template provides a production-ready implementation with all best practices included. \"\"\" import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error from rdkit import Chem from rdkit.Chem import AllChem import pandas as pd from tqdm import tqdm import matplotlib.pyplot as plt import json import os from datetime import datetime # ============================================================================ # 1. CONFIGURATION # ============================================================================ class Config : \"\"\"Configuration for model training\"\"\" # Data data_path = 'molecular_data.csv' smiles_column = 'SMILES' target_column = 'property' # Features fingerprint_radius = 2 fingerprint_bits = 2048 # Model input_dim = 2048 hidden_dims = [ 512 , 256 , 128 ] output_dim = 1 dropout_rate = 0.3 # Training batch_size = 64 learning_rate = 0.001 weight_decay = 1e-5 num_epochs = 100 patience = 20 # Device device = 'cuda' if torch . cuda . is_available () else 'cpu' # Paths model_dir = 'models' results_dir = 'results' # Random seed random_seed = 42 # ============================================================================ # 2. DATA HANDLING # ============================================================================ class MolecularDataset ( Dataset ): \"\"\"Dataset for molecular property prediction\"\"\" def __init__ ( self , smiles_list , targets , config , scaler = None ): self . config = config self . fingerprints = [] self . targets = [] # Generate fingerprints for smiles , target in tqdm ( zip ( smiles_list , targets ), desc = \"Processing molecules\" ): mol = Chem . MolFromSmiles ( smiles ) if mol is not None : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , config . fingerprint_radius , nBits = config . fingerprint_bits ) self . fingerprints . append ( np . array ( fp )) self . targets . append ( target ) # Convert to tensors self . fingerprints = np . array ( self . fingerprints ) # Scale features if scaler is None : self . scaler = StandardScaler () self . fingerprints = self . scaler . fit_transform ( self . fingerprints ) else : self . scaler = scaler self . fingerprints = self . scaler . transform ( self . fingerprints ) self . fingerprints = torch . FloatTensor ( self . fingerprints ) self . targets = torch . FloatTensor ( self . targets ) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . fingerprints ) def __getitem__ ( self , idx ): return self . fingerprints [ idx ], self . targets [ idx ] # ============================================================================ # 3. MODEL DEFINITION # ============================================================================ class MolecularNN ( nn . Module ): \"\"\"Feedforward neural network for molecular property prediction\"\"\" def __init__ ( self , config ): super ( MolecularNN , self ) . __init__ () layers = [] prev_dim = config . input_dim # Hidden layers for hidden_dim in config . hidden_dims : layers . extend ([ nn . Linear ( prev_dim , hidden_dim ), nn . BatchNorm1d ( hidden_dim ), nn . ReLU (), nn . Dropout ( config . dropout_rate ) ]) prev_dim = hidden_dim # Output layer layers . append ( nn . Linear ( prev_dim , config . output_dim )) self . network = nn . Sequential ( * layers ) # Initialize weights self . _initialize_weights () def _initialize_weights ( self ): \"\"\"He initialization for ReLU networks\"\"\" for m in self . modules (): if isinstance ( m , nn . Linear ): nn . init . kaiming_normal_ ( m . weight , mode = 'fan_in' , nonlinearity = 'relu' ) if m . bias is not None : nn . init . constant_ ( m . bias , 0 ) def forward ( self , x ): return self . network ( x ) # ============================================================================ # 4. TRAINING # ============================================================================ class Trainer : \"\"\"Training manager\"\"\" def __init__ ( self , model , config ): self . model = model self . config = config self . device = torch . device ( config . device ) self . model . to ( self . device ) # Optimizer and criterion self . criterion = nn . MSELoss () self . optimizer = optim . Adam ( model . parameters (), lr = config . learning_rate , weight_decay = config . weight_decay ) self . scheduler = optim . lr_scheduler . ReduceLROnPlateau ( self . optimizer , mode = 'min' , factor = 0.5 , patience = 10 , verbose = True ) # History self . history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : [], 'val_mae' : [], 'val_r2' : [] } # Best model tracking self . best_val_loss = float ( 'inf' ) self . best_model_state = None self . patience_counter = 0 def train_epoch ( self , train_loader ): \"\"\"Train for one epoch\"\"\" self . model . train () total_loss = 0 for batch_x , batch_y in train_loader : batch_x = batch_x . to ( self . device ) batch_y = batch_y . to ( self . device ) self . optimizer . zero_grad () predictions = self . model ( batch_x ) loss = self . criterion ( predictions , batch_y ) loss . backward () # Gradient clipping torch . nn . utils . clip_grad_norm_ ( self . model . parameters (), max_norm = 1.0 ) self . optimizer . step () total_loss += loss . item () return total_loss / len ( train_loader ) def validate ( self , val_loader ): \"\"\"Validate the model\"\"\" self . model . eval () total_loss = 0 all_predictions = [] all_targets = [] with torch . no_grad (): for batch_x , batch_y in val_loader : batch_x = batch_x . to ( self . device ) batch_y = batch_y . to ( self . device ) predictions = self . model ( batch_x ) loss = self . criterion ( predictions , batch_y ) total_loss += loss . item () all_predictions . extend ( predictions . cpu () . numpy ()) all_targets . extend ( batch_y . cpu () . numpy ()) # Calculate metrics avg_loss = total_loss / len ( val_loader ) all_predictions = np . array ( all_predictions ) . flatten () all_targets = np . array ( all_targets ) . flatten () rmse = np . sqrt ( mean_squared_error ( all_targets , all_predictions )) mae = mean_absolute_error ( all_targets , all_predictions ) r2 = r2_score ( all_targets , all_predictions ) return avg_loss , rmse , mae , r2 def train ( self , train_loader , val_loader ): \"\"\"Complete training loop\"\"\" print ( f \"Training on { self . device } \" ) print ( f \"Model has { sum ( p . numel () for p in self . model . parameters ()) : , } parameters\" ) for epoch in range ( self . config . num_epochs ): # Train train_loss = self . train_epoch ( train_loader ) # Validate val_loss , val_rmse , val_mae , val_r2 = self . validate ( val_loader ) # Update history self . history [ 'train_loss' ] . append ( train_loss ) self . history [ 'val_loss' ] . append ( val_loss ) self . history [ 'val_rmse' ] . append ( val_rmse ) self . history [ 'val_mae' ] . append ( val_mae ) self . history [ 'val_r2' ] . append ( val_r2 ) # Learning rate scheduling self . scheduler . step ( val_loss ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \" \\n Epoch { epoch + 1 } / { self . config . num_epochs } \" ) print ( f \" Train Loss: { train_loss : .4f } \" ) print ( f \" Val Loss: { val_loss : .4f } , RMSE: { val_rmse : .4f } , \" f \"MAE: { val_mae : .4f } , R\u00b2: { val_r2 : .4f } \" ) # Early stopping if val_loss < self . best_val_loss : self . best_val_loss = val_loss self . best_model_state = self . model . state_dict () . copy () self . patience_counter = 0 else : self . patience_counter += 1 if self . patience_counter >= self . config . patience : print ( f \" \\n Early stopping at epoch { epoch + 1 } \" ) break # Load best model self . model . load_state_dict ( self . best_model_state ) print ( f \" \\n Training complete! Best validation loss: { self . best_val_loss : .4f } \" ) def save_model ( self , filepath ): \"\"\"Save model and training info\"\"\" torch . save ({ 'model_state_dict' : self . model . state_dict (), 'optimizer_state_dict' : self . optimizer . state_dict (), 'config' : vars ( self . config ), 'history' : self . history , 'best_val_loss' : self . best_val_loss }, filepath ) print ( f \"Model saved to { filepath } \" ) def load_model ( self , filepath ): \"\"\"Load saved model\"\"\" checkpoint = torch . load ( filepath , map_location = self . device ) self . model . load_state_dict ( checkpoint [ 'model_state_dict' ]) self . optimizer . load_state_dict ( checkpoint [ 'optimizer_state_dict' ]) self . history = checkpoint [ 'history' ] print ( f \"Model loaded from { filepath } \" ) # ============================================================================ # 5. EVALUATION # ============================================================================ def evaluate_model ( model , test_loader , device ): \"\"\"Evaluate model on test set\"\"\" model . eval () all_predictions = [] all_targets = [] with torch . no_grad (): for batch_x , batch_y in test_loader : batch_x = batch_x . to ( device ) predictions = model ( batch_x ) all_predictions . extend ( predictions . cpu () . numpy ()) all_targets . extend ( batch_y . numpy ()) all_predictions = np . array ( all_predictions ) . flatten () all_targets = np . array ( all_targets ) . flatten () # Calculate metrics rmse = np . sqrt ( mean_squared_error ( all_targets , all_predictions )) mae = mean_absolute_error ( all_targets , all_predictions ) r2 = r2_score ( all_targets , all_predictions ) print ( \" \\n Test Set Evaluation:\" ) print ( f \" RMSE: { rmse : .4f } \" ) print ( f \" MAE: { mae : .4f } \" ) print ( f \" R\u00b2: { r2 : .4f } \" ) return all_predictions , { 'rmse' : rmse , 'mae' : mae , 'r2' : r2 } def plot_results ( history , predictions , targets , save_path = 'results.png' ): \"\"\"Plot training history and predictions\"\"\" fig , axes = plt . subplots ( 2 , 2 , figsize = ( 12 , 10 )) # Training history axes [ 0 , 0 ] . plot ( history [ 'train_loss' ], label = 'Train Loss' ) axes [ 0 , 0 ] . plot ( history [ 'val_loss' ], label = 'Val Loss' ) axes [ 0 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 0 ] . set_ylabel ( 'Loss' ) axes [ 0 , 0 ] . set_title ( 'Training History' ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( True ) # Metrics axes [ 0 , 1 ] . plot ( history [ 'val_rmse' ], label = 'RMSE' ) axes [ 0 , 1 ] . plot ( history [ 'val_mae' ], label = 'MAE' ) axes [ 0 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 1 ] . set_ylabel ( 'Error' ) axes [ 0 , 1 ] . set_title ( 'Validation Errors' ) axes [ 0 , 1 ] . legend () axes [ 0 , 1 ] . grid ( True ) # Predictions axes [ 1 , 0 ] . scatter ( targets , predictions , alpha = 0.5 ) axes [ 1 , 0 ] . plot ([ targets . min (), targets . max ()], [ targets . min (), targets . max ()], 'r--' , lw = 2 ) axes [ 1 , 0 ] . set_xlabel ( 'True Values' ) axes [ 1 , 0 ] . set_ylabel ( 'Predictions' ) axes [ 1 , 0 ] . set_title ( 'Predictions vs True Values' ) axes [ 1 , 0 ] . grid ( True ) # Residuals residuals = targets - predictions axes [ 1 , 1 ] . hist ( residuals , bins = 30 ) axes [ 1 , 1 ] . set_xlabel ( 'Residuals' ) axes [ 1 , 1 ] . set_ylabel ( 'Frequency' ) axes [ 1 , 1 ] . set_title ( 'Residual Distribution' ) axes [ 1 , 1 ] . grid ( True ) plt . tight_layout () plt . savefig ( save_path , dpi = 150 , bbox_inches = 'tight' ) print ( f \"Results saved to { save_path } \" ) # ============================================================================ # 6. MAIN EXECUTION # ============================================================================ def main (): \"\"\"Main execution function\"\"\" # Set random seeds np . random . seed ( Config . random_seed ) torch . manual_seed ( Config . random_seed ) # Create directories os . makedirs ( Config . model_dir , exist_ok = True ) os . makedirs ( Config . results_dir , exist_ok = True ) # Load data print ( \"Loading data...\" ) df = pd . read_csv ( Config . data_path ) smiles = df [ Config . smiles_column ] . values targets = df [ Config . target_column ] . values # Split data print ( \"Splitting data...\" ) smiles_train , smiles_temp , y_train , y_temp = train_test_split ( smiles , targets , test_size = 0.3 , random_state = Config . random_seed ) smiles_val , smiles_test , y_val , y_test = train_test_split ( smiles_temp , y_temp , test_size = 0.5 , random_state = Config . random_seed ) print ( f \"Train: { len ( smiles_train ) } , Val: { len ( smiles_val ) } , Test: { len ( smiles_test ) } \" ) # Create datasets print ( \"Creating datasets...\" ) train_dataset = MolecularDataset ( smiles_train , y_train , Config ) val_dataset = MolecularDataset ( smiles_val , y_val , Config , scaler = train_dataset . scaler ) test_dataset = MolecularDataset ( smiles_test , y_test , Config , scaler = train_dataset . scaler ) # Create data loaders train_loader = DataLoader ( train_dataset , batch_size = Config . batch_size , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = Config . batch_size ) test_loader = DataLoader ( test_dataset , batch_size = Config . batch_size ) # Create model print ( \"Creating model...\" ) model = MolecularNN ( Config ) # Create trainer trainer = Trainer ( model , Config ) # Train print ( \" \\n Starting training...\" ) trainer . train ( train_loader , val_loader ) # Save model timestamp = datetime . now () . strftime ( \"%Y%m %d _%H%M%S\" ) model_path = os . path . join ( Config . model_dir , f 'model_ { timestamp } .pth' ) trainer . save_model ( model_path ) # Evaluate on test set print ( \" \\n Evaluating on test set...\" ) predictions , metrics = evaluate_model ( model , test_loader , trainer . device ) # Plot results results_path = os . path . join ( Config . results_dir , f 'results_ { timestamp } .png' ) plot_results ( trainer . history , predictions , y_test , results_path ) # Save metrics metrics_path = os . path . join ( Config . results_dir , f 'metrics_ { timestamp } .json' ) with open ( metrics_path , 'w' ) as f : json . dump ( metrics , f , indent = 2 ) print ( \" \\n Pipeline complete!\" ) if __name__ == \"__main__\" : main () Appendix B: SMILES Processing Utilities \u00b6 \"\"\" SMILES Processing Utilities Complete toolkit for SMILES handling, tokenization, and augmentation \"\"\" import numpy as np from rdkit import Chem from rdkit.Chem import AllChem import re # ============================================================================ # SMILES TOKENIZATION # ============================================================================ class SMILESTokenizer : \"\"\"Advanced SMILES tokenizer with comprehensive vocabulary\"\"\" def __init__ ( self ): # Define token patterns (order matters!) self . token_patterns = [ r 'Br' , # Bromine (must come before 'r') r 'Cl' , # Chlorine (must come before 'l') r '@@' , # Chirality r '@' , # Chirality r '\\[([^\\]]+)\\]' , # Bracketed atoms r '[A-Z][a-z]?' , # Elements r '[#%\\(\\)\\+\\-0-9= \\\\ /]' , # Other tokens ] self . regex = re . compile ( '|' . join ( self . token_patterns )) # Special tokens self . pad_token = '<PAD>' self . unk_token = '<UNK>' self . start_token = '<START>' self . end_token = '<END>' self . mask_token = '<MASK>' # Build vocabulary from common SMILES self . _build_vocabulary () def _build_vocabulary ( self ): \"\"\"Build comprehensive vocabulary\"\"\" # Common SMILES tokens common_tokens = [ # Elements 'C' , 'N' , 'O' , 'S' , 'P' , 'F' , 'Cl' , 'Br' , 'I' , 'B' , 'Si' , 'Se' , 'As' , # Aromatic 'c' , 'n' , 'o' , 's' , 'p' , # Bonds '-' , '=' , '#' , '/' , ' \\\\ ' , # Branches '(' , ')' , # Rings '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , '%10' , '%11' , # Chirality '@' , '@@' , # Brackets '[' , ']' , # Charges '+' , '-' , '++' , '--' , # Hydrogens 'H' , ] # Special tokens special_tokens = [ self . pad_token , self . unk_token , self . start_token , self . end_token , self . mask_token ] # Combined vocabulary self . vocab = special_tokens + common_tokens self . token_to_idx = { token : idx for idx , token in enumerate ( self . vocab )} self . idx_to_token = { idx : token for token , idx in self . token_to_idx . items ()} def tokenize ( self , smiles ): \"\"\"Tokenize SMILES string\"\"\" tokens = self . regex . findall ( smiles ) return tokens def encode ( self , smiles , max_length = None , add_special_tokens = True ): \"\"\"Convert SMILES to token indices\"\"\" tokens = self . tokenize ( smiles ) if add_special_tokens : tokens = [ self . start_token ] + tokens + [ self . end_token ] # Convert to indices indices = [ self . token_to_idx . get ( token , self . token_to_idx [ self . unk_token ]) for token in tokens ] # Pad or truncate if max_length is not None : if len ( indices ) < max_length : indices += [ self . token_to_idx [ self . pad_token ]] * ( max_length - len ( indices )) else : indices = indices [: max_length ] return indices def decode ( self , indices ): \"\"\"Convert token indices back to SMILES\"\"\" tokens = [ self . idx_to_token . get ( idx , self . unk_token ) for idx in indices ] # Remove special tokens and padding tokens = [ t for t in tokens if t not in [ self . pad_token , self . start_token , self . end_token ]] return '' . join ( tokens ) def batch_encode ( self , smiles_list , max_length = None , add_special_tokens = True ): \"\"\"Encode batch of SMILES\"\"\" return [ self . encode ( s , max_length , add_special_tokens ) for s in smiles_list ] # ============================================================================ # SMILES VALIDATION AND CANONICALIZATION # ============================================================================ def validate_smiles ( smiles ): \"\"\"Check if SMILES is valid\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) return mol is not None except : return False def canonicalize_smiles ( smiles ): \"\"\"Convert to canonical SMILES\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None return Chem . MolToSmiles ( mol , canonical = True ) except : return None def remove_stereochemistry ( smiles ): \"\"\"Remove stereochemistry information\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None Chem . RemoveStereochemistry ( mol ) return Chem . MolToSmiles ( mol ) except : return None # ============================================================================ # SMILES AUGMENTATION # ============================================================================ def enumerate_smiles ( smiles , n_variants = 10 ): \"\"\"Generate different SMILES representations of same molecule\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) if mol is None : return [ smiles ] variants = set () for _ in range ( n_variants * 2 ): # Generate more, then sample variant = Chem . MolToSmiles ( mol , doRandom = True ) variants . add ( variant ) variants = list ( variants ) # Ensure original is included if smiles not in variants : variants . append ( smiles ) return variants [: n_variants ] except : return [ smiles ] def augment_smiles_dataset ( smiles_list , labels , augmentation_factor = 5 ): \"\"\"Augment SMILES dataset with enumeration\"\"\" augmented_smiles = [] augmented_labels = [] for smiles , label in zip ( smiles_list , labels ): variants = enumerate_smiles ( smiles , n_variants = augmentation_factor ) augmented_smiles . extend ( variants ) augmented_labels . extend ([ label ] * len ( variants )) return augmented_smiles , augmented_labels # ============================================================================ # SMILES FILTERING # ============================================================================ def filter_smiles_dataset ( smiles_list , labels , remove_invalid = True , remove_duplicates = True , max_length = None , min_atoms = None , max_atoms = None ): \"\"\"Filter SMILES dataset based on criteria\"\"\" filtered_smiles = [] filtered_labels = [] seen_canonical = set () for smiles , label in zip ( smiles_list , labels ): # Check validity if remove_invalid : mol = Chem . MolFromSmiles ( smiles ) if mol is None : continue # Check duplicates if remove_duplicates : canonical = canonicalize_smiles ( smiles ) if canonical in seen_canonical : continue seen_canonical . add ( canonical ) # Check length if max_length is not None and len ( smiles ) > max_length : continue # Check atom count if min_atoms is not None or max_atoms is not None : mol = Chem . MolFromSmiles ( smiles ) n_atoms = mol . GetNumHeavyAtoms () if min_atoms is not None and n_atoms < min_atoms : continue if max_atoms is not None and n_atoms > max_atoms : continue filtered_smiles . append ( smiles ) filtered_labels . append ( label ) return filtered_smiles , filtered_labels # ============================================================================ # SMILES STATISTICS # ============================================================================ def compute_smiles_statistics ( smiles_list ): \"\"\"Compute statistics about SMILES dataset\"\"\" stats = { 'total_molecules' : len ( smiles_list ), 'valid_molecules' : 0 , 'length_stats' : {}, 'atom_stats' : {}, 'ring_stats' : {}, } lengths = [] atom_counts = [] ring_counts = [] for smiles in smiles_list : mol = Chem . MolFromSmiles ( smiles ) if mol is not None : stats [ 'valid_molecules' ] += 1 lengths . append ( len ( smiles )) atom_counts . append ( mol . GetNumHeavyAtoms ()) ring_counts . append ( Chem . GetSSSR ( mol )) # Length statistics stats [ 'length_stats' ] = { 'mean' : np . mean ( lengths ), 'std' : np . std ( lengths ), 'min' : np . min ( lengths ), 'max' : np . max ( lengths ), 'median' : np . median ( lengths ) } # Atom statistics stats [ 'atom_stats' ] = { 'mean' : np . mean ( atom_counts ), 'std' : np . std ( atom_counts ), 'min' : np . min ( atom_counts ), 'max' : np . max ( atom_counts ), 'median' : np . median ( atom_counts ) } # Ring statistics stats [ 'ring_stats' ] = { 'mean' : np . mean ( ring_counts ), 'std' : np . std ( ring_counts ), 'min' : np . min ( ring_counts ), 'max' : np . max ( ring_counts ) } return stats # ============================================================================ # EXAMPLE USAGE # ============================================================================ if __name__ == \"__main__\" : # Example SMILES smiles_examples = [ \"CCO\" , # Ethanol \"CC(=O)OC1=CC=CC=C1C(=O)O\" , # Aspirin \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\" , # Caffeine ] # Initialize tokenizer tokenizer = SMILESTokenizer () # Tokenize for smiles in smiles_examples : tokens = tokenizer . tokenize ( smiles ) encoded = tokenizer . encode ( smiles , max_length = 50 ) decoded = tokenizer . decode ( encoded ) print ( f \" \\n SMILES: { smiles } \" ) print ( f \"Tokens: { tokens } \" ) print ( f \"Encoded: { encoded [: 10 ] } ...\" ) print ( f \"Decoded: { decoded } \" ) # Augmentation print ( \" \\n\\n SMILES Enumeration:\" ) variants = enumerate_smiles ( smiles_examples [ 1 ], n_variants = 5 ) for i , variant in enumerate ( variants , 1 ): print ( f \" { i } . { variant } \" ) # Statistics print ( \" \\n\\n Dataset Statistics:\" ) stats = compute_smiles_statistics ( smiles_examples ) print ( f \"Total molecules: { stats [ 'total_molecules' ] } \" ) print ( f \"Valid molecules: { stats [ 'valid_molecules' ] } \" ) print ( f \"Average length: { stats [ 'length_stats' ][ 'mean' ] : .1f } \" ) print ( f \"Average atoms: { stats [ 'atom_stats' ][ 'mean' ] : .1f } \" ) Appendix C: Model Evaluation Suite \u00b6 \"\"\" Comprehensive Model Evaluation Suite Tools for thorough model performance analysis \"\"\" import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import ( mean_squared_error , mean_absolute_error , r2_score , confusion_matrix , roc_auc_score , precision_recall_curve , roc_curve , classification_report ) from scipy import stats import pandas as pd # ============================================================================ # REGRESSION METRICS # ============================================================================ class RegressionEvaluator : \"\"\"Comprehensive regression model evaluation\"\"\" def __init__ ( self , y_true , y_pred ): self . y_true = np . array ( y_true ) . flatten () self . y_pred = np . array ( y_pred ) . flatten () self . residuals = self . y_true - self . y_pred def compute_metrics ( self ): \"\"\"Compute all regression metrics\"\"\" metrics = {} # Basic metrics metrics [ 'MSE' ] = mean_squared_error ( self . y_true , self . y_pred ) metrics [ 'RMSE' ] = np . sqrt ( metrics [ 'MSE' ]) metrics [ 'MAE' ] = mean_absolute_error ( self . y_true , self . y_pred ) metrics [ 'R2' ] = r2_score ( self . y_true , self . y_pred ) # Additional metrics metrics [ 'Max_Error' ] = np . max ( np . abs ( self . residuals )) metrics [ 'Mean_Residual' ] = np . mean ( self . residuals ) metrics [ 'Std_Residual' ] = np . std ( self . residuals ) # Relative metrics metrics [ 'MAPE' ] = np . mean ( np . abs ( self . residuals / ( self . y_true + 1e-10 ))) * 100 # Correlation metrics [ 'Pearson_r' ], metrics [ 'Pearson_p' ] = stats . pearsonr ( self . y_true , self . y_pred ) metrics [ 'Spearman_r' ], metrics [ 'Spearman_p' ] = stats . spearmanr ( self . y_true , self . y_pred ) return metrics def plot_results ( self , save_path = 'regression_evaluation.png' ): \"\"\"Create comprehensive visualization\"\"\" fig , axes = plt . subplots ( 2 , 3 , figsize = ( 18 , 12 )) # 1. Predictions vs True axes [ 0 , 0 ] . scatter ( self . y_true , self . y_pred , alpha = 0.5 ) axes [ 0 , 0 ] . plot ([ self . y_true . min (), self . y_true . max ()], [ self . y_true . min (), self . y_true . max ()], 'r--' , lw = 2 ) axes [ 0 , 0 ] . set_xlabel ( 'True Values' ) axes [ 0 , 0 ] . set_ylabel ( 'Predicted Values' ) axes [ 0 , 0 ] . set_title ( 'Predictions vs True Values' ) axes [ 0 , 0 ] . grid ( True ) # Add R\u00b2 to plot r2 = r2_score ( self . y_true , self . y_pred ) axes [ 0 , 0 ] . text ( 0.05 , 0.95 , f 'R\u00b2 = { r2 : .3f } ' , transform = axes [ 0 , 0 ] . transAxes , verticalalignment = 'top' , bbox = dict ( boxstyle = 'round' , facecolor = 'wheat' , alpha = 0.5 )) # 2. Residuals vs Predicted axes [ 0 , 1 ] . scatter ( self . y_pred , self . residuals , alpha = 0.5 ) axes [ 0 , 1 ] . axhline ( y = 0 , color = 'r' , linestyle = '--' ) axes [ 0 , 1 ] . set_xlabel ( 'Predicted Values' ) axes [ 0 , 1 ] . set_ylabel ( 'Residuals' ) axes [ 0 , 1 ] . set_title ( 'Residual Plot' ) axes [ 0 , 1 ] . grid ( True ) # 3. Residuals Distribution axes [ 0 , 2 ] . hist ( self . residuals , bins = 50 , edgecolor = 'black' ) axes [ 0 , 2 ] . set_xlabel ( 'Residuals' ) axes [ 0 , 2 ] . set_ylabel ( 'Frequency' ) axes [ 0 , 2 ] . set_title ( 'Residual Distribution' ) axes [ 0 , 2 ] . axvline ( x = 0 , color = 'r' , linestyle = '--' ) axes [ 0 , 2 ] . grid ( True ) # 4. Q-Q Plot stats . probplot ( self . residuals , dist = \"norm\" , plot = axes [ 1 , 0 ]) axes [ 1 , 0 ] . set_title ( 'Q-Q Plot' ) axes [ 1 , 0 ] . grid ( True ) # 5. Absolute Error vs True abs_errors = np . abs ( self . residuals ) axes [ 1 , 1 ] . scatter ( self . y_true , abs_errors , alpha = 0.5 ) axes [ 1 , 1 ] . set_xlabel ( 'True Values' ) axes [ 1 , 1 ] . set_ylabel ( 'Absolute Error' ) axes [ 1 , 1 ] . set_title ( 'Absolute Error vs True Values' ) axes [ 1 , 1 ] . grid ( True ) # 6. Error Distribution by Range # Bin true values and compute error statistics bins = np . linspace ( self . y_true . min (), self . y_true . max (), 10 ) bin_indices = np . digitize ( self . y_true , bins ) bin_means = [] bin_stds = [] bin_centers = [] for i in range ( 1 , len ( bins )): mask = bin_indices == i if np . sum ( mask ) > 0 : bin_means . append ( np . mean ( abs_errors [ mask ])) bin_stds . append ( np . std ( abs_errors [ mask ])) bin_centers . append (( bins [ i - 1 ] + bins [ i ]) / 2 ) axes [ 1 , 2 ] . errorbar ( bin_centers , bin_means , yerr = bin_stds , fmt = 'o-' , capsize = 5 ) axes [ 1 , 2 ] . set_xlabel ( 'True Value Range' ) axes [ 1 , 2 ] . set_ylabel ( 'Mean Absolute Error' ) axes [ 1 , 2 ] . set_title ( 'Error by Value Range' ) axes [ 1 , 2 ] . grid ( True ) plt . tight_layout () plt . savefig ( save_path , dpi = 150 , bbox_inches = 'tight' ) plt . show () print ( f \"Evaluation plots saved to { save_path } \" ) def print_report ( self ): \"\"\"Print detailed evaluation report\"\"\" metrics = self . compute_metrics () print ( \" \\n \" + \"=\" * 70 ) print ( \"REGRESSION EVALUATION REPORT\" ) print ( \"=\" * 70 ) print ( \" \\n 1. Basic Metrics:\" ) print ( f \" MSE: { metrics [ 'MSE' ] : .4f } \" ) print ( f \" RMSE: { metrics [ 'RMSE' ] : .4f } \" ) print ( f \" MAE: { metrics [ 'MAE' ] : .4f } \" ) print ( f \" R\u00b2: { metrics [ 'R2' ] : .4f } \" ) print ( \" \\n 2. Error Statistics:\" ) print ( f \" Max Error: { metrics [ 'Max_Error' ] : .4f } \" ) print ( f \" Mean Residual: { metrics [ 'Mean_Residual' ] : .4f } \" ) print ( f \" Std Residual: { metrics [ 'Std_Residual' ] : .4f } \" ) print ( f \" MAPE: { metrics [ 'MAPE' ] : .2f } %\" ) print ( \" \\n 3. Correlation:\" ) print ( f \" Pearson r: { metrics [ 'Pearson_r' ] : .4f } (p= { metrics [ 'Pearson_p' ] : .4e } )\" ) print ( f \" Spearman r: { metrics [ 'Spearman_r' ] : .4f } (p= { metrics [ 'Spearman_p' ] : .4e } )\" ) print ( \" \\n 4. Data Statistics:\" ) print ( f \" True values: Mean= { np . mean ( self . y_true ) : .4f } , \" f \"Std= { np . std ( self . y_true ) : .4f } \" ) print ( f \" Predictions: Mean= { np . mean ( self . y_pred ) : .4f } , \" f \"Std= { np . std ( self . y_pred ) : .4f } \" ) print ( f \" N samples: { len ( self . y_true ) } \" ) print ( \" \\n \" + \"=\" * 70 ) # ============================================================================ # MODEL COMPARISON # ============================================================================ def compare_models ( results_dict , metric = 'RMSE' ): \"\"\" Compare multiple models Args: results_dict: Dict of {model_name: {'y_true': ..., 'y_pred': ...}} metric: Metric to use for comparison \"\"\" comparison_data = [] for model_name , results in results_dict . items (): evaluator = RegressionEvaluator ( results [ 'y_true' ], results [ 'y_pred' ]) metrics = evaluator . compute_metrics () comparison_data . append ({ 'Model' : model_name , 'RMSE' : metrics [ 'RMSE' ], 'MAE' : metrics [ 'MAE' ], 'R2' : metrics [ 'R2' ], 'Pearson_r' : metrics [ 'Pearson_r' ] }) df = pd . DataFrame ( comparison_data ) df = df . sort_values ( by = metric ) print ( \" \\n \" + \"=\" * 70 ) print ( \"MODEL COMPARISON\" ) print ( \"=\" * 70 ) print ( df . to_string ( index = False )) # Plot comparison fig , axes = plt . subplots ( 1 , 3 , figsize = ( 15 , 5 )) metrics_to_plot = [ 'RMSE' , 'MAE' , 'R2' ] for idx , metric in enumerate ( metrics_to_plot ): axes [ idx ] . bar ( df [ 'Model' ], df [ metric ]) axes [ idx ] . set_xlabel ( 'Model' ) axes [ idx ] . set_ylabel ( metric ) axes [ idx ] . set_title ( f ' { metric } Comparison' ) axes [ idx ] . tick_params ( axis = 'x' , rotation = 45 ) axes [ idx ] . grid ( True , axis = 'y' ) plt . tight_layout () plt . savefig ( 'model_comparison.png' , dpi = 150 , bbox_inches = 'tight' ) plt . show () return df # ============================================================================ # EXAMPLE USAGE # ============================================================================ if __name__ == \"__main__\" : # Generate synthetic data np . random . seed ( 42 ) n_samples = 1000 y_true = np . random . randn ( n_samples ) * 2 + 5 y_pred = y_true + np . random . randn ( n_samples ) * 0.5 # Evaluate evaluator = RegressionEvaluator ( y_true , y_pred ) evaluator . print_report () evaluator . plot_results () # Compare models results_dict = { 'Model A' : { 'y_true' : y_true , 'y_pred' : y_pred }, 'Model B' : { 'y_true' : y_true , 'y_pred' : y_true + np . random . randn ( n_samples ) * 0.7 }, 'Model C' : { 'y_true' : y_true , 'y_pred' : y_true + np . random . randn ( n_samples ) * 0.3 }, } compare_models ( results_dict )","title":"Neural Networks"},{"location":"day2/#day__2__deep__learning__for__molecular__systems","text":"Machine Learning for Molecular Systems - Advanced Course Course Module 2: Neural Networks and Deep Learning Applications","title":"Day 2: Deep Learning for Molecular Systems"},{"location":"day2/#table__of__contents","text":"Deep Learning Fundamentals Feedforward Neural Networks Multi-Task Learning Convolutional Neural Networks Recurrent Neural Networks Transfer Learning Complete Practical Exercise Model Interpretation Best Practices Key Takeaways Resources Homework Assignment Appendix","title":"Table of Contents"},{"location":"day2/#1__deep__learning__fundamentals","text":"","title":"1. Deep Learning Fundamentals"},{"location":"day2/#11__why__deep__learning__for__molecules","text":"Deep learning has revolutionized molecular property prediction by automatically learning complex representations from raw molecular data. Unlike traditional machine learning approaches that rely on hand-crafted descriptors, deep learning models can: Key Advantages: Automatic Feature Learning : Neural networks learn hierarchical representations directly from molecular structures (SMILES, graphs, 3D coordinates) without manual feature engineering Complex Pattern Recognition : Capture non-linear relationships and subtle structural patterns that affect molecular properties Transfer Learning : Pre-trained models on large chemical databases can be fine-tuned for specific tasks with limited data End-to-End Learning : Direct mapping from molecular representation to property prediction in a single unified model Multi-Task Learning : Simultaneously predict multiple properties, sharing learned representations across tasks Detailed Examples: Example 1: Solubility Prediction Traditional ML approach requires calculating molecular descriptors (LogP, molecular weight, H-bond donors/acceptors). Deep learning models can learn these patterns directly from SMILES strings: Input: \"CC(=O)OC1=CC=CC=C1C(=O)O\" (Aspirin) Model learns: aromatic rings \u2192 non-polar regions carboxyl groups \u2192 polar regions overall balance \u2192 solubility estimate Output: LogS = -1.5 (moderate solubility) Example 2: Toxicity Prediction Deep learning excels at identifying toxic substructures (toxicophores) without explicit rules: Model automatically learns: - Aromatic amines \u2192 potential carcinogens - Epoxides \u2192 DNA reactivity - Nitro groups \u2192 mutagenicity risk - Combination patterns \u2192 synergistic effects Example 3: Drug-Likeness Rather than using Lipinski\u2019s Rule of Five, neural networks learn implicit drug-likeness: Training on FDA-approved drugs, the model learns: - Optimal molecular weight ranges - Hydrogen bonding patterns - Lipophilicity balance - Metabolic stability indicators - Blood-brain barrier permeability","title":"1.1 Why Deep Learning for Molecules?"},{"location":"day2/#12__neural__network__basics","text":"A neural network consists of interconnected layers of artificial neurons that transform input data through learned weights and biases. Architecture Components: Input Layer : Receives molecular features (fingerprints, descriptors, or embeddings) - Size determined by feature dimension - No activation function applied Hidden Layers : Transform inputs through non-linear operations - Each neuron computes: z = \u03a3(w_i \u00d7 x_i) + b - Applies activation function: a = f(z) - Multiple layers enable hierarchical feature learning Output Layer : Produces final predictions - Regression: Single neuron with linear activation - Binary classification: Single neuron with sigmoid activation - Multi-class: Multiple neurons with softmax activation Mathematical Foundation: For a single neuron: Input: x = [x\u2081, x\u2082, ..., x\u2099] Weights: w = [w\u2081, w\u2082, ..., w\u2099] Bias: b Linear transformation: z = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b = w^T x + b Activation: a = f(z) For a layer: Z^[l] = W^[l] \u00d7 A^[l-1] + b^[l] A^[l] = f(Z^[l]) Where: - l = layer index - W^[l] = weight matrix for layer l - A^[l-1] = activations from previous layer - b^[l] = bias vector for layer l Forward Propagation Example: # Simple 3-layer network import numpy as np def forward_propagation ( X , parameters ): \"\"\" X: input features (n_features, m_samples) parameters: dictionary containing W1, b1, W2, b2, W3, b3 \"\"\" # Layer 1: Input \u2192 Hidden (128 neurons) Z1 = np . dot ( parameters [ 'W1' ], X ) + parameters [ 'b1' ] A1 = relu ( Z1 ) # Layer 2: Hidden \u2192 Hidden (64 neurons) Z2 = np . dot ( parameters [ 'W2' ], A1 ) + parameters [ 'b2' ] A2 = relu ( Z2 ) # Layer 3: Hidden \u2192 Output (1 neuron for regression) Z3 = np . dot ( parameters [ 'W3' ], A2 ) + parameters [ 'b3' ] A3 = Z3 # Linear activation for regression cache = { 'Z1' : Z1 , 'A1' : A1 , 'Z2' : Z2 , 'A2' : A2 , 'Z3' : Z3 , 'A3' : A3 } return A3 , cache","title":"1.2 Neural Network Basics"},{"location":"day2/#13__activation__functions","text":"Activation functions introduce non-linearity, enabling neural networks to learn complex patterns. ReLU (Rectified Linear Unit) f(x) = max(0, x) f'(x) = 1 if x > 0, else 0 Advantages: - Computationally efficient - Helps mitigate vanishing gradient problem - Induces sparsity (many neurons output 0) - Default choice for hidden layers Disadvantages: - Dead ReLU problem (neurons stuck at 0) - Not zero-centered def relu ( x ): return np . maximum ( 0 , x ) def relu_derivative ( x ): return ( x > 0 ) . astype ( float ) Visualization Concept: | / | / | / | / ___|/_________ 0 Leaky ReLU f(x) = max(0.01x, x) Advantages: - Prevents dead neurons - Small gradient for negative values def leaky_relu ( x , alpha = 0.01 ): return np . where ( x > 0 , x , alpha * x ) Sigmoid f(x) = 1 / (1 + e^(-x)) f'(x) = f(x) \u00d7 (1 - f(x)) Use Cases: - Binary classification output layer - Gate mechanisms in LSTM/GRU Disadvantages: - Vanishing gradient problem - Not zero-centered - Computationally expensive def sigmoid ( x ): return 1 / ( 1 + np . exp ( - x )) def sigmoid_derivative ( x ): s = sigmoid ( x ) return s * ( 1 - s ) Tanh (Hyperbolic Tangent) f(x) = (e^x - e^(-x)) / (e^x + e^(-x)) f'(x) = 1 - f(x)\u00b2 Advantages: - Zero-centered (better than sigmoid) - Stronger gradients than sigmoid def tanh ( x ): return np . tanh ( x ) def tanh_derivative ( x ): return 1 - np . tanh ( x ) ** 2 Softmax (Multi-Class Output) f(x_i) = e^(x_i) / \u03a3(e^(x_j)) Properties: - Outputs sum to 1 (probability distribution) - Used for multi-class classification def softmax ( x ): exp_x = np . exp ( x - np . max ( x , axis = 0 , keepdims = True )) # Numerical stability return exp_x / np . sum ( exp_x , axis = 0 , keepdims = True ) Activation Function Selection Guide: Layer Type Recommended Activation Reason Hidden layers (general) ReLU Fast, effective, prevents vanishing gradients Hidden layers (negative values important) Leaky ReLU / ELU Allows negative activations Output (regression) Linear Unrestricted output range Output (binary classification) Sigmoid Output in [0, 1] range Output (multi-class) Softmax Probability distribution Recurrent networks Tanh Zero-centered, bounded","title":"1.3 Activation Functions"},{"location":"day2/#14__loss__functions__and__backpropagation","text":"Loss Functions quantify how well the model\u2019s predictions match the true values. Mean Squared Error (MSE) - Regression L(y, \u0177) = (1/n) \u00d7 \u03a3(y_i - \u0177_i)\u00b2 Characteristics: - Penalizes large errors more heavily - Sensitive to outliers - Smooth gradient def mse_loss ( y_true , y_pred ): return np . mean (( y_true - y_pred ) ** 2 ) def mse_derivative ( y_true , y_pred ): return 2 * ( y_pred - y_true ) / y_true . shape [ 0 ] Mean Absolute Error (MAE) - Robust Regression L(y, \u0177) = (1/n) \u00d7 \u03a3|y_i - \u0177_i| Advantages: - Less sensitive to outliers - All errors weighted equally def mae_loss ( y_true , y_pred ): return np . mean ( np . abs ( y_true - y_pred )) Binary Cross-Entropy - Binary Classification L(y, \u0177) = -(1/n) \u00d7 \u03a3[y_i \u00d7 log(\u0177_i) + (1-y_i) \u00d7 log(1-\u0177_i)] Used when: - Predicting probability of single class - Output: sigmoid activation def binary_crossentropy ( y_true , y_pred ): epsilon = 1e-15 # Prevent log(0) y_pred = np . clip ( y_pred , epsilon , 1 - epsilon ) return - np . mean ( y_true * np . log ( y_pred ) + ( 1 - y_true ) * np . log ( 1 - y_pred )) Categorical Cross-Entropy - Multi-Class Classification L(y, \u0177) = -(1/n) \u00d7 \u03a3 \u03a3 y_ij \u00d7 log(\u0177_ij) Used when: - Multiple mutually exclusive classes - Output: softmax activation def categorical_crossentropy ( y_true , y_pred ): epsilon = 1e-15 y_pred = np . clip ( y_pred , epsilon , 1 - epsilon ) return - np . sum ( y_true * np . log ( y_pred )) / y_true . shape [ 0 ] Backpropagation Algorithm Backpropagation computes gradients of the loss with respect to all parameters using the chain rule. Algorithm Steps: Forward Pass : Compute predictions and loss Output Layer Gradient : \u2202L/\u2202a^[L] Backward Pass : Compute gradients layer by layer Parameter Update : Update weights and biases Mathematical Derivation: For layer l: dZ^[l] = dA^[l] \u2299 f'(Z^[l]) dW^[l] = (1/m) \u00d7 dZ^[l] \u00d7 A^[l-1]^T db^[l] = (1/m) \u00d7 \u03a3 dZ^[l] dA^[l-1] = W^[l]^T \u00d7 dZ^[l] Where: - \u2299 represents element-wise multiplication - f' is the derivative of activation function - m is the number of training examples Complete Backpropagation Implementation: def backward_propagation ( X , Y , cache , parameters ): \"\"\" Implements backpropagation for 3-layer network Args: X: input features Y: true labels cache: forward propagation cache parameters: model parameters (W1, b1, W2, b2, W3, b3) Returns: gradients: dictionary containing dW1, db1, dW2, db2, dW3, db3 \"\"\" m = X . shape [ 1 ] # Retrieve cached values A1 , A2 , A3 = cache [ 'A1' ], cache [ 'A2' ], cache [ 'A3' ] Z1 , Z2 = cache [ 'Z1' ], cache [ 'Z2' ] # Output layer (Layer 3) dZ3 = A3 - Y # For MSE loss with linear activation dW3 = ( 1 / m ) * np . dot ( dZ3 , A2 . T ) db3 = ( 1 / m ) * np . sum ( dZ3 , axis = 1 , keepdims = True ) # Hidden layer 2 dA2 = np . dot ( parameters [ 'W3' ] . T , dZ3 ) dZ2 = dA2 * relu_derivative ( Z2 ) dW2 = ( 1 / m ) * np . dot ( dZ2 , A1 . T ) db2 = ( 1 / m ) * np . sum ( dZ2 , axis = 1 , keepdims = True ) # Hidden layer 1 dA1 = np . dot ( parameters [ 'W2' ] . T , dZ2 ) dZ1 = dA1 * relu_derivative ( Z1 ) dW1 = ( 1 / m ) * np . dot ( dZ1 , X . T ) db1 = ( 1 / m ) * np . sum ( dZ1 , axis = 1 , keepdims = True ) gradients = { 'dW1' : dW1 , 'db1' : db1 , 'dW2' : dW2 , 'db2' : db2 , 'dW3' : dW3 , 'db3' : db3 } return gradients","title":"1.4 Loss Functions and Backpropagation"},{"location":"day2/#15__optimization__algorithms","text":"Optimization algorithms update model parameters to minimize the loss function. Gradient Descent (Batch) Updates parameters using the entire dataset: W := W - \u03b1 \u00d7 \u2202L/\u2202W b := b - \u03b1 \u00d7 \u2202L/\u2202b Where \u03b1 is the learning rate Pros : Stable convergence, exact gradient Cons : Slow for large datasets, memory intensive def gradient_descent ( parameters , gradients , learning_rate ): \"\"\" Update parameters using batch gradient descent \"\"\" for key in parameters . keys (): parameters [ key ] -= learning_rate * gradients [ 'd' + key ] return parameters Stochastic Gradient Descent (SGD) Updates parameters using one sample at a time: W := W - \u03b1 \u00d7 \u2202L_i/\u2202W (for single sample i) Pros : Fast updates, can escape local minima Cons : Noisy updates, unstable convergence Mini-Batch Gradient Descent Compromise between batch and stochastic (typically 32-256 samples): def mini_batch_gradient_descent ( X , Y , parameters , batch_size = 32 , learning_rate = 0.01 ): \"\"\" Mini-batch gradient descent implementation \"\"\" m = X . shape [ 1 ] num_batches = m // batch_size for i in range ( num_batches ): # Get mini-batch start = i * batch_size end = start + batch_size X_batch = X [:, start : end ] Y_batch = Y [:, start : end ] # Forward propagation A3 , cache = forward_propagation ( X_batch , parameters ) # Backward propagation gradients = backward_propagation ( X_batch , Y_batch , cache , parameters ) # Update parameters parameters = gradient_descent ( parameters , gradients , learning_rate ) return parameters Momentum Accelerates SGD by accumulating velocity in relevant direction: v := \u03b2 \u00d7 v + (1-\u03b2) \u00d7 \u2202L/\u2202W W := W - \u03b1 \u00d7 v Common \u03b2 values: 0.9, 0.99 Benefits: - Smooths out oscillations - Faster convergence - Better navigation of ravines def momentum_optimizer ( parameters , gradients , velocity , beta = 0.9 , learning_rate = 0.01 ): \"\"\" Parameters update with momentum \"\"\" for key in parameters . keys (): # Update velocity velocity [ 'v' + key ] = beta * velocity [ 'v' + key ] + ( 1 - beta ) * gradients [ 'd' + key ] # Update parameters parameters [ key ] -= learning_rate * velocity [ 'v' + key ] return parameters , velocity RMSprop (Root Mean Square Propagation) Adapts learning rate per parameter based on recent gradients: s := \u03b2 \u00d7 s + (1-\u03b2) \u00d7 (\u2202L/\u2202W)\u00b2 W := W - \u03b1 \u00d7 (\u2202L/\u2202W) / \u221a(s + \u03b5) Where \u03b5 is a small constant for numerical stability (typically 1e-8) Benefits: - Adaptive learning rates - Works well for non-stationary objectives - Good for RNNs def rmsprop_optimizer ( parameters , gradients , cache , beta = 0.999 , learning_rate = 0.001 , epsilon = 1e-8 ): \"\"\" RMSprop optimization \"\"\" for key in parameters . keys (): # Update cache (squared gradients) cache [ 's' + key ] = beta * cache [ 's' + key ] + ( 1 - beta ) * gradients [ 'd' + key ] ** 2 # Update parameters parameters [ key ] -= learning_rate * gradients [ 'd' + key ] / ( np . sqrt ( cache [ 's' + key ]) + epsilon ) return parameters , cache Adam (Adaptive Moment Estimation) Combines momentum and RMSprop: m := \u03b2\u2081 \u00d7 m + (1-\u03b2\u2081) \u00d7 \u2202L/\u2202W (momentum) v := \u03b2\u2082 \u00d7 v + (1-\u03b2\u2082) \u00d7 (\u2202L/\u2202W)\u00b2 (RMSprop) m\u0302 := m / (1-\u03b2\u2081^t) (bias correction) v\u0302 := v / (1-\u03b2\u2082^t) (bias correction) W := W - \u03b1 \u00d7 m\u0302 / (\u221av\u0302 + \u03b5) Common values: \u03b2\u2081=0.9, \u03b2\u2082=0.999, \u03b5=1e-8 Benefits: - Most popular optimizer for deep learning - Works well with sparse gradients - Combines benefits of momentum and RMSprop - Bias correction for initialization def adam_optimizer ( parameters , gradients , adam_cache , t , beta1 = 0.9 , beta2 = 0.999 , learning_rate = 0.001 , epsilon = 1e-8 ): \"\"\" Adam optimization with bias correction Args: t: iteration number (for bias correction) \"\"\" for key in parameters . keys (): # Update momentum adam_cache [ 'm' + key ] = beta1 * adam_cache [ 'm' + key ] + ( 1 - beta1 ) * gradients [ 'd' + key ] # Update RMSprop adam_cache [ 'v' + key ] = beta2 * adam_cache [ 'v' + key ] + ( 1 - beta2 ) * gradients [ 'd' + key ] ** 2 # Bias correction m_corrected = adam_cache [ 'm' + key ] / ( 1 - beta1 ** t ) v_corrected = adam_cache [ 'v' + key ] / ( 1 - beta2 ** t ) # Update parameters parameters [ key ] -= learning_rate * m_corrected / ( np . sqrt ( v_corrected ) + epsilon ) return parameters , adam_cache AdamW (Adam with Weight Decay) Adam with decoupled weight decay regularization: W := W - \u03b1 \u00d7 (m\u0302 / (\u221av\u0302 + \u03b5) + \u03bb \u00d7 W) Where \u03bb is the weight decay coefficient Benefits: - Better generalization than Adam - Proper weight decay implementation - State-of-the-art for many tasks # Using PyTorch implementation import torch.optim as optim optimizer = optim . AdamW ( model . parameters (), lr = 0.001 , weight_decay = 0.01 ) Optimizer Selection Guide: Scenario Recommended Optimizer Learning Rate General purpose / starting point Adam 1e-3 Large dataset, need speed SGD with momentum 1e-2 (with decay) RNNs / sequence models Adam or RMSprop 1e-3 to 1e-4 Fine-tuning pretrained models AdamW 1e-5 to 1e-4 Non-stationary problems RMSprop 1e-3 When overfitting occurs AdamW or SGD Lower rates Learning Rate Scheduling: # Learning rate decay def lr_decay ( initial_lr , epoch , decay_rate = 0.95 ): return initial_lr * ( decay_rate ** epoch ) # Step decay def step_decay ( initial_lr , epoch , drop = 0.5 , epochs_drop = 10 ): return initial_lr * ( drop ** np . floor ( epoch / epochs_drop )) # Cosine annealing def cosine_annealing ( initial_lr , epoch , total_epochs ): return initial_lr * 0.5 * ( 1 + np . cos ( np . pi * epoch / total_epochs ))","title":"1.5 Optimization Algorithms"},{"location":"day2/#2__feedforward__neural__networks","text":"","title":"2. Feedforward Neural Networks"},{"location":"day2/#21__architecture__design","text":"A feedforward neural network (FNN) is the simplest type of artificial neural network where information moves in only one direction\u2014forward\u2014from input to output. Design Principles: Layer Size Guidelines: - Input layer : Size = number of molecular features (e.g., 2048 for Morgan fingerprints) - Hidden layers : Start with 128-256 neurons, gradually decrease - Output layer : - Regression: 1 neuron - Binary classification: 1 neuron - Multi-class: Number of classes Architecture Patterns: Pattern 1: Pyramid Structure (Recommended for most molecular tasks) Input (2048) \u2192 Hidden1 (512) \u2192 Hidden2 (256) \u2192 Hidden3 (128) \u2192 Output (1) - Progressively reduces dimensionality - Learns hierarchical features Pattern 2: Hourglass Structure Input (2048) \u2192 Hidden1 (256) \u2192 Hidden2 (128) \u2192 Hidden3 (256) \u2192 Output (1) - Creates bottleneck representation - Useful for learning compressed features Pattern 3: Constant Width Input (2048) \u2192 Hidden1 (256) \u2192 Hidden2 (256) \u2192 Hidden3 (256) \u2192 Output (1) - Maintains capacity throughout - Good for complex non-linear mappings Depth vs Width Trade-off: Architecture Pros Cons Best For Deep & Narrow (5+ layers, 64-128 neurons) Hierarchical features, fewer parameters Harder to train, vanishing gradients Complex patterns, large datasets Shallow & Wide (2-3 layers, 512+ neurons) Easier to train, stable More parameters, less hierarchical Simple patterns, small datasets Balanced (3-4 layers, 128-256 neurons) Good trade-off - General purpose, recommended starting point Complete Architecture Implementation: import torch import torch.nn as nn import torch.nn.functional as F class MolecularFNN ( nn . Module ): \"\"\" Feedforward Neural Network for molecular property prediction \"\"\" def __init__ ( self , input_dim = 2048 , hidden_dims = [ 512 , 256 , 128 ], output_dim = 1 , dropout_rate = 0.3 ): \"\"\" Args: input_dim: Size of input features (e.g., fingerprint length) hidden_dims: List of hidden layer sizes output_dim: Number of output neurons (1 for regression) dropout_rate: Dropout probability for regularization \"\"\" super ( MolecularFNN , self ) . __init__ () # Build layers dynamically layers = [] prev_dim = input_dim for hidden_dim in hidden_dims : layers . append ( nn . Linear ( prev_dim , hidden_dim )) layers . append ( nn . BatchNorm1d ( hidden_dim )) # Batch normalization layers . append ( nn . ReLU ()) layers . append ( nn . Dropout ( dropout_rate )) prev_dim = hidden_dim # Output layer layers . append ( nn . Linear ( prev_dim , output_dim )) self . network = nn . Sequential ( * layers ) # Initialize weights self . _initialize_weights () def _initialize_weights ( self ): \"\"\"He initialization for ReLU networks\"\"\" for m in self . modules (): if isinstance ( m , nn . Linear ): nn . init . kaiming_normal_ ( m . weight , mode = 'fan_in' , nonlinearity = 'relu' ) if m . bias is not None : nn . init . constant_ ( m . bias , 0 ) def forward ( self , x ): \"\"\" Forward pass Args: x: Input tensor of shape (batch_size, input_dim) Returns: Output predictions of shape (batch_size, output_dim) \"\"\" return self . network ( x ) def get_embeddings ( self , x ): \"\"\" Extract learned representations from second-to-last layer Useful for visualization and transfer learning \"\"\" for layer in self . network [: - 1 ]: # All layers except final x = layer ( x ) return x # Example instantiation model = MolecularFNN ( input_dim = 2048 , # Morgan fingerprint size hidden_dims = [ 512 , 256 , 128 ], output_dim = 1 , # Single regression output dropout_rate = 0.3 ) print ( model ) print ( f \" \\n Total parameters: { sum ( p . numel () for p in model . parameters ()) : , } \" )","title":"2.1 Architecture Design"},{"location":"day2/#22__full__training__example","text":"Complete Training Pipeline: import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error from rdkit import Chem from rdkit.Chem import AllChem import pandas as pd from tqdm import tqdm # ============================================================================ # 1. DATA PREPARATION # ============================================================================ class MolecularDataset ( Dataset ): \"\"\"Custom Dataset for molecular data\"\"\" def __init__ ( self , smiles_list , labels , radius = 2 , n_bits = 2048 ): \"\"\" Args: smiles_list: List of SMILES strings labels: Array of target values radius: Morgan fingerprint radius n_bits: Fingerprint length \"\"\" self . fingerprints = [] self . labels = [] for smiles , label in zip ( smiles_list , labels ): mol = Chem . MolFromSmiles ( smiles ) if mol is not None : # Generate Morgan fingerprint fp = AllChem . GetMorganFingerprintAsBitVect ( mol , radius , nBits = n_bits ) self . fingerprints . append ( np . array ( fp )) self . labels . append ( label ) self . fingerprints = torch . FloatTensor ( np . array ( self . fingerprints )) self . labels = torch . FloatTensor ( np . array ( self . labels )) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . fingerprints ) def __getitem__ ( self , idx ): return self . fingerprints [ idx ], self . labels [ idx ] # ============================================================================ # 2. TRAINING FUNCTION # ============================================================================ def train_epoch ( model , train_loader , criterion , optimizer , device ): \"\"\"Train for one epoch\"\"\" model . train () total_loss = 0 for batch_x , batch_y in train_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) # Forward pass optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) # Backward pass loss . backward () optimizer . step () total_loss += loss . item () return total_loss / len ( train_loader ) # ============================================================================ # 3. VALIDATION FUNCTION # ============================================================================ def validate ( model , val_loader , criterion , device ): \"\"\"Validate the model\"\"\" model . eval () total_loss = 0 all_predictions = [] all_labels = [] with torch . no_grad (): for batch_x , batch_y in val_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) total_loss += loss . item () all_predictions . extend ( predictions . cpu () . numpy ()) all_labels . extend ( batch_y . cpu () . numpy ()) avg_loss = total_loss / len ( val_loader ) # Calculate metrics all_predictions = np . array ( all_predictions ) . flatten () all_labels = np . array ( all_labels ) . flatten () rmse = np . sqrt ( mean_squared_error ( all_labels , all_predictions )) mae = mean_absolute_error ( all_labels , all_predictions ) r2 = r2_score ( all_labels , all_predictions ) return avg_loss , rmse , mae , r2 # ============================================================================ # 4. COMPLETE TRAINING PIPELINE # ============================================================================ def train_molecular_model ( smiles_train , y_train , smiles_val , y_val , config = None ): \"\"\" Complete training pipeline for molecular property prediction Args: smiles_train: Training SMILES y_train: Training labels smiles_val: Validation SMILES y_val: Validation labels config: Configuration dictionary Returns: trained_model: Best model history: Training history \"\"\" # Default configuration if config is None : config = { 'input_dim' : 2048 , 'hidden_dims' : [ 512 , 256 , 128 ], 'output_dim' : 1 , 'dropout_rate' : 0.3 , 'learning_rate' : 0.001 , 'batch_size' : 64 , 'epochs' : 100 , 'patience' : 15 , 'device' : 'cuda' if torch . cuda . is_available () else 'cpu' } device = torch . device ( config [ 'device' ]) print ( f \"Using device: { device } \" ) # Create datasets print ( \"Creating datasets...\" ) train_dataset = MolecularDataset ( smiles_train , y_train ) val_dataset = MolecularDataset ( smiles_val , y_val ) # Create data loaders train_loader = DataLoader ( train_dataset , batch_size = config [ 'batch_size' ], shuffle = True , num_workers = 0 ) val_loader = DataLoader ( val_dataset , batch_size = config [ 'batch_size' ], shuffle = False , num_workers = 0 ) # Initialize model model = MolecularFNN ( input_dim = config [ 'input_dim' ], hidden_dims = config [ 'hidden_dims' ], output_dim = config [ 'output_dim' ], dropout_rate = config [ 'dropout_rate' ] ) . to ( device ) # Loss and optimizer criterion = nn . MSELoss () optimizer = optim . Adam ( model . parameters (), lr = config [ 'learning_rate' ]) # Learning rate scheduler scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = 'min' , factor = 0.5 , patience = 5 , verbose = True ) # Training history history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : [], 'val_mae' : [], 'val_r2' : [] } # Early stopping best_val_loss = float ( 'inf' ) patience_counter = 0 best_model_state = None # Training loop print ( \" \\n Starting training...\" ) for epoch in range ( config [ 'epochs' ]): # Train train_loss = train_epoch ( model , train_loader , criterion , optimizer , device ) # Validate val_loss , val_rmse , val_mae , val_r2 = validate ( model , val_loader , criterion , device ) # Learning rate scheduling scheduler . step ( val_loss ) # Save history history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_rmse' ] . append ( val_rmse ) history [ 'val_mae' ] . append ( val_mae ) history [ 'val_r2' ] . append ( val_r2 ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } / { config [ 'epochs' ] } \" ) print ( f \" Train Loss: { train_loss : .4f } \" ) print ( f \" Val Loss: { val_loss : .4f } , RMSE: { val_rmse : .4f } , \" f \"MAE: { val_mae : .4f } , R\u00b2: { val_r2 : .4f } \" ) # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 best_model_state = model . state_dict () . copy () else : patience_counter += 1 if patience_counter >= config [ 'patience' ]: print ( f \" \\n Early stopping at epoch { epoch + 1 } \" ) break # Load best model model . load_state_dict ( best_model_state ) print ( \" \\n Training complete!\" ) print ( f \"Best validation loss: { best_val_loss : .4f } \" ) return model , history # ============================================================================ # 5. EXAMPLE USAGE # ============================================================================ # Generate synthetic data for demonstration def generate_synthetic_data ( n_samples = 1000 ): \"\"\"Generate synthetic molecular data\"\"\" np . random . seed ( 42 ) # Simple SMILES for demonstration (in practice, use real dataset) base_smiles = [ 'CCO' , 'CC(C)O' , 'CCCO' , 'CC(C)CO' , 'CCCCO' ] smiles_list = np . random . choice ( base_smiles , n_samples ) # Synthetic labels (in practice, use real property values) labels = np . random . randn ( n_samples ) * 2 + 5 return smiles_list , labels # Generate data smiles , labels = generate_synthetic_data ( 1000 ) # Split data smiles_train , smiles_temp , y_train , y_temp = train_test_split ( smiles , labels , test_size = 0.3 , random_state = 42 ) smiles_val , smiles_test , y_val , y_test = train_test_split ( smiles_temp , y_temp , test_size = 0.5 , random_state = 42 ) # Train model model , history = train_molecular_model ( smiles_train , y_train , smiles_val , y_val ) # Visualize training history import matplotlib.pyplot as plt plt . figure ( figsize = ( 12 , 4 )) plt . subplot ( 1 , 3 , 1 ) plt . plot ( history [ 'train_loss' ], label = 'Train Loss' ) plt . plot ( history [ 'val_loss' ], label = 'Val Loss' ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . title ( 'Training and Validation Loss' ) plt . subplot ( 1 , 3 , 2 ) plt . plot ( history [ 'val_rmse' ], label = 'RMSE' ) plt . plot ( history [ 'val_mae' ], label = 'MAE' ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Error' ) plt . legend () plt . title ( 'Validation Errors' ) plt . subplot ( 1 , 3 , 3 ) plt . plot ( history [ 'val_r2' ]) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'R\u00b2 Score' ) plt . title ( 'Validation R\u00b2' ) plt . tight_layout () plt . savefig ( 'training_history.png' , dpi = 150 , bbox_inches = 'tight' ) plt . show ()","title":"2.2 Full Training Example"},{"location":"day2/#23__best__practices__and__tips","text":"1. Data Preprocessing Normalize inputs : Scale features to similar ranges from sklearn.preprocessing import StandardScaler scaler = StandardScaler () X_train_scaled = scaler . fit_transform ( X_train ) X_val_scaled = scaler . transform ( X_val ) Handle missing values : Remove or impute before training Remove duplicates : Ensure no data leakage between train/val/test sets 2. Architecture Selection Start simple : Begin with 2-3 hidden layers Increase gradually : Add complexity only if needed Monitor overfitting : If train loss << val loss, model too complex 3. Hyperparameter Tuning Priority Priority Hyperparameter Impact Typical Range HIGH Learning rate Critical for convergence 1e-4 to 1e-2 HIGH Batch size Memory and convergence speed 32, 64, 128, 256 MEDIUM Number of layers Model capacity 2-5 MEDIUM Neurons per layer Model capacity 64, 128, 256, 512 MEDIUM Dropout rate Regularization 0.1-0.5 LOW Optimizer Usually Adam works Adam, AdamW LOW Activation function ReLU usually best ReLU, Leaky ReLU 4. Regularization Techniques # L2 Regularization (Weight Decay) optimizer = optim . Adam ( model . parameters (), lr = 0.001 , weight_decay = 1e-5 ) # Dropout (already in model architecture) nn . Dropout ( p = 0.3 ) # Batch Normalization (already in model architecture) nn . BatchNorm1d ( hidden_dim ) # Early Stopping (implemented in training loop) if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 else : patience_counter += 1 5. Debugging Checklist Problem : Loss is NaN Solution : Reduce learning rate, check for invalid inputs, add gradient clipping torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 1.0 ) Problem : Loss not decreasing Solution : Increase learning rate, check data preprocessing, verify labels Problem : Training loss decreasing but validation loss increasing Solution : Overfitting - increase dropout, add L2 regularization, reduce model size Problem : Both losses high and not improving Solution : Underfitting - increase model capacity, decrease regularization, train longer 6. Monitoring Training # Use TensorBoard for real-time monitoring from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter ( 'runs/experiment_1' ) # During training loop writer . add_scalar ( 'Loss/train' , train_loss , epoch ) writer . add_scalar ( 'Loss/val' , val_loss , epoch ) writer . add_scalar ( 'Metrics/RMSE' , val_rmse , epoch ) writer . add_scalar ( 'Metrics/R2' , val_r2 , epoch ) # View with: tensorboard --logdir=runs 7. Model Saving and Loading # Save complete model state torch . save ({ 'epoch' : epoch , 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (), 'loss' : best_val_loss , 'history' : history }, 'best_model.pth' ) # Load model checkpoint = torch . load ( 'best_model.pth' ) model . load_state_dict ( checkpoint [ 'model_state_dict' ]) optimizer . load_state_dict ( checkpoint [ 'optimizer_state_dict' ]) 8. Tips for Molecular Data Use appropriate fingerprints : Morgan (most common), MACCS, RDKit fingerprints Consider molecular size : Normalize by molecular weight or atom count if relevant Handle invalid SMILES : Filter out molecules that RDKit cannot parse Augmentation : Consider SMILES enumeration for data augmentation # SMILES enumeration for augmentation from rdkit import Chem def enumerate_smiles ( smiles , n_variants = 5 ): \"\"\"Generate different SMILES representations of same molecule\"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : return [ smiles ] variants = [] for _ in range ( n_variants ): variants . append ( Chem . MolToSmiles ( mol , doRandom = True )) return list ( set ( variants ))","title":"2.3 Best Practices and Tips"},{"location":"day2/#3__multi-task__learning","text":"","title":"3. Multi-Task Learning"},{"location":"day2/#31__why__and__when__to__use__multi-task__learning","text":"Multi-task learning (MTL) is a machine learning paradigm where a model simultaneously learns multiple related tasks, sharing representations between tasks. Key Benefits: Improved Generalization : Shared representations act as implicit regularization Data Efficiency : Tasks with more data help tasks with less data Related Features : Capture common patterns across molecular properties Transfer Learning : Learn general molecular representations Computational Efficiency : One model predicts multiple properties When to Use MTL: Ideal Scenarios: - Related Properties : Predicting solubility, LogP, and permeability (all related to molecular polarity) - Limited Data : Some tasks have abundant data, others have scarce data - Shared Features : Tasks depend on similar molecular features - Multiple Endpoints : Drug discovery (ADMET properties all relevant) Not Recommended: - Unrelated Tasks : Predicting solubility and catalytic activity (different mechanisms) - Conflicting Objectives : Tasks requiring opposite feature representations - Single Task is Sufficient : When only one property matters Examples in Drug Discovery: Task Group 1: ADMET Properties \u251c\u2500 Absorption (Caco-2 permeability) \u251c\u2500 Distribution (BBB permeability, plasma protein binding) \u251c\u2500 Metabolism (CYP450 inhibition, metabolic stability) \u251c\u2500 Excretion (clearance, half-life) \u2514\u2500 Toxicity (hERG inhibition, hepatotoxicity) Task Group 2: Physical Properties \u251c\u2500 Solubility (aqueous, LogS) \u251c\u2500 Lipophilicity (LogP, LogD) \u2514\u2500 Permeability (PAMPA, Caco-2) Task Group 3: Biological Activity \u251c\u2500 Target binding (IC50, Ki) \u251c\u2500 Cell viability (CC50) \u2514\u2500 Selectivity across targets","title":"3.1 Why and When to Use Multi-Task Learning"},{"location":"day2/#32__multi-task__architecture","text":"Hard Parameter Sharing (Most Common) All tasks share hidden layers, separate output heads: Input (Molecular Features) | Shared Hidden Layers / | \\ Task 1 Task 2 Task 3 Output Output Output Complete Implementation: import torch import torch.nn as nn class MultiTaskMolecularModel ( nn . Module ): \"\"\" Multi-task neural network for molecular property prediction \"\"\" def __init__ ( self , input_dim = 2048 , shared_dims = [ 512 , 256 ], task_configs = None , dropout_rate = 0.3 ): \"\"\" Args: input_dim: Size of input features shared_dims: List of shared hidden layer sizes task_configs: List of dicts with task specifications [{'name': 'task1', 'output_dim': 1, 'task_type': 'regression'}, ...] dropout_rate: Dropout probability \"\"\" super ( MultiTaskMolecularModel , self ) . __init__ () if task_configs is None : task_configs = [ { 'name' : 'solubility' , 'output_dim' : 1 , 'task_type' : 'regression' }, { 'name' : 'toxicity' , 'output_dim' : 1 , 'task_type' : 'classification' } ] self . task_configs = task_configs self . task_names = [ task [ 'name' ] for task in task_configs ] # Shared layers shared_layers = [] prev_dim = input_dim for hidden_dim in shared_dims : shared_layers . extend ([ nn . Linear ( prev_dim , hidden_dim ), nn . BatchNorm1d ( hidden_dim ), nn . ReLU (), nn . Dropout ( dropout_rate ) ]) prev_dim = hidden_dim self . shared_network = nn . Sequential ( * shared_layers ) # Task-specific heads self . task_heads = nn . ModuleDict () for task in task_configs : task_name = task [ 'name' ] output_dim = task [ 'output_dim' ] # Task-specific layers (2 layers) task_head = nn . Sequential ( nn . Linear ( prev_dim , 128 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 128 , output_dim ) ) self . task_heads [ task_name ] = task_head def forward ( self , x ): \"\"\" Forward pass through shared network and all task heads Args: x: Input tensor (batch_size, input_dim) Returns: Dictionary of predictions for each task \"\"\" # Shared representations shared_features = self . shared_network ( x ) # Task-specific predictions outputs = {} for task_name in self . task_names : outputs [ task_name ] = self . task_heads [ task_name ]( shared_features ) return outputs def get_shared_features ( self , x ): \"\"\"Extract shared representations for visualization or transfer learning\"\"\" return self . shared_network ( x ) # Example instantiation task_configs = [ { 'name' : 'solubility' , 'output_dim' : 1 , 'task_type' : 'regression' }, { 'name' : 'bbb_permeability' , 'output_dim' : 1 , 'task_type' : 'regression' }, { 'name' : 'toxicity' , 'output_dim' : 1 , 'task_type' : 'classification' }, { 'name' : 'cyp450_inhibition' , 'output_dim' : 5 , 'task_type' : 'multi-class' } ] model = MultiTaskMolecularModel ( input_dim = 2048 , shared_dims = [ 512 , 256 ], task_configs = task_configs , dropout_rate = 0.3 ) print ( model ) Soft Parameter Sharing (Advanced) Each task has its own network, but networks are constrained to be similar: class SoftParameterSharingModel ( nn . Module ): \"\"\" Soft parameter sharing: separate networks with similarity constraints \"\"\" def __init__ ( self , input_dim , hidden_dims , num_tasks ): super ( SoftParameterSharingModel , self ) . __init__ () # Create separate networks for each task self . task_networks = nn . ModuleList ([ self . _build_network ( input_dim , hidden_dims ) for _ in range ( num_tasks ) ]) def _build_network ( self , input_dim , hidden_dims ): layers = [] prev_dim = input_dim for hidden_dim in hidden_dims : layers . extend ([ nn . Linear ( prev_dim , hidden_dim ), nn . ReLU () ]) prev_dim = hidden_dim layers . append ( nn . Linear ( prev_dim , 1 )) return nn . Sequential ( * layers ) def forward ( self , x ): outputs = [] for network in self . task_networks : outputs . append ( network ( x )) return torch . cat ( outputs , dim = 1 ) def compute_similarity_loss ( self , lambda_reg = 0.01 ): \"\"\" Regularization term to keep task networks similar \"\"\" similarity_loss = 0 num_tasks = len ( self . task_networks ) for i in range ( num_tasks ): for j in range ( i + 1 , num_tasks ): # L2 distance between parameters for p1 , p2 in zip ( self . task_networks [ i ] . parameters (), self . task_networks [ j ] . parameters ()): similarity_loss += torch . norm ( p1 - p2 , p = 2 ) return lambda_reg * similarity_loss","title":"3.2 Multi-Task Architecture"},{"location":"day2/#33__training__strategies","text":"Multi-Task Loss Function: def compute_multitask_loss ( outputs , labels , task_configs , task_weights = None ): \"\"\" Compute weighted combination of task-specific losses Args: outputs: Dict of predictions {task_name: predictions} labels: Dict of true labels {task_name: labels} task_configs: List of task configurations task_weights: Dict of loss weights {task_name: weight} Returns: total_loss: Combined loss task_losses: Dict of individual task losses \"\"\" if task_weights is None : task_weights = { task [ 'name' ]: 1.0 for task in task_configs } task_losses = {} total_loss = 0 for task in task_configs : task_name = task [ 'name' ] task_type = task [ 'task_type' ] # Skip if no labels for this task in batch if task_name not in labels or labels [ task_name ] is None : continue pred = outputs [ task_name ] true = labels [ task_name ] # Select appropriate loss function if task_type == 'regression' : loss = nn . MSELoss ()( pred , true ) elif task_type == 'classification' : loss = nn . BCEWithLogitsLoss ()( pred , true ) elif task_type == 'multi-class' : loss = nn . CrossEntropyLoss ()( pred , true ) else : raise ValueError ( f \"Unknown task type: { task_type } \" ) task_losses [ task_name ] = loss . item () total_loss += task_weights [ task_name ] * loss return total_loss , task_losses Complete Training Loop: def train_multitask_model ( model , train_loader , val_loader , task_configs , num_epochs = 100 , device = 'cuda' ): \"\"\" Complete multi-task training pipeline \"\"\" model = model . to ( device ) optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , patience = 5 ) # Initialize task weights (can be learned or fixed) task_weights = { task [ 'name' ]: 1.0 for task in task_configs } history = { 'train_loss' : [], 'val_loss' : [], 'task_losses' : {}} best_val_loss = float ( 'inf' ) for epoch in range ( num_epochs ): # Training model . train () train_loss = 0 train_task_losses = { task [ 'name' ]: 0 for task in task_configs } for batch_x , batch_labels in train_loader : batch_x = batch_x . to ( device ) batch_labels = { k : v . to ( device ) if v is not None else None for k , v in batch_labels . items ()} optimizer . zero_grad () outputs = model ( batch_x ) loss , task_losses = compute_multitask_loss ( outputs , batch_labels , task_configs , task_weights ) loss . backward () optimizer . step () train_loss += loss . item () for task_name , task_loss in task_losses . items (): train_task_losses [ task_name ] += task_loss # Validation model . eval () val_loss = 0 val_task_losses = { task [ 'name' ]: 0 for task in task_configs } with torch . no_grad (): for batch_x , batch_labels in val_loader : batch_x = batch_x . to ( device ) batch_labels = { k : v . to ( device ) if v is not None else None for k , v in batch_labels . items ()} outputs = model ( batch_x ) loss , task_losses = compute_multitask_loss ( outputs , batch_labels , task_configs , task_weights ) val_loss += loss . item () for task_name , task_loss in task_losses . items (): val_task_losses [ task_name ] += task_loss # Average losses train_loss /= len ( train_loader ) val_loss /= len ( val_loader ) # Learning rate scheduling scheduler . step ( val_loss ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } / { num_epochs } \" ) print ( f \" Train Loss: { train_loss : .4f } , Val Loss: { val_loss : .4f } \" ) for task in task_configs : task_name = task [ 'name' ] print ( f \" { task_name } : { val_task_losses [ task_name ] / len ( val_loader ) : .4f } \" ) # Save best model if val_loss < best_val_loss : best_val_loss = val_loss torch . save ( model . state_dict (), 'best_multitask_model.pth' ) return model , history","title":"3.3 Training Strategies"},{"location":"day2/#34__task__balancing__techniques","text":"Balancing multiple tasks is crucial for effective multi-task learning. 1. Manual Weight Tuning # Simple fixed weights task_weights = { 'solubility' : 1.0 , 'bbb_permeability' : 2.0 , # Prioritize this task 'toxicity' : 1.5 , 'cyp450_inhibition' : 1.0 } 2. Uncertainty Weighting (Kendall et al., 2018) Learns task weights based on homoscedastic uncertainty: class MultiTaskLossWithUncertainty ( nn . Module ): \"\"\" Automatically learns task weights based on uncertainty \"\"\" def __init__ ( self , num_tasks ): super ( MultiTaskLossWithUncertainty , self ) . __init__ () # Log variance for each task (learned parameters) self . log_vars = nn . Parameter ( torch . zeros ( num_tasks )) def forward ( self , losses ): \"\"\" Args: losses: List of individual task losses Returns: Weighted total loss \"\"\" total_loss = 0 for i , loss in enumerate ( losses ): precision = torch . exp ( - self . log_vars [ i ]) total_loss += precision * loss + self . log_vars [ i ] return total_loss # Usage in training uncertainty_loss = MultiTaskLossWithUncertainty ( num_tasks = 4 ) optimizer = optim . Adam ( list ( model . parameters ()) + list ( uncertainty_loss . parameters ())) 3. Gradient Normalization (GradNorm) Balances tasks by normalizing gradient magnitudes: def compute_gradnorm_weights ( model , losses , alpha = 1.5 ): \"\"\" Compute task weights using GradNorm algorithm Args: model: Neural network model losses: List of task losses alpha: Hyperparameter for balancing (default: 1.5) Returns: Updated task weights \"\"\" # Get gradients for last shared layer shared_params = list ( model . shared_network . parameters ())[ - 1 ] gradients = [] for loss in losses : grad = torch . autograd . grad ( loss , shared_params , retain_graph = True )[ 0 ] gradients . append ( torch . norm ( grad )) # Compute inverse training rate loss_ratios = [ loss / losses [ 0 ] for loss in losses ] mean_loss_ratio = sum ( loss_ratios ) / len ( loss_ratios ) # Compute target gradients target_grads = [ mean_loss_ratio * ( ratio ** alpha ) for ratio in loss_ratios ] # Compute weights weights = [ target / grad for target , grad in zip ( target_grads , gradients )] # Normalize weights = [ w / sum ( weights ) * len ( weights ) for w in weights ] return weights 4. Dynamic Task Prioritization Adjust weights during training based on task performance: class DynamicTaskWeighting : \"\"\" Dynamically adjust task weights during training \"\"\" def __init__ ( self , num_tasks , initial_weights = None ): if initial_weights is None : self . weights = [ 1.0 ] * num_tasks else : self . weights = initial_weights self . loss_history = [[] for _ in range ( num_tasks )] def update_weights ( self , epoch , task_losses , strategy = 'inverse_performance' ): \"\"\" Update weights based on task performance Strategies: - 'inverse_performance': Higher weight for worse-performing tasks - 'uncertainty': Higher weight for high-variance tasks - 'curriculum': Gradually increase difficulty \"\"\" for i , loss in enumerate ( task_losses ): self . loss_history [ i ] . append ( loss ) if epoch < 5 : # Wait for some history return self . weights if strategy == 'inverse_performance' : # Give more weight to tasks with higher recent loss recent_losses = [ np . mean ( history [ - 5 :]) for history in self . loss_history ] self . weights = [ loss / sum ( recent_losses ) * len ( recent_losses ) for loss in recent_losses ] elif strategy == 'uncertainty' : # Give more weight to high-variance tasks variances = [ np . var ( history [ - 10 :]) for history in self . loss_history ] self . weights = [ var / sum ( variances ) * len ( variances ) for var in variances ] return self . weights","title":"3.4 Task Balancing Techniques"},{"location":"day2/#35__performance__comparison","text":"Evaluation Metrics for Multi-Task Learning: def evaluate_multitask_model ( model , test_loader , task_configs , device = 'cuda' ): \"\"\" Comprehensive evaluation of multi-task model \"\"\" model . eval () model = model . to ( device ) # Store predictions and labels predictions = { task [ 'name' ]: [] for task in task_configs } true_labels = { task [ 'name' ]: [] for task in task_configs } with torch . no_grad (): for batch_x , batch_labels in test_loader : batch_x = batch_x . to ( device ) outputs = model ( batch_x ) for task in task_configs : task_name = task [ 'name' ] if task_name in batch_labels and batch_labels [ task_name ] is not None : predictions [ task_name ] . extend ( outputs [ task_name ] . cpu () . numpy () ) true_labels [ task_name ] . extend ( batch_labels [ task_name ] . cpu () . numpy () ) # Compute metrics for each task results = {} for task in task_configs : task_name = task [ 'name' ] task_type = task [ 'task_type' ] pred = np . array ( predictions [ task_name ]) . flatten () true = np . array ( true_labels [ task_name ]) . flatten () if task_type == 'regression' : from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score results [ task_name ] = { 'RMSE' : np . sqrt ( mean_squared_error ( true , pred )), 'MAE' : mean_absolute_error ( true , pred ), 'R2' : r2_score ( true , pred ) } elif task_type == 'classification' : from sklearn.metrics import roc_auc_score , accuracy_score , f1_score pred_binary = ( pred > 0 ) . astype ( int ) results [ task_name ] = { 'ROC-AUC' : roc_auc_score ( true , pred ), 'Accuracy' : accuracy_score ( true , pred_binary ), 'F1' : f1_score ( true , pred_binary ) } return results # Compare with single-task models def compare_single_vs_multitask ( smiles_data , labels_dict , task_configs ): \"\"\" Train single-task models and compare with multi-task model \"\"\" results = { 'single_task' : {}, 'multi_task' : {}} # Train single-task models for task in task_configs : task_name = task [ 'name' ] print ( f \" \\n Training single-task model for { task_name } ...\" ) single_model = MolecularFNN ( input_dim = 2048 , output_dim = 1 ) # Train single_model (code similar to previous examples) # ... results [ 'single_task' ][ task_name ] = evaluate_model ( single_model , test_data ) # Train multi-task model print ( \" \\n Training multi-task model...\" ) multitask_model = MultiTaskMolecularModel ( task_configs = task_configs ) # Train multitask_model # ... results [ 'multi_task' ] = evaluate_multitask_model ( multitask_model , test_data , task_configs ) # Print comparison print ( \" \\n \" + \"=\" * 60 ) print ( \"SINGLE-TASK vs MULTI-TASK COMPARISON\" ) print ( \"=\" * 60 ) for task in task_configs : task_name = task [ 'name' ] print ( f \" \\n { task_name . upper () } :\" ) print ( f \" Single-task RMSE: { results [ 'single_task' ][ task_name ][ 'RMSE' ] : .4f } \" ) print ( f \" Multi-task RMSE: { results [ 'multi_task' ][ task_name ][ 'RMSE' ] : .4f } \" ) improvement = (( results [ 'single_task' ][ task_name ][ 'RMSE' ] - results [ 'multi_task' ][ task_name ][ 'RMSE' ]) / results [ 'single_task' ][ task_name ][ 'RMSE' ] * 100 ) print ( f \" Improvement: { improvement : .2f } %\" ) return results Expected Results: Task Single-Task RMSE Multi-Task RMSE Improvement Solubility 0.85 0.72 15.3% BBB Permeability 0.92 0.79 14.1% Toxicity (AUC) 0.78 0.84 7.7% CYP450 Inhibition 0.88 0.81 8.0% Multi-task learning typically shows 10-20% improvement, especially for tasks with limited data.","title":"3.5 Performance Comparison"},{"location":"day2/#4__convolutional__neural__networks","text":"Convolutional Neural Networks (CNNs) excel at extracting local patterns and spatial hierarchies, making them suitable for sequence and image data representations of molecules.","title":"4. Convolutional Neural Networks"},{"location":"day2/#41__1d__cnns__for__smiles","text":"SMILES strings can be treated as sequences where local substructures (like functional groups) are important patterns. Architecture Overview: SMILES: \"CCO\" \u2192 Embedding \u2192 Conv1D layers \u2192 Pooling \u2192 Dense \u2192 Output Complete Implementation: import torch import torch.nn as nn import torch.nn.functional as F class SMILES_CNN ( nn . Module ): \"\"\" 1D Convolutional Neural Network for SMILES strings \"\"\" def __init__ ( self , vocab_size = 50 , embedding_dim = 128 , num_filters = 64 , filter_sizes = [ 3 , 5 , 7 ], hidden_dim = 256 , output_dim = 1 , dropout_rate = 0.3 ): \"\"\" Args: vocab_size: Size of character vocabulary (typically 40-60 for SMILES) embedding_dim: Dimension of character embeddings num_filters: Number of filters per filter size filter_sizes: List of filter sizes (kernel sizes) hidden_dim: Size of fully connected layer output_dim: Output size (1 for regression) dropout_rate: Dropout probability \"\"\" super ( SMILES_CNN , self ) . __init__ () # Character embedding layer self . embedding = nn . Embedding ( vocab_size , embedding_dim , padding_idx = 0 ) # Multiple convolutional layers with different kernel sizes self . convs = nn . ModuleList ([ nn . Conv1d ( in_channels = embedding_dim , out_channels = num_filters , kernel_size = fs ) for fs in filter_sizes ]) # Fully connected layers self . fc1 = nn . Linear ( len ( filter_sizes ) * num_filters , hidden_dim ) self . fc2 = nn . Linear ( hidden_dim , output_dim ) # Regularization self . dropout = nn . Dropout ( dropout_rate ) self . batch_norm = nn . BatchNorm1d ( len ( filter_sizes ) * num_filters ) def forward ( self , x ): \"\"\" Args: x: Input tensor of shape (batch_size, max_length) Returns: Output predictions of shape (batch_size, output_dim) \"\"\" # Embedding: (batch_size, max_length) -> (batch_size, max_length, embedding_dim) embedded = self . embedding ( x ) # Transpose for Conv1d: (batch_size, embedding_dim, max_length) embedded = embedded . transpose ( 1 , 2 ) # Apply convolutions and max pooling conv_outputs = [] for conv in self . convs : # Convolution + ReLU: (batch_size, num_filters, length - kernel_size + 1) conv_out = F . relu ( conv ( embedded )) # Max pooling: (batch_size, num_filters, 1) pooled = F . max_pool1d ( conv_out , conv_out . size ( 2 )) # Flatten: (batch_size, num_filters) conv_outputs . append ( pooled . squeeze ( 2 )) # Concatenate all filter outputs: (batch_size, len(filter_sizes) * num_filters) concatenated = torch . cat ( conv_outputs , dim = 1 ) # Batch normalization normalized = self . batch_norm ( concatenated ) # Fully connected layers hidden = F . relu ( self . fc1 ( self . dropout ( normalized ))) output = self . fc2 ( self . dropout ( hidden )) return output # ============================================================================ # SMILES Tokenization # ============================================================================ class SMILESTokenizer : \"\"\" Tokenizer for SMILES strings \"\"\" def __init__ ( self ): # Common SMILES tokens self . tokens = [ 'C' , 'N' , 'O' , 'S' , 'F' , 'Cl' , 'Br' , 'I' , 'P' , # Atoms 'c' , 'n' , 'o' , 's' , # Aromatic atoms '=' , '#' , # Bonds '(' , ')' , # Branches '[' , ']' , # Atom properties '+' , '-' , # Charges '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , # Ring numbers '@' , '@@' , # Chirality 'H' , # Hydrogen '/' , ' \\\\ ' # Stereochemistry ] # Special tokens self . special_tokens = [ '<PAD>' , '<UNK>' , '<START>' , '<END>' ] # Create vocabulary self . vocab = self . special_tokens + self . tokens self . token_to_idx = { token : idx for idx , token in enumerate ( self . vocab )} self . idx_to_token = { idx : token for token , idx in self . token_to_idx . items ()} self . pad_idx = self . token_to_idx [ '<PAD>' ] self . unk_idx = self . token_to_idx [ '<UNK>' ] def tokenize ( self , smiles ): \"\"\" Tokenize SMILES string into characters \"\"\" tokens = [] i = 0 while i < len ( smiles ): # Check for two-character tokens (Cl, Br, @@) if i < len ( smiles ) - 1 : two_char = smiles [ i : i + 2 ] if two_char in self . tokens : tokens . append ( two_char ) i += 2 continue # Single character token token = smiles [ i ] tokens . append ( token if token in self . tokens else '<UNK>' ) i += 1 return tokens def encode ( self , smiles , max_length = 100 ): \"\"\" Convert SMILES to integer sequence \"\"\" tokens = self . tokenize ( smiles ) # Convert to indices indices = [ self . token_to_idx . get ( token , self . unk_idx ) for token in tokens ] # Pad or truncate if len ( indices ) < max_length : indices += [ self . pad_idx ] * ( max_length - len ( indices )) else : indices = indices [: max_length ] return indices def batch_encode ( self , smiles_list , max_length = 100 ): \"\"\" Encode batch of SMILES strings \"\"\" return [ self . encode ( smiles , max_length ) for smiles in smiles_list ] # ============================================================================ # Dataset and Training # ============================================================================ class SMILES_Dataset ( Dataset ): \"\"\" Dataset for SMILES strings \"\"\" def __init__ ( self , smiles_list , labels , tokenizer , max_length = 100 ): self . tokenizer = tokenizer self . encoded_smiles = [] self . labels = [] for smiles , label in zip ( smiles_list , labels ): try : encoded = torch . LongTensor ( tokenizer . encode ( smiles , max_length )) self . encoded_smiles . append ( encoded ) self . labels . append ( label ) except : continue self . labels = torch . FloatTensor ( self . labels ) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . encoded_smiles ) def __getitem__ ( self , idx ): return self . encoded_smiles [ idx ], self . labels [ idx ] # Example usage tokenizer = SMILESTokenizer () print ( f \"Vocabulary size: { len ( tokenizer . vocab ) } \" ) # Create model model = SMILES_CNN ( vocab_size = len ( tokenizer . vocab ), embedding_dim = 128 , num_filters = 64 , filter_sizes = [ 3 , 5 , 7 ], hidden_dim = 256 , output_dim = 1 , dropout_rate = 0.3 ) # Create dataset smiles_train = [ \"CCO\" , \"CC(C)O\" , \"CCCO\" , \"CC(C)CO\" ] # Example SMILES labels_train = [ 1.5 , 1.8 , 1.2 , 1.6 ] # Example labels train_dataset = SMILES_Dataset ( smiles_train , labels_train , tokenizer , max_length = 100 ) train_loader = DataLoader ( train_dataset , batch_size = 32 , shuffle = True ) # Training loop (similar to previous examples) optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) criterion = nn . MSELoss () for epoch in range ( 50 ): model . train () for batch_smiles , batch_labels in train_loader : optimizer . zero_grad () predictions = model ( batch_smiles ) loss = criterion ( predictions , batch_labels ) loss . backward () optimizer . step () Key Design Choices: Multiple Filter Sizes : Capture patterns of different lengths (3=short, 5=medium, 7=long functional groups) Embedding Layer : Learn distributed representations of SMILES characters Max Pooling : Extract most important features regardless of position Concatenation : Combine features from different filter sizes","title":"4.1 1D CNNs for SMILES"},{"location":"day2/#42__2d__cnns__for__molecular__images","text":"Molecules can be represented as 2D images (chemical structure diagrams) or as heatmaps of molecular properties. Image Representation Approaches: Chemical Structure Diagrams : Rendered 2D molecular structures 3D Conformer Projections : 2D projections of 3D molecular conformations Property Heatmaps : Grids showing electrostatic potential, electron density, etc. Implementation: import torch import torch.nn as nn import torchvision.models as models class Molecular2DCNN ( nn . Module ): \"\"\" 2D CNN for molecular images \"\"\" def __init__ ( self , num_classes = 1 , pretrained = False ): \"\"\" Args: num_classes: Number of output classes/values pretrained: Whether to use pretrained ImageNet weights \"\"\" super ( Molecular2DCNN , self ) . __init__ () # Option 1: Custom CNN architecture self . conv_layers = nn . Sequential ( # First conv block nn . Conv2d ( 3 , 32 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 32 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ), # Second conv block nn . Conv2d ( 32 , 64 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 64 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ), # Third conv block nn . Conv2d ( 64 , 128 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 128 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ), # Fourth conv block nn . Conv2d ( 128 , 256 , kernel_size = 3 , padding = 1 ), nn . BatchNorm2d ( 256 ), nn . ReLU (), nn . MaxPool2d ( 2 , 2 ) ) # Fully connected layers self . fc_layers = nn . Sequential ( nn . Linear ( 256 * 14 * 14 , 512 ), # Assuming 224x224 input nn . ReLU (), nn . Dropout ( 0.5 ), nn . Linear ( 512 , num_classes ) ) def forward ( self , x ): \"\"\" Args: x: Input images of shape (batch_size, 3, 224, 224) Returns: Output predictions of shape (batch_size, num_classes) \"\"\" x = self . conv_layers ( x ) x = x . view ( x . size ( 0 ), - 1 ) # Flatten x = self . fc_layers ( x ) return x class MolecularResNet ( nn . Module ): \"\"\" ResNet-based model for molecular images (transfer learning) \"\"\" def __init__ ( self , num_classes = 1 , pretrained = True ): super ( MolecularResNet , self ) . __init__ () # Load pretrained ResNet self . resnet = models . resnet50 ( pretrained = pretrained ) # Replace final layer num_features = self . resnet . fc . in_features self . resnet . fc = nn . Linear ( num_features , num_classes ) def forward ( self , x ): return self . resnet ( x ) # Generate molecular images from SMILES from rdkit import Chem from rdkit.Chem import Draw from PIL import Image import io import numpy as np def smiles_to_image ( smiles , size = ( 224 , 224 )): \"\"\" Convert SMILES to image Args: smiles: SMILES string size: Image size (width, height) Returns: numpy array of shape (3, height, width) \"\"\" mol = Chem . MolFromSmiles ( smiles ) if mol is None : # Return blank image if invalid SMILES return np . zeros (( 3 , size [ 1 ], size [ 0 ])) # Draw molecule img = Draw . MolToImage ( mol , size = size ) # Convert to numpy array img_array = np . array ( img ) # Convert to (C, H, W) format if len ( img_array . shape ) == 2 : # Grayscale img_array = np . stack ([ img_array ] * 3 ) else : # RGB img_array = img_array . transpose ( 2 , 0 , 1 ) return img_array / 255.0 # Normalize to [0, 1] class MolecularImageDataset ( Dataset ): \"\"\" Dataset for molecular images \"\"\" def __init__ ( self , smiles_list , labels , transform = None , size = ( 224 , 224 )): self . images = [] self . labels = [] for smiles , label in zip ( smiles_list , labels ): img = smiles_to_image ( smiles , size ) self . images . append ( torch . FloatTensor ( img )) self . labels . append ( label ) self . labels = torch . FloatTensor ( self . labels ) . reshape ( - 1 , 1 ) self . transform = transform def __len__ ( self ): return len ( self . images ) def __getitem__ ( self , idx ): image = self . images [ idx ] if self . transform : image = self . transform ( image ) return image , self . labels [ idx ]","title":"4.2 2D CNNs for Molecular Images"},{"location":"day2/#43__when__to__use__each__approach","text":"Comparison Table: Approach Best For Pros Cons 1D CNN on SMILES - Sequence patterns - Functional groups - Large datasets - Fast training - No molecular rendering - Handles variable length - Limited spatial info - SMILES representation bias 2D CNN on Images - Spatial relationships - Stereochemistry - Visual patterns - Captures 2D structure - Transfer learning from ImageNet - Interpretable - Slow image generation - Fixed size input - Loss of 3D info Graph Neural Networks - Atom/bond relationships - 3D structure - Small molecules - Natural molecular representation - Permutation invariant - Interpretable - More complex implementation - (Covered in Day 3) Decision Guide: START \u251c\u2500 Need fast training? \u2192 1D CNN on SMILES \u251c\u2500 Have molecular images? \u2192 2D CNN \u251c\u2500 3D structure important? \u2192 GNN (Day 3) \u251c\u2500 Sequence patterns important? \u2192 1D CNN or RNN \u2514\u2500 Spatial relationships important? \u2192 2D CNN or GNN Example Use Cases: Use 1D CNN on SMILES: - Toxicity prediction (functional group patterns) - Synthetic accessibility (molecular complexity) - Quick property screening Use 2D CNN on Images: - Structure-activity relationship visualization - Similarity search with visual features - Transfer learning from chemical structure databases Hybrid Approach: class HybridMolecularModel ( nn . Module ): \"\"\" Combine 1D CNN (SMILES) and 2D CNN (images) for robust predictions \"\"\" def __init__ ( self , vocab_size , embedding_dim ): super ( HybridMolecularModel , self ) . __init__ () # 1D CNN branch for SMILES self . smiles_cnn = SMILES_CNN ( vocab_size , embedding_dim ) # 2D CNN branch for images self . image_cnn = Molecular2DCNN () # Fusion layer self . fusion = nn . Linear ( 2 , 1 ) # Combine predictions def forward ( self , smiles , images ): \"\"\" Args: smiles: Encoded SMILES (batch_size, max_length) images: Molecular images (batch_size, 3, 224, 224) Returns: Combined predictions \"\"\" smiles_pred = self . smiles_cnn ( smiles ) image_pred = self . image_cnn ( images ) # Concatenate and fuse combined = torch . cat ([ smiles_pred , image_pred ], dim = 1 ) final_pred = self . fusion ( combined ) return final_pred","title":"4.3 When to Use Each Approach"},{"location":"day2/#5__recurrent__neural__networks","text":"Recurrent Neural Networks (RNNs) process sequential data by maintaining hidden states that capture information from previous time steps. They are ideal for SMILES strings where the order of tokens matters.","title":"5. Recurrent Neural Networks"},{"location":"day2/#51__lstm__for__smiles__sequences","text":"Long Short-Term Memory (LSTM) networks address the vanishing gradient problem in traditional RNNs through gating mechanisms. LSTM Architecture: Input \u2192 Embedding \u2192 LSTM layers \u2192 Final hidden state \u2192 Dense \u2192 Output Complete Implementation: import torch import torch.nn as nn class SMILES_LSTM ( nn . Module ): \"\"\" LSTM model for SMILES-based molecular property prediction \"\"\" def __init__ ( self , vocab_size = 50 , embedding_dim = 128 , hidden_dim = 256 , num_layers = 2 , output_dim = 1 , dropout_rate = 0.3 , bidirectional = True ): \"\"\" Args: vocab_size: Size of SMILES vocabulary embedding_dim: Dimension of character embeddings hidden_dim: Size of LSTM hidden state num_layers: Number of stacked LSTM layers output_dim: Output size (1 for regression) dropout_rate: Dropout between LSTM layers bidirectional: Whether to use bidirectional LSTM \"\"\" super ( SMILES_LSTM , self ) . __init__ () self . hidden_dim = hidden_dim self . num_layers = num_layers self . bidirectional = bidirectional # Embedding layer self . embedding = nn . Embedding ( vocab_size , embedding_dim , padding_idx = 0 ) # LSTM layers self . lstm = nn . LSTM ( input_size = embedding_dim , hidden_size = hidden_dim , num_layers = num_layers , batch_first = True , dropout = dropout_rate if num_layers > 1 else 0 , bidirectional = bidirectional ) # Fully connected layers lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim self . fc = nn . Sequential ( nn . Linear ( lstm_output_dim , 128 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 128 , output_dim ) ) def forward ( self , x ): \"\"\" Args: x: Input tensor of shape (batch_size, sequence_length) Returns: Output predictions of shape (batch_size, output_dim) \"\"\" # Embedding: (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim) embedded = self . embedding ( x ) # LSTM: (batch_size, seq_length, hidden_dim * num_directions) lstm_out , ( hidden , cell ) = self . lstm ( embedded ) # Use final hidden state if self . bidirectional : # Concatenate forward and backward final hidden states hidden_forward = hidden [ - 2 , :, :] hidden_backward = hidden [ - 1 , :, :] final_hidden = torch . cat ([ hidden_forward , hidden_backward ], dim = 1 ) else : final_hidden = hidden [ - 1 , :, :] # Fully connected layers output = self . fc ( final_hidden ) return output # Create model model = SMILES_LSTM ( vocab_size = 50 , embedding_dim = 128 , hidden_dim = 256 , num_layers = 2 , output_dim = 1 , dropout_rate = 0.3 , bidirectional = True ) print ( model ) print ( f \"Total parameters: { sum ( p . numel () for p in model . parameters ()) : , } \" ) LSTM Internals: # Manual LSTM cell implementation for understanding class LSTMCell ( nn . Module ): \"\"\" Single LSTM cell showing internal gates \"\"\" def __init__ ( self , input_size , hidden_size ): super ( LSTMCell , self ) . __init__ () # Gates: forget, input, output, candidate self . W_f = nn . Linear ( input_size + hidden_size , hidden_size ) # Forget gate self . W_i = nn . Linear ( input_size + hidden_size , hidden_size ) # Input gate self . W_o = nn . Linear ( input_size + hidden_size , hidden_size ) # Output gate self . W_c = nn . Linear ( input_size + hidden_size , hidden_size ) # Candidate def forward ( self , x_t , h_prev , c_prev ): \"\"\" Args: x_t: Input at time t (batch_size, input_size) h_prev: Previous hidden state (batch_size, hidden_size) c_prev: Previous cell state (batch_size, hidden_size) Returns: h_t: New hidden state c_t: New cell state \"\"\" # Concatenate input and previous hidden state combined = torch . cat ([ x_t , h_prev ], dim = 1 ) # Forget gate: decides what to forget from cell state f_t = torch . sigmoid ( self . W_f ( combined )) # Input gate: decides what new information to store i_t = torch . sigmoid ( self . W_i ( combined )) # Candidate: new candidate values for cell state c_tilde = torch . tanh ( self . W_c ( combined )) # Update cell state c_t = f_t * c_prev + i_t * c_tilde # Output gate: decides what to output from cell state o_t = torch . sigmoid ( self . W_o ( combined )) # Update hidden state h_t = o_t * torch . tanh ( c_t ) return h_t , c_t Attention Mechanism for LSTM: class SMILES_LSTM_Attention ( nn . Module ): \"\"\" LSTM with attention mechanism for SMILES \"\"\" def __init__ ( self , vocab_size , embedding_dim , hidden_dim , output_dim ): super ( SMILES_LSTM_Attention , self ) . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) self . lstm = nn . LSTM ( embedding_dim , hidden_dim , batch_first = True , bidirectional = True ) # Attention layer self . attention = nn . Linear ( hidden_dim * 2 , 1 ) # Output layer self . fc = nn . Linear ( hidden_dim * 2 , output_dim ) def forward ( self , x ): # Embedding and LSTM embedded = self . embedding ( x ) lstm_out , _ = self . lstm ( embedded ) # (batch, seq_len, hidden*2) # Attention weights attention_scores = self . attention ( lstm_out ) # (batch, seq_len, 1) attention_weights = torch . softmax ( attention_scores , dim = 1 ) # Weighted sum of LSTM outputs context = torch . sum ( attention_weights * lstm_out , dim = 1 ) # (batch, hidden*2) # Output output = self . fc ( context ) return output , attention_weights","title":"5.1 LSTM for SMILES Sequences"},{"location":"day2/#52__gru__alternative","text":"Gated Recurrent Unit (GRU) is a simpler alternative to LSTM with fewer parameters. GRU vs LSTM: Feature LSTM GRU Gates 3 (input, forget, output) 2 (reset, update) Cell state Separate (c_t and h_t) Unified (h_t only) Parameters More Fewer (~25% less) Training speed Slower Faster Performance Slightly better on complex tasks Comparable on most tasks Memory Higher Lower GRU Implementation: class SMILES_GRU ( nn . Module ): \"\"\" GRU model for SMILES-based molecular property prediction \"\"\" def __init__ ( self , vocab_size = 50 , embedding_dim = 128 , hidden_dim = 256 , num_layers = 2 , output_dim = 1 , dropout_rate = 0.3 , bidirectional = True ): super ( SMILES_GRU , self ) . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim , padding_idx = 0 ) # GRU layers (API similar to LSTM) self . gru = nn . GRU ( input_size = embedding_dim , hidden_size = hidden_dim , num_layers = num_layers , batch_first = True , dropout = dropout_rate if num_layers > 1 else 0 , bidirectional = bidirectional ) gru_output_dim = hidden_dim * 2 if bidirectional else hidden_dim self . fc = nn . Sequential ( nn . Linear ( gru_output_dim , 128 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 128 , output_dim ) ) def forward ( self , x ): embedded = self . embedding ( x ) gru_out , hidden = self . gru ( embedded ) # Use final hidden state if self . gru . bidirectional : hidden_forward = hidden [ - 2 , :, :] hidden_backward = hidden [ - 1 , :, :] final_hidden = torch . cat ([ hidden_forward , hidden_backward ], dim = 1 ) else : final_hidden = hidden [ - 1 , :, :] output = self . fc ( final_hidden ) return output GRU Internals: class GRUCell ( nn . Module ): \"\"\" Single GRU cell for understanding \"\"\" def __init__ ( self , input_size , hidden_size ): super ( GRUCell , self ) . __init__ () # Gates: reset and update self . W_r = nn . Linear ( input_size + hidden_size , hidden_size ) # Reset gate self . W_z = nn . Linear ( input_size + hidden_size , hidden_size ) # Update gate self . W_h = nn . Linear ( input_size + hidden_size , hidden_size ) # Candidate def forward ( self , x_t , h_prev ): \"\"\" Args: x_t: Input at time t h_prev: Previous hidden state Returns: h_t: New hidden state \"\"\" combined = torch . cat ([ x_t , h_prev ], dim = 1 ) # Reset gate: decides how much past to forget r_t = torch . sigmoid ( self . W_r ( combined )) # Update gate: decides how much to update z_t = torch . sigmoid ( self . W_z ( combined )) # Candidate: new candidate hidden state combined_reset = torch . cat ([ x_t , r_t * h_prev ], dim = 1 ) h_tilde = torch . tanh ( self . W_h ( combined_reset )) # Final hidden state: interpolation between prev and candidate h_t = ( 1 - z_t ) * h_prev + z_t * h_tilde return h_t","title":"5.2 GRU Alternative"},{"location":"day2/#53__comparison__with__cnns","text":"Performance Comparison: def compare_cnn_lstm_gru ( smiles_train , y_train , smiles_val , y_val ): \"\"\" Compare CNN, LSTM, and GRU on same dataset \"\"\" tokenizer = SMILESTokenizer () # Prepare data train_dataset = SMILES_Dataset ( smiles_train , y_train , tokenizer ) val_dataset = SMILES_Dataset ( smiles_val , y_val , tokenizer ) train_loader = DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = 64 ) results = {} # Models to compare models = { '1D CNN' : SMILES_CNN ( vocab_size = len ( tokenizer . vocab )), 'LSTM' : SMILES_LSTM ( vocab_size = len ( tokenizer . vocab )), 'GRU' : SMILES_GRU ( vocab_size = len ( tokenizer . vocab )), 'LSTM+Attention' : SMILES_LSTM_Attention ( vocab_size = len ( tokenizer . vocab )) } for name , model in models . items (): print ( f \" \\n Training { name } ...\" ) optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) criterion = nn . MSELoss () # Training loop train_model ( model , train_loader , val_loader , criterion , optimizer , epochs = 50 ) # Evaluate val_metrics = evaluate_model ( model , val_loader , criterion ) results [ name ] = { 'RMSE' : val_metrics [ 'rmse' ], 'R2' : val_metrics [ 'r2' ], 'Parameters' : sum ( p . numel () for p in model . parameters ()), 'Training time' : val_metrics [ 'time' ] } # Print comparison print ( \" \\n \" + \"=\" * 80 ) print ( \"MODEL COMPARISON\" ) print ( \"=\" * 80 ) print ( f \" { 'Model' : <20 } { 'RMSE' : <10 } { 'R2' : <10 } { 'Parameters' : <15 } { 'Time (s)' : <10 } \" ) print ( \"-\" * 80 ) for name , metrics in results . items (): print ( f \" { name : <20 } { metrics [ 'RMSE' ] : <10.4f } { metrics [ 'R2' ] : <10.4f } \" f \" { metrics [ 'Parameters' ] : <15, } { metrics [ 'Training time' ] : <10.1f } \" ) return results Typical Results: Model RMSE R\u00b2 Parameters Training Time Inference Speed 1D CNN 0.72 0.85 1.2M Fast (1x) Very Fast LSTM 0.68 0.87 2.5M Slow (3x) Slow GRU 0.69 0.86 1.9M Medium (2x) Medium LSTM+Attention 0.65 0.89 2.8M Slowest (3.5x) Slow Recommendations: Use CNN when: Speed is priority, local patterns important, large datasets Use LSTM when: Long-range dependencies matter, sequential information crucial Use GRU when: Want RNN benefits with fewer parameters, faster training Use LSTM+Attention when: Need interpretability, best performance, sufficient data","title":"5.3 Comparison with CNNs"},{"location":"day2/#6__transfer__learning","text":"Transfer learning leverages knowledge learned from one task (usually on large datasets) to improve performance on another related task (often with limited data).","title":"6. Transfer Learning"},{"location":"day2/#61__why__transfer__learning__for__molecules","text":"Challenges in Molecular Machine Learning: Limited Labeled Data : Experimental measurements are expensive Example: Only ~10K molecules with measured BBB permeability Contrast with millions of images in ImageNet Data Imbalance : Some properties measured more than others Solubility: ~100K datapoints Metabolic stability: ~1K datapoints Related Tasks : Many molecular properties share underlying features Lipophilicity and permeability both depend on polarity Multiple ADMET properties relate to molecular shape Benefits of Transfer Learning: Data Efficiency : Achieve good performance with 10-100x less labeled data Faster Convergence : Pre-trained models converge in fewer epochs Better Generalization : Pre-learned features capture general molecular patterns Domain Adaptation : Adapt models trained on one molecule type to another Example Scenario: Problem: Predict BBB permeability (only 5,000 labeled molecules) Solution 1 (From Scratch): \u251c\u2500 Train model on 5,000 BBB molecules \u251c\u2500 Performance: R\u00b2 = 0.65 \u2514\u2500 Training time: 50 epochs Solution 2 (Transfer Learning): \u251c\u2500 Pre-train on 1M molecules (solubility + LogP + toxicity) \u251c\u2500 Fine-tune on 5,000 BBB molecules \u251c\u2500 Performance: R\u00b2 = 0.82 (+26%) \u2514\u2500 Training time: 10 epochs (5x faster)","title":"6.1 Why Transfer Learning for Molecules"},{"location":"day2/#62__pre-training__strategies","text":"1. Self-Supervised Pre-training with Autoencoders Learn molecular representations by reconstructing input: class MolecularAutoencoder ( nn . Module ): \"\"\" Autoencoder for learning molecular representations \"\"\" def __init__ ( self , input_dim = 2048 , encoding_dim = 256 ): super ( MolecularAutoencoder , self ) . __init__ () # Encoder self . encoder = nn . Sequential ( nn . Linear ( input_dim , 1024 ), nn . ReLU (), nn . BatchNorm1d ( 1024 ), nn . Linear ( 1024 , 512 ), nn . ReLU (), nn . BatchNorm1d ( 512 ), nn . Linear ( 512 , encoding_dim ), nn . ReLU () ) # Decoder self . decoder = nn . Sequential ( nn . Linear ( encoding_dim , 512 ), nn . ReLU (), nn . BatchNorm1d ( 512 ), nn . Linear ( 512 , 1024 ), nn . ReLU (), nn . BatchNorm1d ( 1024 ), nn . Linear ( 1024 , input_dim ), nn . Sigmoid () # Output in [0, 1] for fingerprints ) def forward ( self , x ): encoding = self . encoder ( x ) reconstruction = self . decoder ( encoding ) return reconstruction def encode ( self , x ): \"\"\"Extract learned representations\"\"\" return self . encoder ( x ) # Pre-training on large unlabeled dataset def pretrain_autoencoder ( smiles_list , num_epochs = 100 ): \"\"\" Pre-train autoencoder on large molecular dataset \"\"\" # Generate fingerprints fingerprints = [] for smiles in smiles_list : mol = Chem . MolFromSmiles ( smiles ) if mol : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) fingerprints . append ( np . array ( fp )) fingerprints = torch . FloatTensor ( np . array ( fingerprints )) dataset = TensorDataset ( fingerprints , fingerprints ) # Input = target dataloader = DataLoader ( dataset , batch_size = 256 , shuffle = True ) # Create and train autoencoder autoencoder = MolecularAutoencoder ( input_dim = 2048 , encoding_dim = 256 ) optimizer = optim . Adam ( autoencoder . parameters (), lr = 0.001 ) criterion = nn . MSELoss () for epoch in range ( num_epochs ): total_loss = 0 for batch_x , _ in dataloader : optimizer . zero_grad () reconstruction = autoencoder ( batch_x ) loss = criterion ( reconstruction , batch_x ) loss . backward () optimizer . step () total_loss += loss . item () if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } , Loss: { total_loss / len ( dataloader ) : .4f } \" ) return autoencoder # Use pre-trained encoder for downstream task class PretrainedMolecularModel ( nn . Module ): \"\"\" Use pre-trained encoder for property prediction \"\"\" def __init__ ( self , pretrained_encoder , output_dim = 1 , freeze_encoder = False ): super ( PretrainedMolecularModel , self ) . __init__ () self . encoder = pretrained_encoder . encoder # Freeze encoder weights if specified if freeze_encoder : for param in self . encoder . parameters (): param . requires_grad = False # Add prediction head self . prediction_head = nn . Sequential ( nn . Linear ( 256 , 128 ), nn . ReLU (), nn . Dropout ( 0.3 ), nn . Linear ( 128 , output_dim ) ) def forward ( self , x ): features = self . encoder ( x ) predictions = self . prediction_head ( features ) return predictions 2. Multi-Task Pre-training Train on multiple related properties simultaneously: def pretrain_multitask ( smiles_list , properties_dict , task_configs ): \"\"\" Pre-train on multiple molecular properties Args: smiles_list: List of SMILES strings properties_dict: Dict of {property_name: values_array} task_configs: List of task configurations Returns: Pre-trained model \"\"\" # Create multi-task model model = MultiTaskMolecularModel ( input_dim = 2048 , shared_dims = [ 512 , 256 ], task_configs = task_configs ) # Prepare dataset fingerprints = [] labels = { task [ 'name' ]: [] for task in task_configs } for smiles in smiles_list : mol = Chem . MolFromSmiles ( smiles ) if mol : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) fingerprints . append ( np . array ( fp )) for task in task_configs : task_name = task [ 'name' ] labels [ task_name ] . append ( properties_dict [ task_name ]) # Train multi-task model # (Training code similar to Section 3) # ... return model 3. Masked Language Model (for SMILES) Similar to BERT, mask random tokens and predict them: class MaskedSMILESModel ( nn . Module ): \"\"\" BERT-like masked language model for SMILES \"\"\" def __init__ ( self , vocab_size , embedding_dim = 256 , hidden_dim = 512 , num_layers = 6 ): super ( MaskedSMILESModel , self ) . __init__ () self . embedding = nn . Embedding ( vocab_size , embedding_dim ) # Transformer encoder encoder_layer = nn . TransformerEncoderLayer ( d_model = embedding_dim , nhead = 8 , dim_feedforward = hidden_dim , dropout = 0.1 ) self . transformer = nn . TransformerEncoder ( encoder_layer , num_layers = num_layers ) # Prediction head for masked tokens self . mlm_head = nn . Linear ( embedding_dim , vocab_size ) def forward ( self , x , mask = None ): embedded = self . embedding ( x ) encoded = self . transformer ( embedded , src_key_padding_mask = mask ) predictions = self . mlm_head ( encoded ) return predictions def create_masked_data ( smiles_list , tokenizer , mask_prob = 0.15 ): \"\"\" Create masked SMILES for pre-training Args: smiles_list: List of SMILES strings tokenizer: SMILES tokenizer mask_prob: Probability of masking each token Returns: masked_inputs, targets \"\"\" masked_inputs = [] targets = [] for smiles in smiles_list : encoded = tokenizer . encode ( smiles ) masked = encoded . copy () for i in range ( len ( encoded )): if np . random . random () < mask_prob : masked [ i ] = tokenizer . token_to_idx [ '<MASK>' ] masked_inputs . append ( masked ) targets . append ( encoded ) return torch . LongTensor ( masked_inputs ), torch . LongTensor ( targets )","title":"6.2 Pre-Training Strategies"},{"location":"day2/#63__fine-tuning__workflow","text":"Complete Fine-Tuning Pipeline: def fine_tune_pretrained_model ( pretrained_model , smiles_train , y_train , smiles_val , y_val , freeze_layers = True ): \"\"\" Fine-tune pre-trained model on downstream task Args: pretrained_model: Pre-trained neural network smiles_train, y_train: Training data for downstream task smiles_val, y_val: Validation data freeze_layers: Whether to freeze early layers Returns: fine_tuned_model, history \"\"\" # Create model with pre-trained weights model = PretrainedMolecularModel ( pretrained_encoder = pretrained_model , output_dim = 1 , freeze_encoder = freeze_layers ) # Prepare data train_dataset = MolecularDataset ( smiles_train , y_train ) val_dataset = MolecularDataset ( smiles_val , y_val ) train_loader = DataLoader ( train_dataset , batch_size = 64 , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = 64 ) # Use lower learning rate for fine-tuning optimizer = optim . Adam ( model . parameters (), lr = 1e-4 ) # 10x smaller than from-scratch criterion = nn . MSELoss () # Training configuration num_epochs = 30 # Fewer epochs needed best_val_loss = float ( 'inf' ) patience = 10 patience_counter = 0 history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : []} for epoch in range ( num_epochs ): # Training model . train () train_loss = 0 for batch_x , batch_y in train_loader : optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) loss . backward () optimizer . step () train_loss += loss . item () # Validation model . eval () val_loss = 0 all_preds = [] all_labels = [] with torch . no_grad (): for batch_x , batch_y in val_loader : predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) val_loss += loss . item () all_preds . extend ( predictions . numpy ()) all_labels . extend ( batch_y . numpy ()) train_loss /= len ( train_loader ) val_loss /= len ( val_loader ) val_rmse = np . sqrt ( mean_squared_error ( all_labels , all_preds )) history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_rmse' ] . append ( val_rmse ) print ( f \"Epoch { epoch + 1 } / { num_epochs } : \" f \"Train Loss= { train_loss : .4f } , Val Loss= { val_loss : .4f } , RMSE= { val_rmse : .4f } \" ) # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 torch . save ( model . state_dict (), 'best_finetuned_model.pth' ) else : patience_counter += 1 if patience_counter >= patience : print ( f \"Early stopping at epoch { epoch + 1 } \" ) break # Load best model model . load_state_dict ( torch . load ( 'best_finetuned_model.pth' )) return model , history # Gradual unfreezing strategy def gradual_unfreezing ( model , train_loader , val_loader , num_phases = 3 ): \"\"\" Gradually unfreeze layers during fine-tuning Phase 1: Freeze all, train head only Phase 2: Unfreeze top encoder layers Phase 3: Unfreeze all layers \"\"\" all_layers = list ( model . encoder . children ()) criterion = nn . MSELoss () for phase in range ( num_phases ): print ( f \" \\n Phase { phase + 1 } / { num_phases } \" ) # Determine which layers to unfreeze if phase == 0 : # Freeze all encoder layers for param in model . encoder . parameters (): param . requires_grad = False elif phase == 1 : # Unfreeze last 1/3 of encoder layers unfreeze_from = len ( all_layers ) * 2 // 3 for i , layer in enumerate ( all_layers ): if i >= unfreeze_from : for param in layer . parameters (): param . requires_grad = True else : # Unfreeze all layers for param in model . encoder . parameters (): param . requires_grad = True # Use decreasing learning rate for each phase lr = 1e-3 / ( 10 ** phase ) optimizer = optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ()), lr = lr ) # Train for this phase for epoch in range ( 10 ): train_epoch ( model , train_loader , criterion , optimizer ) return model","title":"6.3 Fine-Tuning Workflow"},{"location":"day2/#64__pre-trained__models__chemberta","text":"Using ChemBERTa (Chemical BERT Architecture): from transformers import AutoTokenizer , AutoModel import torch class ChemBERTaFineTuner ( nn . Module ): \"\"\" Fine-tune ChemBERTa for molecular property prediction \"\"\" def __init__ ( self , output_dim = 1 , dropout_rate = 0.3 ): super ( ChemBERTaFineTuner , self ) . __init__ () # Load pre-trained ChemBERTa self . chemberta = AutoModel . from_pretrained ( \"seyonec/ChemBERTa-zinc-base-v1\" ) # Add prediction head hidden_size = self . chemberta . config . hidden_size self . prediction_head = nn . Sequential ( nn . Linear ( hidden_size , 256 ), nn . ReLU (), nn . Dropout ( dropout_rate ), nn . Linear ( 256 , output_dim ) ) def forward ( self , input_ids , attention_mask ): \"\"\" Args: input_ids: Tokenized SMILES (batch_size, seq_length) attention_mask: Attention mask (batch_size, seq_length) Returns: Predictions (batch_size, output_dim) \"\"\" # Get ChemBERTa embeddings outputs = self . chemberta ( input_ids = input_ids , attention_mask = attention_mask ) # Use [CLS] token representation cls_embedding = outputs . last_hidden_state [:, 0 , :] # Prediction predictions = self . prediction_head ( cls_embedding ) return predictions # Usage example def use_chemberta ( smiles_list , labels ): \"\"\" Fine-tune ChemBERTa on your dataset \"\"\" # Load tokenizer tokenizer = AutoTokenizer . from_pretrained ( \"seyonec/ChemBERTa-zinc-base-v1\" ) # Tokenize SMILES encoded = tokenizer ( smiles_list , padding = True , truncation = True , max_length = 512 , return_tensors = 'pt' ) # Create model model = ChemBERTaFineTuner ( output_dim = 1 ) # Training loop optimizer = optim . AdamW ( model . parameters (), lr = 2e-5 ) criterion = nn . MSELoss () model . train () for epoch in range ( 10 ): optimizer . zero_grad () predictions = model ( input_ids = encoded [ 'input_ids' ], attention_mask = encoded [ 'attention_mask' ] ) loss = criterion ( predictions . squeeze (), labels ) loss . backward () optimizer . step () print ( f \"Epoch { epoch + 1 } , Loss: { loss . item () : .4f } \" ) return model","title":"6.4 Pre-Trained Models (ChemBERTa)"},{"location":"day2/#65__results__and__comparisons","text":"Comparison Study: def compare_transfer_learning_strategies ( smiles_train , y_train , smiles_val , y_val ): \"\"\" Compare different transfer learning approaches \"\"\" results = {} # Strategy 1: Train from scratch (baseline) print ( \" \\n 1. Training from scratch...\" ) scratch_model = MolecularFNN ( input_dim = 2048 ) scratch_history = train_model ( scratch_model , smiles_train , y_train , smiles_val , y_val ) results [ 'From Scratch' ] = evaluate_model ( scratch_model , smiles_val , y_val ) # Strategy 2: Pre-trained autoencoder print ( \" \\n 2. Pre-trained autoencoder + fine-tuning...\" ) # Pre-train on large unlabeled dataset (e.g., ZINC database) large_smiles = load_large_dataset () # Assume 1M molecules autoencoder = pretrain_autoencoder ( large_smiles , num_epochs = 100 ) transfer_model = PretrainedMolecularModel ( autoencoder , freeze_encoder = True ) transfer_history = fine_tune_pretrained_model ( transfer_model , smiles_train , y_train , smiles_val , y_val ) results [ 'Autoencoder Transfer' ] = evaluate_model ( transfer_model , smiles_val , y_val ) # Strategy 3: Multi-task pre-training print ( \" \\n 3. Multi-task pre-training + fine-tuning...\" ) multitask_model = pretrain_multitask ( large_smiles , properties_dict , task_configs ) multitask_transfer = fine_tune_pretrained_model ( multitask_model , smiles_train , y_train , smiles_val , y_val ) results [ 'Multi-Task Transfer' ] = evaluate_model ( multitask_transfer , smiles_val , y_val ) # Strategy 4: ChemBERTa print ( \" \\n 4. ChemBERTa fine-tuning...\" ) chemberta_model = ChemBERTaFineTuner () chemberta_history = fine_tune_chemberta ( chemberta_model , smiles_train , y_train , smiles_val , y_val ) results [ 'ChemBERTa' ] = evaluate_model ( chemberta_model , smiles_val , y_val ) # Print comparison print ( \" \\n \" + \"=\" * 70 ) print ( \"TRANSFER LEARNING COMPARISON\" ) print ( \"=\" * 70 ) print ( f \" { 'Strategy' : <30 } { 'RMSE' : <10 } { 'R\u00b2' : <10 } { 'Training Time' : <15 } \" ) print ( \"-\" * 70 ) for strategy , metrics in results . items (): print ( f \" { strategy : <30 } { metrics [ 'RMSE' ] : <10.4f } { metrics [ 'R\u00b2' ] : <10.4f } \" f \" { metrics [ 'time' ] : <15.1f } s\" ) return results Expected Results (BBB Permeability Example): Strategy RMSE R\u00b2 Data Required Training Time Improvement From Scratch 0.95 0.68 5,000 120 min Baseline Autoencoder Transfer 0.78 0.79 5,000 45 min +16% Multi-Task Transfer 0.72 0.82 5,000 35 min +20% ChemBERTa 0.65 0.87 5,000 25 min +28% ChemBERTa (Low Data) 0.82 0.75 500 15 min Still viable Key Insights: Transfer learning provides 15-30% improvement in performance Training time reduced by 60-80% Most effective when target task has limited data (<10K samples) ChemBERTa performs best due to massive pre-training on 77M molecules Multi-task pre-training excellent when related properties available","title":"6.5 Results and Comparisons"},{"location":"day2/#7__complete__practical__exercise","text":"","title":"7. Complete Practical Exercise"},{"location":"day2/#71__problem__bbb__permeability__prediction","text":"Background: Blood-Brain Barrier (BBB) permeability is crucial for CNS drugs. We\u2019ll predict log(BB), the logarithm of the brain-to-blood concentration ratio. Dataset: - Training: 1,500 molecules - Validation: 300 molecules - Test: 200 molecules - Features: SMILES strings - Target: log(BB) values (continuous, range: -3 to 2)","title":"7.1 Problem: BBB Permeability Prediction"},{"location":"day2/#72__full__pipeline","text":"# ============================================================================ # COMPLETE BBB PERMEABILITY PREDICTION PIPELINE # ============================================================================ import numpy as np import pandas as pd import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error , mean_absolute_error , r2_score from rdkit import Chem from rdkit.Chem import AllChem , Descriptors import matplotlib.pyplot as plt import seaborn as sns from tqdm import tqdm import time # Set random seeds for reproducibility np . random . seed ( 42 ) torch . manual_seed ( 42 ) # ============================================================================ # 1. DATA LOADING AND PREPROCESSING # ============================================================================ def load_bbb_data ( filepath = 'bbb_permeability.csv' ): \"\"\" Load BBB permeability dataset Expected columns: SMILES, logBB \"\"\" try : df = pd . read_csv ( filepath ) except : # Generate synthetic data for demonstration print ( \"Generating synthetic BBB data...\" ) df = generate_synthetic_bbb_data ( 2000 ) # Remove invalid SMILES valid_indices = [] for idx , smiles in enumerate ( df [ 'SMILES' ]): mol = Chem . MolFromSmiles ( smiles ) if mol is not None : valid_indices . append ( idx ) df = df . iloc [ valid_indices ] . reset_index ( drop = True ) print ( f \"Loaded { len ( df ) } valid molecules\" ) print ( f \"logBB range: [ { df [ 'logBB' ] . min () : .2f } , { df [ 'logBB' ] . max () : .2f } ]\" ) return df def generate_synthetic_bbb_data ( n_samples = 2000 ): \"\"\" Generate synthetic BBB data for demonstration \"\"\" # Common drug-like SMILES templates templates = [ \"CCO\" , \"CC(C)O\" , \"CCCO\" , \"CC(C)CO\" , \"CCCCO\" , \"c1ccccc1\" , \"c1ccccc1C\" , \"c1ccccc1O\" , \"c1ccccc1N\" , \"CC(=O)O\" , \"CC(=O)N\" , \"CCNC\" , \"CCN(C)C\" , \"c1ccc(cc1)C(=O)O\" , \"c1ccc(cc1)N\" ] smiles_list = [] logbb_list = [] for _ in range ( n_samples ): # Random combination of templates smiles = np . random . choice ( templates ) # Calculate simple features for synthetic logBB mol = Chem . MolFromSmiles ( smiles ) if mol : mw = Descriptors . MolWt ( mol ) logp = Descriptors . MolLogP ( mol ) tpsa = Descriptors . TPSA ( mol ) # Synthetic logBB based on known correlations logbb = 0.1 * logp - 0.01 * tpsa - 0.003 * mw + np . random . normal ( 0 , 0.3 ) logbb = np . clip ( logbb , - 3 , 2 ) smiles_list . append ( smiles ) logbb_list . append ( logbb ) df = pd . DataFrame ({ 'SMILES' : smiles_list , 'logBB' : logbb_list }) return df # ============================================================================ # 2. FEATURE EXTRACTION # ============================================================================ def compute_molecular_fingerprints ( smiles_list , radius = 2 , n_bits = 2048 ): \"\"\" Compute Morgan fingerprints for molecules \"\"\" fingerprints = [] for smiles in tqdm ( smiles_list , desc = \"Computing fingerprints\" ): mol = Chem . MolFromSmiles ( smiles ) if mol : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , radius , nBits = n_bits ) fingerprints . append ( np . array ( fp )) else : fingerprints . append ( np . zeros ( n_bits )) return np . array ( fingerprints ) def compute_molecular_descriptors ( smiles_list ): \"\"\" Compute RDKit molecular descriptors \"\"\" descriptors_list = [] descriptor_functions = [ Descriptors . MolWt , Descriptors . MolLogP , Descriptors . NumHDonors , Descriptors . NumHAcceptors , Descriptors . TPSA , Descriptors . NumRotatableBonds , Descriptors . NumAromaticRings , Descriptors . FractionCsp3 ] for smiles in tqdm ( smiles_list , desc = \"Computing descriptors\" ): mol = Chem . MolFromSmiles ( smiles ) if mol : desc = [ func ( mol ) for func in descriptor_functions ] descriptors_list . append ( desc ) else : descriptors_list . append ([ 0 ] * len ( descriptor_functions )) return np . array ( descriptors_list ) # ============================================================================ # 3. DEEP LEARNING MODEL # ============================================================================ class BBBPermeabilityModel ( nn . Module ): \"\"\" Neural network for BBB permeability prediction \"\"\" def __init__ ( self , input_dim = 2048 ): super ( BBBPermeabilityModel , self ) . __init__ () self . network = nn . Sequential ( nn . Linear ( input_dim , 512 ), nn . BatchNorm1d ( 512 ), nn . ReLU (), nn . Dropout ( 0.3 ), nn . Linear ( 512 , 256 ), nn . BatchNorm1d ( 256 ), nn . ReLU (), nn . Dropout ( 0.3 ), nn . Linear ( 256 , 128 ), nn . BatchNorm1d ( 128 ), nn . ReLU (), nn . Dropout ( 0.2 ), nn . Linear ( 128 , 1 ) ) def forward ( self , x ): return self . network ( x ) class BBBDataset ( Dataset ): \"\"\"Dataset for BBB permeability\"\"\" def __init__ ( self , features , labels ): self . features = torch . FloatTensor ( features ) self . labels = torch . FloatTensor ( labels ) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . features ) def __getitem__ ( self , idx ): return self . features [ idx ], self . labels [ idx ] # ============================================================================ # 4. TRAINING FUNCTION # ============================================================================ def train_deep_learning_model ( X_train , y_train , X_val , y_val , num_epochs = 100 , batch_size = 64 , lr = 0.001 ): \"\"\" Train deep learning model for BBB permeability \"\"\" device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) print ( f \"Using device: { device } \" ) # Create datasets and dataloaders train_dataset = BBBDataset ( X_train , y_train ) val_dataset = BBBDataset ( X_val , y_val ) train_loader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = batch_size ) # Initialize model model = BBBPermeabilityModel ( input_dim = X_train . shape [ 1 ]) . to ( device ) # Loss and optimizer criterion = nn . MSELoss () optimizer = optim . Adam ( model . parameters (), lr = lr ) scheduler = optim . lr_scheduler . ReduceLROnPlateau ( optimizer , patience = 10 , factor = 0.5 ) # Training history history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : [], 'val_mae' : [], 'val_r2' : [] } best_val_loss = float ( 'inf' ) patience_counter = 0 patience = 20 start_time = time . time () # Training loop for epoch in range ( num_epochs ): # Training model . train () train_loss = 0 for batch_x , batch_y in train_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) loss . backward () optimizer . step () train_loss += loss . item () # Validation model . eval () val_loss = 0 all_preds = [] all_labels = [] with torch . no_grad (): for batch_x , batch_y in val_loader : batch_x , batch_y = batch_x . to ( device ), batch_y . to ( device ) predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) val_loss += loss . item () all_preds . extend ( predictions . cpu () . numpy ()) all_labels . extend ( batch_y . cpu () . numpy ()) # Calculate metrics train_loss /= len ( train_loader ) val_loss /= len ( val_loader ) all_preds = np . array ( all_preds ) . flatten () all_labels = np . array ( all_labels ) . flatten () val_rmse = np . sqrt ( mean_squared_error ( all_labels , all_preds )) val_mae = mean_absolute_error ( all_labels , all_preds ) val_r2 = r2_score ( all_labels , all_preds ) # Update history history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_rmse' ] . append ( val_rmse ) history [ 'val_mae' ] . append ( val_mae ) history [ 'val_r2' ] . append ( val_r2 ) # Learning rate scheduling scheduler . step ( val_loss ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \"Epoch { epoch + 1 } / { num_epochs } \" ) print ( f \" Train Loss: { train_loss : .4f } \" ) print ( f \" Val Loss: { val_loss : .4f } , RMSE: { val_rmse : .4f } , \" f \"MAE: { val_mae : .4f } , R\u00b2: { val_r2 : .4f } \" ) # Early stopping if val_loss < best_val_loss : best_val_loss = val_loss patience_counter = 0 torch . save ( model . state_dict (), 'best_bbb_model.pth' ) else : patience_counter += 1 if patience_counter >= patience : print ( f \" \\n Early stopping at epoch { epoch + 1 } \" ) break training_time = time . time () - start_time # Load best model model . load_state_dict ( torch . load ( 'best_bbb_model.pth' )) return model , history , training_time # ============================================================================ # 5. RANDOM FOREST BASELINE # ============================================================================ def train_random_forest_model ( X_train , y_train , X_val , y_val ): \"\"\" Train Random Forest baseline model \"\"\" start_time = time . time () # Train Random Forest rf_model = RandomForestRegressor ( n_estimators = 500 , max_depth = 20 , min_samples_split = 5 , min_samples_leaf = 2 , random_state = 42 , n_jobs =- 1 ) rf_model . fit ( X_train , y_train ) # Predictions train_pred = rf_model . predict ( X_train ) val_pred = rf_model . predict ( X_val ) # Metrics train_rmse = np . sqrt ( mean_squared_error ( y_train , train_pred )) val_rmse = np . sqrt ( mean_squared_error ( y_val , val_pred )) val_mae = mean_absolute_error ( y_val , val_pred ) val_r2 = r2_score ( y_val , val_pred ) training_time = time . time () - start_time print ( \" \\n Random Forest Results:\" ) print ( f \" Train RMSE: { train_rmse : .4f } \" ) print ( f \" Val RMSE: { val_rmse : .4f } \" ) print ( f \" Val MAE: { val_mae : .4f } \" ) print ( f \" Val R\u00b2: { val_r2 : .4f } \" ) print ( f \" Training time: { training_time : .2f } s\" ) return rf_model , { 'train_rmse' : train_rmse , 'val_rmse' : val_rmse , 'val_mae' : val_mae , 'val_r2' : val_r2 , 'time' : training_time } # ============================================================================ # 6. EVALUATION AND VISUALIZATION # ============================================================================ def evaluate_model ( model , X_test , y_test , model_type = 'dl' ): \"\"\" Comprehensive model evaluation \"\"\" if model_type == 'dl' : device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) model . eval () test_dataset = BBBDataset ( X_test , y_test ) test_loader = DataLoader ( test_dataset , batch_size = 64 ) predictions = [] with torch . no_grad (): for batch_x , _ in test_loader : batch_x = batch_x . to ( device ) pred = model ( batch_x ) predictions . extend ( pred . cpu () . numpy ()) predictions = np . array ( predictions ) . flatten () else : predictions = model . predict ( X_test ) # Calculate metrics rmse = np . sqrt ( mean_squared_error ( y_test , predictions )) mae = mean_absolute_error ( y_test , predictions ) r2 = r2_score ( y_test , predictions ) print ( f \" \\n Test Set Evaluation:\" ) print ( f \" RMSE: { rmse : .4f } \" ) print ( f \" MAE: { mae : .4f } \" ) print ( f \" R\u00b2: { r2 : .4f } \" ) return predictions , { 'rmse' : rmse , 'mae' : mae , 'r2' : r2 } def visualize_results ( y_true , y_pred_dl , y_pred_rf , history ): \"\"\" Create comprehensive visualization of results \"\"\" fig , axes = plt . subplots ( 2 , 3 , figsize = ( 18 , 12 )) # 1. Training history axes [ 0 , 0 ] . plot ( history [ 'train_loss' ], label = 'Train Loss' ) axes [ 0 , 0 ] . plot ( history [ 'val_loss' ], label = 'Val Loss' ) axes [ 0 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 0 ] . set_ylabel ( 'Loss' ) axes [ 0 , 0 ] . set_title ( 'Training History' ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( True ) # 2. Metrics evolution axes [ 0 , 1 ] . plot ( history [ 'val_rmse' ], label = 'RMSE' , color = 'red' ) axes [ 0 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 1 ] . set_ylabel ( 'RMSE' ) axes [ 0 , 1 ] . set_title ( 'Validation RMSE' ) axes [ 0 , 1 ] . grid ( True ) ax2 = axes [ 0 , 1 ] . twinx () ax2 . plot ( history [ 'val_r2' ], label = 'R\u00b2' , color = 'blue' ) ax2 . set_ylabel ( 'R\u00b2' ) # 3. R\u00b2 evolution axes [ 0 , 2 ] . plot ( history [ 'val_r2' ], color = 'green' ) axes [ 0 , 2 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 2 ] . set_ylabel ( 'R\u00b2 Score' ) axes [ 0 , 2 ] . set_title ( 'Validation R\u00b2' ) axes [ 0 , 2 ] . grid ( True ) # 4. DL predictions scatter plot axes [ 1 , 0 ] . scatter ( y_true , y_pred_dl , alpha = 0.5 ) axes [ 1 , 0 ] . plot ([ y_true . min (), y_true . max ()], [ y_true . min (), y_true . max ()], 'r--' , lw = 2 ) axes [ 1 , 0 ] . set_xlabel ( 'True logBB' ) axes [ 1 , 0 ] . set_ylabel ( 'Predicted logBB' ) axes [ 1 , 0 ] . set_title ( 'Deep Learning Predictions' ) axes [ 1 , 0 ] . grid ( True ) # 5. RF predictions scatter plot axes [ 1 , 1 ] . scatter ( y_true , y_pred_rf , alpha = 0.5 , color = 'orange' ) axes [ 1 , 1 ] . plot ([ y_true . min (), y_true . max ()], [ y_true . min (), y_true . max ()], 'r--' , lw = 2 ) axes [ 1 , 1 ] . set_xlabel ( 'True logBB' ) axes [ 1 , 1 ] . set_ylabel ( 'Predicted logBB' ) axes [ 1 , 1 ] . set_title ( 'Random Forest Predictions' ) axes [ 1 , 1 ] . grid ( True ) # 6. Residuals comparison residuals_dl = y_true - y_pred_dl residuals_rf = y_true - y_pred_rf axes [ 1 , 2 ] . hist ( residuals_dl , bins = 30 , alpha = 0.5 , label = 'Deep Learning' ) axes [ 1 , 2 ] . hist ( residuals_rf , bins = 30 , alpha = 0.5 , label = 'Random Forest' ) axes [ 1 , 2 ] . set_xlabel ( 'Residuals' ) axes [ 1 , 2 ] . set_ylabel ( 'Frequency' ) axes [ 1 , 2 ] . set_title ( 'Residuals Distribution' ) axes [ 1 , 2 ] . legend () axes [ 1 , 2 ] . grid ( True ) plt . tight_layout () plt . savefig ( 'bbb_prediction_results.png' , dpi = 150 , bbox_inches = 'tight' ) plt . show () # ============================================================================ # 7. MAIN EXECUTION # ============================================================================ def main (): \"\"\" Main execution function \"\"\" print ( \"=\" * 70 ) print ( \"BBB PERMEABILITY PREDICTION - COMPLETE PIPELINE\" ) print ( \"=\" * 70 ) # 1. Load data print ( \" \\n 1. Loading data...\" ) df = load_bbb_data () # 2. Split data print ( \" \\n 2. Splitting data...\" ) train_df , temp_df = train_test_split ( df , test_size = 0.3 , random_state = 42 ) val_df , test_df = train_test_split ( temp_df , test_size = 0.5 , random_state = 42 ) print ( f \" Train: { len ( train_df ) } molecules\" ) print ( f \" Val: { len ( val_df ) } molecules\" ) print ( f \" Test: { len ( test_df ) } molecules\" ) # 3. Extract features print ( \" \\n 3. Extracting features...\" ) X_train = compute_molecular_fingerprints ( train_df [ 'SMILES' ] . values ) X_val = compute_molecular_fingerprints ( val_df [ 'SMILES' ] . values ) X_test = compute_molecular_fingerprints ( test_df [ 'SMILES' ] . values ) y_train = train_df [ 'logBB' ] . values y_val = val_df [ 'logBB' ] . values y_test = test_df [ 'logBB' ] . values # 4. Train Deep Learning model print ( \" \\n 4. Training Deep Learning model...\" ) dl_model , history , dl_time = train_deep_learning_model ( X_train , y_train , X_val , y_val , num_epochs = 100 , batch_size = 64 , lr = 0.001 ) print ( f \" Training time: { dl_time : .2f } s\" ) # 5. Train Random Forest baseline print ( \" \\n 5. Training Random Forest baseline...\" ) rf_model , rf_results = train_random_forest_model ( X_train , y_train , X_val , y_val ) # 6. Evaluate on test set print ( \" \\n 6. Evaluating models on test set...\" ) print ( \" \\n Deep Learning Model:\" ) dl_predictions , dl_metrics = evaluate_model ( dl_model , X_test , y_test , 'dl' ) print ( \" \\n Random Forest Model:\" ) rf_predictions , rf_metrics = evaluate_model ( rf_model , X_test , y_test , 'rf' ) # 7. Compare results print ( \" \\n \" + \"=\" * 70 ) print ( \"FINAL COMPARISON\" ) print ( \"=\" * 70 ) print ( f \" { 'Metric' : <15 } { 'Deep Learning' : <20 } { 'Random Forest' : <20 } { 'Improvement' } \" ) print ( \"-\" * 70 ) print ( f \" { 'RMSE' : <15 } { dl_metrics [ 'rmse' ] : <20.4f } { rf_metrics [ 'rmse' ] : <20.4f } \" f \" { ( rf_metrics [ 'rmse' ] - dl_metrics [ 'rmse' ]) / rf_metrics [ 'rmse' ] * 100 : >6.1f } %\" ) print ( f \" { 'MAE' : <15 } { dl_metrics [ 'mae' ] : <20.4f } { rf_metrics [ 'mae' ] : <20.4f } \" f \" { ( rf_metrics [ 'mae' ] - dl_metrics [ 'mae' ]) / rf_metrics [ 'mae' ] * 100 : >6.1f } %\" ) print ( f \" { 'R\u00b2' : <15 } { dl_metrics [ 'r2' ] : <20.4f } { rf_metrics [ 'r2' ] : <20.4f } \" f \" { ( dl_metrics [ 'r2' ] - rf_metrics [ 'r2' ]) / rf_metrics [ 'r2' ] * 100 : >6.1f } %\" ) print ( f \" { 'Training Time' : <15 } { dl_time : <20.1f } s { rf_results [ 'time' ] : <20.1f } s\" ) # 8. Visualize results print ( \" \\n 8. Generating visualizations...\" ) visualize_results ( y_test , dl_predictions , rf_predictions , history ) print ( \" \\n \" + \"=\" * 70 ) print ( \"PIPELINE COMPLETE!\" ) print ( \"=\" * 70 ) return { 'dl_model' : dl_model , 'rf_model' : rf_model , 'dl_metrics' : dl_metrics , 'rf_metrics' : rf_metrics , 'history' : history } # Run the pipeline if __name__ == \"__main__\" : results = main ()","title":"7.2 Full Pipeline"},{"location":"day2/#73__expected__output","text":"====================================================================== BBB PERMEABILITY PREDICTION - COMPLETE PIPELINE ====================================================================== 1. Loading data... Loaded 2000 valid molecules logBB range: [-2.85, 1.92] 2. Splitting data... Train: 1400 molecules Val: 300 molecules Test: 300 molecules 3. Extracting features... Computing fingerprints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1400/1400 [00:05<00:00] Computing fingerprints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:01<00:00] Computing fingerprints: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 300/300 [00:01<00:00] 4. Training Deep Learning model... Using device: cuda Epoch 10/100 Train Loss: 0.3245 Val Loss: 0.3678, RMSE: 0.6065, MAE: 0.4532, R\u00b2: 0.7234 Epoch 20/100 Train Loss: 0.2156 Val Loss: 0.2987, RMSE: 0.5465, MAE: 0.4012, R\u00b2: 0.7789 ... (training continues) Early stopping at epoch 68 Training time: 142.35s 5. Training Random Forest baseline... Random Forest Results: Train RMSE: 0.1234 Val RMSE: 0.6234 Val MAE: 0.4689 Val R\u00b2: 0.7456 Training time: 56.78s 6. Evaluating models on test set... Deep Learning Model: Test Set Evaluation: RMSE: 0.5234 MAE: 0.3876 R\u00b2: 0.7923 Random Forest Model: Test Set Evaluation: RMSE: 0.5987 MAE: 0.4456 R\u00b2: 0.7534 ====================================================================== FINAL COMPARISON ====================================================================== Metric Deep Learning Random Forest Improvement ---------------------------------------------------------------------- RMSE 0.5234 0.5987 12.6% MAE 0.3876 0.4456 13.0% R\u00b2 0.7923 0.7534 5.2% Training Time 142.4s 56.8s 8. Generating visualizations... ====================================================================== PIPELINE COMPLETE! ======================================================================","title":"7.3 Expected Output"},{"location":"day2/#8__model__interpretation","text":"Understanding what your model has learned is crucial for trust, debugging, and scientific insight.","title":"8. Model Interpretation"},{"location":"day2/#81__gradient-based__importance","text":"Calculate feature importance by examining gradients: def compute_gradient_importance ( model , X , device = 'cuda' ): \"\"\" Compute feature importance using gradients Args: model: Trained neural network X: Input features (numpy array or tensor) Returns: importance_scores: Feature importance for each sample \"\"\" model . eval () if isinstance ( X , np . ndarray ): X = torch . FloatTensor ( X ) X = X . to ( device ) X . requires_grad = True # Forward pass output = model ( X ) # Compute gradients gradients = [] for i in range ( output . shape [ 0 ]): model . zero_grad () if X . grad is not None : X . grad . zero_ () output [ i ] . backward ( retain_graph = True ) gradients . append ( X . grad [ i ] . cpu () . detach () . numpy () . copy ()) gradients = np . array ( gradients ) # Importance = absolute gradient * input value importance = np . abs ( gradients ) * X . cpu () . detach () . numpy () return importance def visualize_feature_importance ( importance , feature_names = None , top_k = 20 ): \"\"\" Visualize feature importance \"\"\" # Average importance across samples avg_importance = np . mean ( importance , axis = 0 ) # Get top-k features top_indices = np . argsort ( avg_importance )[ - top_k :][:: - 1 ] top_importance = avg_importance [ top_indices ] if feature_names is not None : top_features = [ feature_names [ i ] for i in top_indices ] else : top_features = [ f \"Feature { i } \" for i in top_indices ] # Plot plt . figure ( figsize = ( 10 , 6 )) plt . barh ( range ( top_k ), top_importance ) plt . yticks ( range ( top_k ), top_features ) plt . xlabel ( 'Importance Score' ) plt . title ( f 'Top { top_k } Most Important Features' ) plt . tight_layout () plt . savefig ( 'feature_importance.png' , dpi = 150 ) plt . show () # Usage importance_scores = compute_gradient_importance ( model , X_test ) visualize_feature_importance ( importance_scores , top_k = 20 )","title":"8.1 Gradient-Based Importance"},{"location":"day2/#82__integrated__gradients","text":"More accurate attribution method that accounts for baseline: def integrated_gradients ( model , X , baseline = None , steps = 50 , device = 'cuda' ): \"\"\" Compute integrated gradients for feature attribution Args: model: Trained neural network X: Input features baseline: Baseline input (default: zero) steps: Number of integration steps Returns: attributions: Feature attributions \"\"\" model . eval () if isinstance ( X , np . ndarray ): X = torch . FloatTensor ( X ) X = X . to ( device ) if baseline is None : baseline = torch . zeros_like ( X ) else : baseline = torch . FloatTensor ( baseline ) . to ( device ) # Generate interpolated inputs alphas = torch . linspace ( 0 , 1 , steps ) . to ( device ) interpolated_inputs = [] for alpha in alphas : interpolated = baseline + alpha * ( X - baseline ) interpolated_inputs . append ( interpolated ) interpolated_inputs = torch . stack ( interpolated_inputs ) # Compute gradients for each interpolated input gradients = [] for interp_input in interpolated_inputs : interp_input . requires_grad = True output = model ( interp_input ) model . zero_grad () output . sum () . backward () gradients . append ( interp_input . grad . cpu () . detach ()) gradients = torch . stack ( gradients ) # Average gradients and multiply by input difference avg_gradients = torch . mean ( gradients , dim = 0 ) attributions = ( X . cpu () - baseline . cpu ()) * avg_gradients return attributions . numpy () def explain_prediction ( model , smiles , tokenizer , importance_method = 'integrated_gradients' ): \"\"\" Explain prediction for a single molecule \"\"\" # Encode SMILES mol = Chem . MolFromSmiles ( smiles ) fp = AllChem . GetMorganFingerprintAsBitVect ( mol , 2 , 2048 ) X = np . array ( fp ) . reshape ( 1 , - 1 ) # Get prediction model . eval () with torch . no_grad (): prediction = model ( torch . FloatTensor ( X )) . item () # Get importance if importance_method == 'integrated_gradients' : importance = integrated_gradients ( model , X ) else : importance = compute_gradient_importance ( model , X ) # Visualize fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 14 , 5 )) # Draw molecule img = Draw . MolToImage ( mol , size = ( 400 , 400 )) ax1 . imshow ( img ) ax1 . axis ( 'off' ) ax1 . set_title ( f 'Predicted logBB: { prediction : .2f } ' ) # Plot importance top_k = 50 top_indices = np . argsort ( np . abs ( importance [ 0 ]))[ - top_k :] ax2 . barh ( range ( top_k ), importance [ 0 , top_indices ]) ax2 . set_xlabel ( 'Attribution Score' ) ax2 . set_ylabel ( 'Fingerprint Bit' ) ax2 . set_title ( 'Feature Attributions (Integrated Gradients)' ) plt . tight_layout () plt . savefig ( f 'explanation_ { smiles [: 10 ] } .png' , dpi = 150 ) plt . show () return prediction , importance","title":"8.2 Integrated Gradients"},{"location":"day2/#83__activation__maximization","text":"Find input patterns that maximally activate specific neurons: def activation_maximization ( model , layer_name , neuron_idx , input_shape = ( 1 , 2048 ), iterations = 1000 , lr = 0.1 ): \"\"\" Find input that maximally activates a specific neuron Args: model: Neural network layer_name: Name of layer to analyze neuron_idx: Index of neuron in that layer input_shape: Shape of input iterations: Number of optimization steps lr: Learning rate Returns: optimal_input: Input that maximizes activation \"\"\" model . eval () # Initialize random input optimal_input = torch . randn ( input_shape , requires_grad = True ) optimizer = optim . Adam ([ optimal_input ], lr = lr ) # Get layer activation = {} def get_activation ( name ): def hook ( model , input , output ): activation [ name ] = output return hook # Register hook for name , module in model . named_modules (): if name == layer_name : module . register_forward_hook ( get_activation ( layer_name )) break # Optimize for i in range ( iterations ): optimizer . zero_grad () _ = model ( optimal_input ) # Loss = negative activation (we want to maximize) loss = - activation [ layer_name ][ 0 , neuron_idx ] loss . backward () optimizer . step () # Clip to valid range [0, 1] for fingerprints with torch . no_grad (): optimal_input . clamp_ ( 0 , 1 ) if ( i + 1 ) % 100 == 0 : print ( f \"Iteration { i + 1 } , Activation: { - loss . item () : .4f } \" ) return optimal_input . detach () . numpy () # Usage optimal_fp = activation_maximization ( model , layer_name = 'network.4' , # Second hidden layer neuron_idx = 42 , iterations = 1000 ) print ( f \"Optimal fingerprint pattern: { optimal_fp [ 0 ][: 20 ] } ...\" ) print ( f \"Number of active bits: { np . sum ( optimal_fp > 0.5 ) } \" ) Attention Visualization for LSTM: def visualize_attention ( model , smiles , tokenizer ): \"\"\" Visualize attention weights for LSTM model with attention \"\"\" # Encode SMILES encoded = tokenizer . encode ( smiles ) X = torch . LongTensor ([ encoded ]) # Get prediction and attention weights model . eval () with torch . no_grad (): prediction , attention_weights = model ( X ) # Plot attention attention = attention_weights [ 0 ] . squeeze () . numpy () tokens = tokenizer . tokenize ( smiles ) plt . figure ( figsize = ( 12 , 4 )) plt . bar ( range ( len ( tokens )), attention [: len ( tokens )]) plt . xticks ( range ( len ( tokens )), tokens , rotation = 45 ) plt . xlabel ( 'SMILES Token' ) plt . ylabel ( 'Attention Weight' ) plt . title ( f 'Attention Weights for: { smiles } \\n Prediction: { prediction . item () : .2f } ' ) plt . tight_layout () plt . savefig ( 'attention_visualization.png' , dpi = 150 ) plt . show ()","title":"8.3 Activation Maximization"},{"location":"day2/#9__best__practices","text":"","title":"9. Best Practices"},{"location":"day2/#91__hyperparameter__tuning__with__optuna","text":"Automated hyperparameter optimization: import optuna from optuna.visualization import plot_optimization_history , plot_param_importances def objective ( trial , X_train , y_train , X_val , y_val ): \"\"\" Objective function for Optuna hyperparameter optimization \"\"\" # Suggest hyperparameters config = { 'hidden_dim_1' : trial . suggest_int ( 'hidden_dim_1' , 256 , 1024 , step = 128 ), 'hidden_dim_2' : trial . suggest_int ( 'hidden_dim_2' , 128 , 512 , step = 64 ), 'hidden_dim_3' : trial . suggest_int ( 'hidden_dim_3' , 64 , 256 , step = 64 ), 'dropout_rate' : trial . suggest_float ( 'dropout_rate' , 0.1 , 0.5 ), 'learning_rate' : trial . suggest_loguniform ( 'learning_rate' , 1e-5 , 1e-2 ), 'batch_size' : trial . suggest_categorical ( 'batch_size' , [ 32 , 64 , 128 ]), 'weight_decay' : trial . suggest_loguniform ( 'weight_decay' , 1e-6 , 1e-3 ) } # Create model model = MolecularFNN ( input_dim = X_train . shape [ 1 ], hidden_dims = [ config [ 'hidden_dim_1' ], config [ 'hidden_dim_2' ], config [ 'hidden_dim_3' ]], output_dim = 1 , dropout_rate = config [ 'dropout_rate' ] ) # Create dataloaders train_dataset = BBBDataset ( X_train , y_train ) val_dataset = BBBDataset ( X_val , y_val ) train_loader = DataLoader ( train_dataset , batch_size = config [ 'batch_size' ], shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = config [ 'batch_size' ]) # Train optimizer = optim . Adam ( model . parameters (), lr = config [ 'learning_rate' ], weight_decay = config [ 'weight_decay' ]) criterion = nn . MSELoss () num_epochs = 50 # Reduced for faster tuning for epoch in range ( num_epochs ): # Training model . train () for batch_x , batch_y in train_loader : optimizer . zero_grad () predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) loss . backward () optimizer . step () # Validation model . eval () val_loss = 0 with torch . no_grad (): for batch_x , batch_y in val_loader : predictions = model ( batch_x ) loss = criterion ( predictions , batch_y ) val_loss += loss . item () val_loss /= len ( val_loader ) # Report intermediate value for pruning trial . report ( val_loss , epoch ) # Handle pruning if trial . should_prune (): raise optuna . TrialPruned () return val_loss # Run optimization study = optuna . create_study ( direction = 'minimize' , pruner = optuna . pruners . MedianPruner ( n_startup_trials = 5 , n_warmup_steps = 10 ) ) study . optimize ( lambda trial : objective ( trial , X_train , y_train , X_val , y_val ), n_trials = 50 , timeout = 7200 # 2 hours ) # Print results print ( \" \\n Best hyperparameters:\" ) for key , value in study . best_params . items (): print ( f \" { key } : { value } \" ) print ( f \" \\n Best validation loss: { study . best_value : .4f } \" ) # Visualize fig1 = plot_optimization_history ( study ) fig1 . write_image ( 'optimization_history.png' ) fig2 = plot_param_importances ( study ) fig2 . write_image ( 'param_importances.png' ) # Train final model with best parameters best_config = study . best_params final_model = MolecularFNN ( input_dim = X_train . shape [ 1 ], hidden_dims = [ best_config [ 'hidden_dim_1' ], best_config [ 'hidden_dim_2' ], best_config [ 'hidden_dim_3' ]], output_dim = 1 , dropout_rate = best_config [ 'dropout_rate' ] )","title":"9.1 Hyperparameter Tuning with Optuna"},{"location":"day2/#92__complete__debugging__checklist","text":"Pre-Training Checks: def pre_training_diagnostics ( model , train_loader , device = 'cpu' ): \"\"\" Run diagnostics before training \"\"\" print ( \"=\" * 70 ) print ( \"PRE-TRAINING DIAGNOSTICS\" ) print ( \"=\" * 70 ) model = model . to ( device ) # 1. Check data loading print ( \" \\n 1. Data Loading Check:\" ) try : batch_x , batch_y = next ( iter ( train_loader )) print ( f \" \u2713 Batch shapes: X= { batch_x . shape } , Y= { batch_y . shape } \" ) print ( f \" \u2713 X range: [ { batch_x . min () : .3f } , { batch_x . max () : .3f } ]\" ) print ( f \" \u2713 Y range: [ { batch_y . min () : .3f } , { batch_y . max () : .3f } ]\" ) print ( f \" \u2713 No NaN in X: { not torch . isnan ( batch_x ) . any () } \" ) print ( f \" \u2713 No NaN in Y: { not torch . isnan ( batch_y ) . any () } \" ) except Exception as e : print ( f \" \u2717 Error: { e } \" ) return False # 2. Check forward pass print ( \" \\n 2. Forward Pass Check:\" ) try : model . eval () with torch . no_grad (): batch_x = batch_x . to ( device ) output = model ( batch_x ) print ( f \" \u2713 Output shape: { output . shape } \" ) print ( f \" \u2713 Output range: [ { output . min () : .3f } , { output . max () : .3f } ]\" ) print ( f \" \u2713 No NaN in output: { not torch . isnan ( output ) . any () } \" ) except Exception as e : print ( f \" \u2717 Error: { e } \" ) return False # 3. Check backward pass print ( \" \\n 3. Backward Pass Check:\" ) try : model . train () batch_x = batch_x . to ( device ) batch_y = batch_y . to ( device ) criterion = nn . MSELoss () optimizer = optim . Adam ( model . parameters (), lr = 0.001 ) optimizer . zero_grad () output = model ( batch_x ) loss = criterion ( output , batch_y ) loss . backward () print ( f \" \u2713 Loss value: { loss . item () : .4f } \" ) print ( f \" \u2713 Loss is finite: { torch . isfinite ( loss ) } \" ) # Check gradients has_gradients = False max_grad = 0 for name , param in model . named_parameters (): if param . grad is not None : has_gradients = True max_grad = max ( max_grad , param . grad . abs () . max () . item ()) print ( f \" \u2713 Gradients computed: { has_gradients } \" ) print ( f \" \u2713 Max gradient: { max_grad : .6f } \" ) optimizer . step () print ( f \" \u2713 Optimizer step successful\" ) except Exception as e : print ( f \" \u2717 Error: { e } \" ) return False # 4. Check model parameters print ( \" \\n 4. Model Parameters Check:\" ) total_params = sum ( p . numel () for p in model . parameters ()) trainable_params = sum ( p . numel () for p in model . parameters () if p . requires_grad ) print ( f \" \u2713 Total parameters: { total_params : , } \" ) print ( f \" \u2713 Trainable parameters: { trainable_params : , } \" ) # 5. Check learning rate print ( \" \\n 5. Optimizer Check:\" ) for param_group in optimizer . param_groups : print ( f \" \u2713 Learning rate: { param_group [ 'lr' ] } \" ) print ( \" \\n \" + \"=\" * 70 ) print ( \"ALL CHECKS PASSED - READY TO TRAIN\" ) print ( \"=\" * 70 ) return True # Run diagnostics if pre_training_diagnostics ( model , train_loader ): # Start training train_model ( ... )","title":"9.2 Complete Debugging Checklist"},{"location":"day2/#93__common__issues__and__solutions","text":"Comprehensive Troubleshooting Table: Issue Symptoms Possible Causes Solutions Loss is NaN Loss becomes NaN during training - Learning rate too high - Gradient explosion - Invalid inputs - Reduce learning rate by 10x - Add gradient clipping - Check for NaN/Inf in data - Use mixed precision training Loss not decreasing Loss stays constant or increases - Learning rate too low - Wrong loss function - Data not normalized - Model too simple - Increase learning rate - Verify loss function matches task - Normalize inputs - Increase model capacity Overfitting Train loss << Val loss - Model too complex - Too little data - Insufficient regularization - Add dropout (0.3-0.5) - Add L2 regularization - Use data augmentation - Reduce model size - Early stopping Underfitting Both losses high - Model too simple - Training time insufficient - Learning rate issues - Increase model capacity - Train longer - Tune learning rate - Remove excessive regularization Slow training Training takes too long - Batch size too small - Model too large - Inefficient data loading - Increase batch size - Use GPU - Enable data loader workers - Use mixed precision Unstable training Loss oscillates wildly - Learning rate too high - Batch size too small - Poor initialization - Reduce learning rate - Increase batch size - Use learning rate scheduler - Use batch normalization Poor test performance Test worse than validation - Data leakage - Different distribution - Overfitting to val set - Check train/val/test splits - Verify data preprocessing - Use stratified splitting Gradient vanishing Gradients become very small - Too many layers - Wrong activation - Poor initialization - Use ReLU/Leaky ReLU - Reduce number of layers - Use skip connections - Use batch normalization Out of memory CUDA out of memory - Batch size too large - Model too large - Gradient accumulation needed - Reduce batch size - Use gradient checkpointing - Use mixed precision - Clear cache regularly Debugging Code: def debug_training_step ( model , batch_x , batch_y , criterion , optimizer ): \"\"\" Detailed debugging of single training step \"\"\" print ( \" \\n \" + \"=\" * 70 ) print ( \"DEBUGGING TRAINING STEP\" ) print ( \"=\" * 70 ) # 1. Input check print ( \" \\n 1. Input Check:\" ) print ( f \" X shape: { batch_x . shape } , dtype: { batch_x . dtype } \" ) print ( f \" Y shape: { batch_y . shape } , dtype: { batch_y . dtype } \" ) print ( f \" X range: [ { batch_x . min () : .4f } , { batch_x . max () : .4f } ]\" ) print ( f \" Y range: [ { batch_y . min () : .4f } , { batch_y . max () : .4f } ]\" ) print ( f \" X has NaN: { torch . isnan ( batch_x ) . any () } \" ) print ( f \" Y has NaN: { torch . isnan ( batch_y ) . any () } \" ) # 2. Forward pass print ( \" \\n 2. Forward Pass:\" ) model . train () optimizer . zero_grad () output = model ( batch_x ) print ( f \" Output shape: { output . shape } \" ) print ( f \" Output range: [ { output . min () : .4f } , { output . max () : .4f } ]\" ) print ( f \" Output has NaN: { torch . isnan ( output ) . any () } \" ) # 3. Loss computation print ( \" \\n 3. Loss Computation:\" ) loss = criterion ( output , batch_y ) print ( f \" Loss value: { loss . item () : .4f } \" ) print ( f \" Loss is finite: { torch . isfinite ( loss ) } \" ) # 4. Backward pass print ( \" \\n 4. Backward Pass:\" ) loss . backward () # Check gradients print ( \" \\n Gradient Statistics:\" ) for name , param in model . named_parameters (): if param . grad is not None : grad_min = param . grad . min () . item () grad_max = param . grad . max () . item () grad_mean = param . grad . mean () . item () grad_norm = param . grad . norm () . item () print ( f \" { name } :\" ) print ( f \" Range: [ { grad_min : .6f } , { grad_max : .6f } ]\" ) print ( f \" Mean: { grad_mean : .6f } , Norm: { grad_norm : .6f } \" ) print ( f \" Has NaN: { torch . isnan ( param . grad ) . any () } \" ) # 5. Optimizer step print ( \" \\n 5. Optimizer Step:\" ) optimizer . step () print ( \" \u2713 Step completed\" ) # 6. Parameter update check print ( \" \\n 6. Parameter Update Check:\" ) new_output = model ( batch_x ) new_loss = criterion ( new_output , batch_y ) print ( f \" New loss: { new_loss . item () : .4f } \" ) print ( f \" Loss change: { new_loss . item () - loss . item () : .6f } \" ) print ( \" \\n \" + \"=\" * 70 ) # Usage: Run on first batch to debug batch_x , batch_y = next ( iter ( train_loader )) debug_training_step ( model , batch_x , batch_y , criterion , optimizer )","title":"9.3 Common Issues and Solutions"},{"location":"day2/#10__key__takeaways","text":"Core Concepts: Deep Learning Advantages for Molecules Automatic feature learning from raw molecular representations Captures complex non-linear relationships Enables transfer learning and multi-task learning State-of-the-art performance on many molecular property prediction tasks Model Selection Guidelines Feedforward NN : Best for general-purpose molecular property prediction with fingerprints 1D CNN : Excellent for SMILES when local patterns (functional groups) matter LSTM/GRU : Best for sequence modeling and when order matters in SMILES Multi-Task : Use when predicting multiple related properties Transfer Learning : Critical when data is limited (<5K samples) Architecture Best Practices Start simple (2-3 layers) and add complexity only if needed Use ReLU activation for hidden layers Apply batch normalization and dropout for regularization Use Adam optimizer with learning rate 1e-3 as default Implement early stopping (patience=15-20) Training Strategies Always split data into train/val/test (70/15/15) Normalize inputs for faster convergence Use learning rate scheduling (ReduceLROnPlateau) Monitor multiple metrics (RMSE, MAE, R\u00b2) Save best model based on validation loss Performance Optimization Deep learning typically provides 10-20% improvement over Random Forest Transfer learning can improve performance by 15-30% with limited data Multi-task learning helps when tasks are related Ensemble methods (combining multiple models) can add another 5-10% Hyperparameter Importance Ranking Learning rate (most critical) Batch size Number of layers and neurons Dropout rate Optimizer choice (Adam usually best) Common Pitfalls to Avoid Training without validation set Not normalizing inputs Ignoring overfitting signs Using too large learning rate Not checking for data leakage Forgetting to set model.eval() during inference Model Interpretation Matters Use gradient-based methods for feature importance Integrated gradients provide better attributions Attention mechanisms add interpretability Always validate interpretations with domain knowledge Production Considerations Save model architecture and weights separately Version control for models and data Monitor model performance degradation over time Implement confidence scoring for predictions Document training procedures and hyperparameters Research Directions Graph Neural Networks for molecular graphs (Day 3) 3D conformation-based models Active learning for efficient data collection Uncertainty quantification Explainable AI for drug discovery Performance Benchmarks (BBB Permeability): Approach R\u00b2 Score RMSE Training Time Data Required Random Forest 0.75 0.60 Fast (1 min) 1K+ Feedforward NN 0.79 0.52 Medium (15 min) 1K+ 1D CNN (SMILES) 0.82 0.48 Medium (20 min) 2K+ LSTM (SMILES) 0.84 0.45 Slow (45 min) 3K+ Transfer Learning 0.87 0.41 Fast (10 min) 500+ Multi-Task 0.86 0.42 Medium (25 min) 1K+ per task","title":"10. Key Takeaways"},{"location":"day2/#11__resources","text":"","title":"11. Resources"},{"location":"day2/#111__essential__papers","text":"Deep Learning Fundamentals: 1. LeCun, Y., Bengio, Y., & Hinton, G. (2015). \u201cDeep learning.\u201d Nature , 521(7553), 436-444. 2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning . MIT Press. Molecular Deep Learning: 3. Wu, Z., et al. (2018). \u201cMoleculeNet: A benchmark for molecular machine learning.\u201d Chemical Science , 9(2), 513-530. 4. Wen, M., et al. (2020). \u201cDeep learning for molecular property prediction.\u201d arXiv preprint arXiv:2003.03167. 5. Goh, G. B., et al. (2017). \u201cDeep learning for computational chemistry.\u201d Journal of Computational Chemistry , 38(16), 1291-1307. Architecture-Specific: 6. G\u00f3mez-Bombarelli, R., et al. (2018). \u201cAutomatic chemical design using a data-driven continuous representation of molecules.\u201d ACS Central Science , 4(2), 268-276. 7. Zheng, S., et al. (2020). \u201cIdentifying structure\u2013property relationships through SMILES syntax analysis with self-attention mechanism.\u201d Journal of Chemical Information and Modeling , 59(2), 914-923. 8. Chithrananda, S., et al. (2020). \u201cChemBERTa: Large-scale self-supervised pretraining for molecular property prediction.\u201d arXiv preprint arXiv:2010.09885. Transfer Learning: 9. Hu, W., et al. (2020). \u201cStrategies for pre-training graph neural networks.\u201d ICLR 2020 . 10. Ramsundar, B., et al. (2015). \u201cMassively multitask networks for drug discovery.\u201d arXiv preprint arXiv:1502.02072. Model Interpretation: 11. Sundararajan, M., et al. (2017). \u201cAxiomatic attribution for deep networks.\u201d ICML 2017 . 12. Jim\u00e9nez-Luna, J., et al. (2020). \u201cDrug discovery with explainable artificial intelligence.\u201d Nature Machine Intelligence , 2(10), 573-584.","title":"11.1 Essential Papers"},{"location":"day2/#112__software__libraries","text":"Deep Learning Frameworks: - PyTorch : https://pytorch.org/ (Recommended for research) - TensorFlow/Keras : https://www.tensorflow.org/ - JAX : https://github.com/google/jax (For advanced users) Molecular Machine Learning: - DeepChem : https://deepchem.io/ (Comprehensive molecular ML library) - RDKit : https://www.rdkit.org/ (Cheminformatics toolkit) - Mordred : https://github.com/mordred-descriptor/mordred (Molecular descriptors) - ChemProp : https://github.com/chemprop/chemprop (Message passing neural networks) Model Interpretation: - Captum : https://captum.ai/ (PyTorch interpretability) - SHAP : https://github.com/slundberg/shap (SHapley Additive exPlanations) - LIME : https://github.com/marcotcr/lime (Local interpretable model-agnostic explanations) Hyperparameter Tuning: - Optuna : https://optuna.org/ (Automated hyperparameter optimization) - Ray Tune : https://docs.ray.io/en/latest/tune/index.html (Scalable tuning) - Weights & Biases : https://wandb.ai/ (Experiment tracking)","title":"11.2 Software Libraries"},{"location":"day2/#113__datasets","text":"Public Molecular Datasets: 1. MoleculeNet : Collection of datasets for molecular property prediction - http://moleculenet.ai/ ZINC Database : 230M purchasable compounds https://zinc.docking.org/ PubChem : 100M+ compounds with biological activities https://pubchem.ncbi.nlm.nih.gov/ ChEMBL : 2M+ bioactive molecules https://www.ebi.ac.uk/chembl/ Tox21 : Toxicity data for 12K compounds https://tripod.nih.gov/tox21/ BBBP (Blood-Brain Barrier Penetration) : 2K molecules Part of MoleculeNet ESOL (Solubility) : 1K molecules with measured solubility https://pubs.acs.org/doi/10.1021/ci034243x","title":"11.3 Datasets"},{"location":"day2/#114__online__courses__and__tutorials","text":"Deep Learning: 1. Deep Learning Specialization (Coursera - Andrew Ng) - https://www.coursera.org/specializations/deep-learning Fast.ai Practical Deep Learning https://course.fast.ai/ Molecular Machine Learning: 3. DeepChem Tutorials - https://deepchem.readthedocs.io/en/latest/get_started/tutorials.html Molecular Machine Learning (MIT) http://molecularml.github.io/ Machine Learning for Drug Discovery (Stanford) http://cs229.stanford.edu/proj2019aut/","title":"11.4 Online Courses and Tutorials"},{"location":"day2/#115__useful__blogs__and__articles","text":"Distill.pub : Interactive ML visualizations https://distill.pub/ Pat Walters\u2019 Blog : Practical cheminformatics http://practicalcheminformatics.blogspot.com/ Is Life Worth Living? : Deep learning for chemistry https://iwatobipen.wordpress.com/ DeepMind Research : Latest AI research https://deepmind.com/research","title":"11.5 Useful Blogs and Articles"},{"location":"day2/#116__community__and__forums","text":"RDKit Discussions : https://github.com/rdkit/rdkit/discussions DeepChem Gitter : https://gitter.im/deepchem/Lobby r/MachineLearning : https://www.reddit.com/r/MachineLearning/ r/cheminformatics : https://www.reddit.com/r/cheminformatics/","title":"11.6 Community and Forums"},{"location":"day2/#12__homework__assignment","text":"","title":"12. Homework Assignment"},{"location":"day2/#instructions","text":"Complete the following 10 exercises to reinforce your understanding of Day 2 concepts. Submit a Jupyter notebook with code, results, and brief explanations. Evaluation Criteria: - Code correctness and clarity (40%) - Results and analysis quality (30%) - Explanations and insights (20%) - Code documentation (10%) Submission Format: - Jupyter notebook (.ipynb) - Include all outputs and visualizations - Add markdown cells with explanations - Ensure code is reproducible","title":"Instructions"},{"location":"day2/#exercise__1__implement__a__custom__neural__network__10__points","text":"Task: Build a feedforward neural network from scratch using only NumPy (no PyTorch/TensorFlow). Requirements: - Implement forward propagation - Implement backward propagation - Train on a simple molecular dataset (e.g., solubility) - Compare performance with PyTorch implementation Deliverables: - Complete implementation with comments - Training loss plot - Comparison table","title":"Exercise 1: Implement a Custom Neural Network (10 points)"},{"location":"day2/#exercise__2__activation__function__comparison__8__points","text":"Task: Compare different activation functions on molecular property prediction. Requirements: - Test: ReLU, Leaky ReLU, ELU, SELU, Tanh - Use same architecture (3 hidden layers) - Plot training curves for each - Analyze convergence speed and final performance Deliverables: - Training curves for all activations - Performance comparison table - Analysis of results (2-3 paragraphs)","title":"Exercise 2: Activation Function Comparison (8 points)"},{"location":"day2/#exercise__3__implement__multi-task__learning__12__points","text":"Task: Build a multi-task model to predict multiple molecular properties. Requirements: - Predict at least 3 properties (e.g., solubility, LogP, TPSA) - Implement task weighting (try 3 different strategies) - Compare with single-task models - Visualize shared representations (t-SNE) Deliverables: - Multi-task model implementation - Performance comparison - t-SNE visualization - Analysis of benefits","title":"Exercise 3: Implement Multi-Task Learning (12 points)"},{"location":"day2/#exercise__4__smiles__cnn__implementation__10__points","text":"Task: Implement and optimize a 1D CNN for SMILES. Requirements: - Test different filter sizes (3, 5, 7, 9) - Test different numbers of filters (32, 64, 128) - Implement data augmentation (SMILES enumeration) - Compare with fingerprint-based model Deliverables: - CNN implementation - Hyperparameter search results - Performance comparison - Best model configuration","title":"Exercise 4: SMILES CNN Implementation (10 points)"},{"location":"day2/#exercise__5__lstm__vs__gru__comparison__10__points","text":"Task: Compare LSTM and GRU architectures for SMILES processing. Requirements: - Implement both LSTM and GRU - Add attention mechanism to both - Compare training time, memory usage, performance - Test on sequences of different lengths Deliverables: - Both implementations - Performance metrics table - Training time comparison - Recommendations","title":"Exercise 5: LSTM vs GRU Comparison (10 points)"},{"location":"day2/#exercise__6__transfer__learning__experiment__12__points","text":"Task: Implement transfer learning for a low-data regime task. Requirements: - Pre-train on large dataset (e.g., solubility, 10K molecules) - Fine-tune on small dataset (BBB permeability, 500 molecules) - Try different freezing strategies - Compare with training from scratch Deliverables: - Pre-training code - Fine-tuning code - Learning curves comparison - Analysis of data efficiency","title":"Exercise 6: Transfer Learning Experiment (12 points)"},{"location":"day2/#exercise__7__model__interpretation__10__points","text":"Task: Implement and compare interpretation methods. Requirements: - Implement gradient-based importance - Implement integrated gradients - Apply to 10 test molecules - Visualize important features - Validate interpretations Deliverables: - Implementation of both methods - Visualizations for 5 molecules - Comparison of methods - Validation with domain knowledge","title":"Exercise 7: Model Interpretation (10 points)"},{"location":"day2/#exercise__8__hyperparameter__tuning__with__optuna__10__points","text":"Task: Use Optuna to find optimal hyperparameters. Requirements: - Define search space for 6+ hyperparameters - Run at least 50 trials - Visualize optimization history - Analyze parameter importance - Train final model with best parameters Deliverables: - Optuna code - Optimization visualizations - Best hyperparameters - Final model performance","title":"Exercise 8: Hyperparameter Tuning with Optuna (10 points)"},{"location":"day2/#exercise__9__complete__pipeline__development__15__points","text":"Task: Build a complete end-to-end pipeline for a novel dataset. Requirements: - Choose a dataset from MoleculeNet (not used in class) - Data preprocessing and splitting - Feature engineering - Model selection (try 3+ architectures) - Hyperparameter tuning - Final evaluation and error analysis Deliverables: - Complete pipeline code - Detailed README - Results report (1-2 pages) - Error analysis","title":"Exercise 9: Complete Pipeline Development (15 points)"},{"location":"day2/#exercise__10__research__paper__implementation__13__points","text":"Task: Implement a model from a recent research paper. Suggested Papers: 1. \u201cMolecular graph convolutions: moving beyond fingerprints\u201d (Duvenaud et al., 2015) 2. \u201cAnalyzing learned molecular representations for property prediction\u201d (Yang et al., 2019) 3. \u201cSelf-Attention-Based Molecular Representation\u201d (Zheng et al., 2019) Requirements: - Implement core architecture from paper - Reproduce key results (within 5% of reported performance) - Apply to new dataset - Write implementation notes Deliverables: - Implementation code - Results comparison with paper - Application to new dataset - Implementation notes (1 page)","title":"Exercise 10: Research Paper Implementation (13 points)"},{"location":"day2/#bonus__exercise__ensemble__methods__5__points","text":"Task: Implement ensemble methods to improve predictions. Requirements: - Create ensemble of 5+ models (different architectures) - Try different ensemble strategies (averaging, stacking, voting) - Compare with individual models - Analyze diversity of predictions Deliverables: - Ensemble implementation - Performance comparison - Diversity analysis","title":"Bonus Exercise: Ensemble Methods (+5 points)"},{"location":"day2/#submission__guidelines","text":"File Structure: homework_day2/ \u251c\u2500\u2500 README.md \u251c\u2500\u2500 exercise_1_custom_nn.ipynb \u251c\u2500\u2500 exercise_2_activation_comparison.ipynb \u251c\u2500\u2500 exercise_3_multitask.ipynb \u251c\u2500\u2500 exercise_4_smiles_cnn.ipynb \u251c\u2500\u2500 exercise_5_lstm_gru.ipynb \u251c\u2500\u2500 exercise_6_transfer_learning.ipynb \u251c\u2500\u2500 exercise_7_interpretation.ipynb \u251c\u2500\u2500 exercise_8_optuna.ipynb \u251c\u2500\u2500 exercise_9_complete_pipeline.ipynb \u251c\u2500\u2500 exercise_10_paper_implementation.ipynb \u251c\u2500\u2500 bonus_ensemble.ipynb (optional) \u251c\u2500\u2500 data/ (if applicable) \u2514\u2500\u2500 models/ (saved models) README.md should include: - Student name and ID - Brief description of each exercise - Key findings and insights - Challenges encountered - Total time spent Deadline: 7 days from course date Grading: Total 100 points (+ 5 bonus)","title":"Submission Guidelines"},{"location":"day2/#13__appendix","text":"","title":"13. Appendix"},{"location":"day2/#appendix__a__complete__feedforward__nn__template","text":"\"\"\" Complete Feedforward Neural Network Template For Molecular Property Prediction This template provides a production-ready implementation with all best practices included. \"\"\" import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset , DataLoader import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error , r2_score , mean_absolute_error from rdkit import Chem from rdkit.Chem import AllChem import pandas as pd from tqdm import tqdm import matplotlib.pyplot as plt import json import os from datetime import datetime # ============================================================================ # 1. CONFIGURATION # ============================================================================ class Config : \"\"\"Configuration for model training\"\"\" # Data data_path = 'molecular_data.csv' smiles_column = 'SMILES' target_column = 'property' # Features fingerprint_radius = 2 fingerprint_bits = 2048 # Model input_dim = 2048 hidden_dims = [ 512 , 256 , 128 ] output_dim = 1 dropout_rate = 0.3 # Training batch_size = 64 learning_rate = 0.001 weight_decay = 1e-5 num_epochs = 100 patience = 20 # Device device = 'cuda' if torch . cuda . is_available () else 'cpu' # Paths model_dir = 'models' results_dir = 'results' # Random seed random_seed = 42 # ============================================================================ # 2. DATA HANDLING # ============================================================================ class MolecularDataset ( Dataset ): \"\"\"Dataset for molecular property prediction\"\"\" def __init__ ( self , smiles_list , targets , config , scaler = None ): self . config = config self . fingerprints = [] self . targets = [] # Generate fingerprints for smiles , target in tqdm ( zip ( smiles_list , targets ), desc = \"Processing molecules\" ): mol = Chem . MolFromSmiles ( smiles ) if mol is not None : fp = AllChem . GetMorganFingerprintAsBitVect ( mol , config . fingerprint_radius , nBits = config . fingerprint_bits ) self . fingerprints . append ( np . array ( fp )) self . targets . append ( target ) # Convert to tensors self . fingerprints = np . array ( self . fingerprints ) # Scale features if scaler is None : self . scaler = StandardScaler () self . fingerprints = self . scaler . fit_transform ( self . fingerprints ) else : self . scaler = scaler self . fingerprints = self . scaler . transform ( self . fingerprints ) self . fingerprints = torch . FloatTensor ( self . fingerprints ) self . targets = torch . FloatTensor ( self . targets ) . reshape ( - 1 , 1 ) def __len__ ( self ): return len ( self . fingerprints ) def __getitem__ ( self , idx ): return self . fingerprints [ idx ], self . targets [ idx ] # ============================================================================ # 3. MODEL DEFINITION # ============================================================================ class MolecularNN ( nn . Module ): \"\"\"Feedforward neural network for molecular property prediction\"\"\" def __init__ ( self , config ): super ( MolecularNN , self ) . __init__ () layers = [] prev_dim = config . input_dim # Hidden layers for hidden_dim in config . hidden_dims : layers . extend ([ nn . Linear ( prev_dim , hidden_dim ), nn . BatchNorm1d ( hidden_dim ), nn . ReLU (), nn . Dropout ( config . dropout_rate ) ]) prev_dim = hidden_dim # Output layer layers . append ( nn . Linear ( prev_dim , config . output_dim )) self . network = nn . Sequential ( * layers ) # Initialize weights self . _initialize_weights () def _initialize_weights ( self ): \"\"\"He initialization for ReLU networks\"\"\" for m in self . modules (): if isinstance ( m , nn . Linear ): nn . init . kaiming_normal_ ( m . weight , mode = 'fan_in' , nonlinearity = 'relu' ) if m . bias is not None : nn . init . constant_ ( m . bias , 0 ) def forward ( self , x ): return self . network ( x ) # ============================================================================ # 4. TRAINING # ============================================================================ class Trainer : \"\"\"Training manager\"\"\" def __init__ ( self , model , config ): self . model = model self . config = config self . device = torch . device ( config . device ) self . model . to ( self . device ) # Optimizer and criterion self . criterion = nn . MSELoss () self . optimizer = optim . Adam ( model . parameters (), lr = config . learning_rate , weight_decay = config . weight_decay ) self . scheduler = optim . lr_scheduler . ReduceLROnPlateau ( self . optimizer , mode = 'min' , factor = 0.5 , patience = 10 , verbose = True ) # History self . history = { 'train_loss' : [], 'val_loss' : [], 'val_rmse' : [], 'val_mae' : [], 'val_r2' : [] } # Best model tracking self . best_val_loss = float ( 'inf' ) self . best_model_state = None self . patience_counter = 0 def train_epoch ( self , train_loader ): \"\"\"Train for one epoch\"\"\" self . model . train () total_loss = 0 for batch_x , batch_y in train_loader : batch_x = batch_x . to ( self . device ) batch_y = batch_y . to ( self . device ) self . optimizer . zero_grad () predictions = self . model ( batch_x ) loss = self . criterion ( predictions , batch_y ) loss . backward () # Gradient clipping torch . nn . utils . clip_grad_norm_ ( self . model . parameters (), max_norm = 1.0 ) self . optimizer . step () total_loss += loss . item () return total_loss / len ( train_loader ) def validate ( self , val_loader ): \"\"\"Validate the model\"\"\" self . model . eval () total_loss = 0 all_predictions = [] all_targets = [] with torch . no_grad (): for batch_x , batch_y in val_loader : batch_x = batch_x . to ( self . device ) batch_y = batch_y . to ( self . device ) predictions = self . model ( batch_x ) loss = self . criterion ( predictions , batch_y ) total_loss += loss . item () all_predictions . extend ( predictions . cpu () . numpy ()) all_targets . extend ( batch_y . cpu () . numpy ()) # Calculate metrics avg_loss = total_loss / len ( val_loader ) all_predictions = np . array ( all_predictions ) . flatten () all_targets = np . array ( all_targets ) . flatten () rmse = np . sqrt ( mean_squared_error ( all_targets , all_predictions )) mae = mean_absolute_error ( all_targets , all_predictions ) r2 = r2_score ( all_targets , all_predictions ) return avg_loss , rmse , mae , r2 def train ( self , train_loader , val_loader ): \"\"\"Complete training loop\"\"\" print ( f \"Training on { self . device } \" ) print ( f \"Model has { sum ( p . numel () for p in self . model . parameters ()) : , } parameters\" ) for epoch in range ( self . config . num_epochs ): # Train train_loss = self . train_epoch ( train_loader ) # Validate val_loss , val_rmse , val_mae , val_r2 = self . validate ( val_loader ) # Update history self . history [ 'train_loss' ] . append ( train_loss ) self . history [ 'val_loss' ] . append ( val_loss ) self . history [ 'val_rmse' ] . append ( val_rmse ) self . history [ 'val_mae' ] . append ( val_mae ) self . history [ 'val_r2' ] . append ( val_r2 ) # Learning rate scheduling self . scheduler . step ( val_loss ) # Print progress if ( epoch + 1 ) % 10 == 0 : print ( f \" \\n Epoch { epoch + 1 } / { self . config . num_epochs } \" ) print ( f \" Train Loss: { train_loss : .4f } \" ) print ( f \" Val Loss: { val_loss : .4f } , RMSE: { val_rmse : .4f } , \" f \"MAE: { val_mae : .4f } , R\u00b2: { val_r2 : .4f } \" ) # Early stopping if val_loss < self . best_val_loss : self . best_val_loss = val_loss self . best_model_state = self . model . state_dict () . copy () self . patience_counter = 0 else : self . patience_counter += 1 if self . patience_counter >= self . config . patience : print ( f \" \\n Early stopping at epoch { epoch + 1 } \" ) break # Load best model self . model . load_state_dict ( self . best_model_state ) print ( f \" \\n Training complete! Best validation loss: { self . best_val_loss : .4f } \" ) def save_model ( self , filepath ): \"\"\"Save model and training info\"\"\" torch . save ({ 'model_state_dict' : self . model . state_dict (), 'optimizer_state_dict' : self . optimizer . state_dict (), 'config' : vars ( self . config ), 'history' : self . history , 'best_val_loss' : self . best_val_loss }, filepath ) print ( f \"Model saved to { filepath } \" ) def load_model ( self , filepath ): \"\"\"Load saved model\"\"\" checkpoint = torch . load ( filepath , map_location = self . device ) self . model . load_state_dict ( checkpoint [ 'model_state_dict' ]) self . optimizer . load_state_dict ( checkpoint [ 'optimizer_state_dict' ]) self . history = checkpoint [ 'history' ] print ( f \"Model loaded from { filepath } \" ) # ============================================================================ # 5. EVALUATION # ============================================================================ def evaluate_model ( model , test_loader , device ): \"\"\"Evaluate model on test set\"\"\" model . eval () all_predictions = [] all_targets = [] with torch . no_grad (): for batch_x , batch_y in test_loader : batch_x = batch_x . to ( device ) predictions = model ( batch_x ) all_predictions . extend ( predictions . cpu () . numpy ()) all_targets . extend ( batch_y . numpy ()) all_predictions = np . array ( all_predictions ) . flatten () all_targets = np . array ( all_targets ) . flatten () # Calculate metrics rmse = np . sqrt ( mean_squared_error ( all_targets , all_predictions )) mae = mean_absolute_error ( all_targets , all_predictions ) r2 = r2_score ( all_targets , all_predictions ) print ( \" \\n Test Set Evaluation:\" ) print ( f \" RMSE: { rmse : .4f } \" ) print ( f \" MAE: { mae : .4f } \" ) print ( f \" R\u00b2: { r2 : .4f } \" ) return all_predictions , { 'rmse' : rmse , 'mae' : mae , 'r2' : r2 } def plot_results ( history , predictions , targets , save_path = 'results.png' ): \"\"\"Plot training history and predictions\"\"\" fig , axes = plt . subplots ( 2 , 2 , figsize = ( 12 , 10 )) # Training history axes [ 0 , 0 ] . plot ( history [ 'train_loss' ], label = 'Train Loss' ) axes [ 0 , 0 ] . plot ( history [ 'val_loss' ], label = 'Val Loss' ) axes [ 0 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 0 ] . set_ylabel ( 'Loss' ) axes [ 0 , 0 ] . set_title ( 'Training History' ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( True ) # Metrics axes [ 0 , 1 ] . plot ( history [ 'val_rmse' ], label = 'RMSE' ) axes [ 0 , 1 ] . plot ( history [ 'val_mae' ], label = 'MAE' ) axes [ 0 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 1 ] . set_ylabel ( 'Error' ) axes [ 0 , 1 ] . set_title ( 'Validation Errors' ) axes [ 0 , 1 ] . legend () axes [ 0 , 1 ] . grid ( True ) # Predictions axes [ 1 , 0 ] . scatter ( targets , predictions , alpha = 0.5 ) axes [ 1 , 0 ] . plot ([ targets . min (), targets . max ()], [ targets . min (), targets . max ()], 'r--' , lw = 2 ) axes [ 1 , 0 ] . set_xlabel ( 'True Values' ) axes [ 1 , 0 ] . set_ylabel ( 'Predictions' ) axes [ 1 , 0 ] . set_title ( 'Predictions vs True Values' ) axes [ 1 , 0 ] . grid ( True ) # Residuals residuals = targets - predictions axes [ 1 , 1 ] . hist ( residuals , bins = 30 ) axes [ 1 , 1 ] . set_xlabel ( 'Residuals' ) axes [ 1 , 1 ] . set_ylabel ( 'Frequency' ) axes [ 1 , 1 ] . set_title ( 'Residual Distribution' ) axes [ 1 , 1 ] . grid ( True ) plt . tight_layout () plt . savefig ( save_path , dpi = 150 , bbox_inches = 'tight' ) print ( f \"Results saved to { save_path } \" ) # ============================================================================ # 6. MAIN EXECUTION # ============================================================================ def main (): \"\"\"Main execution function\"\"\" # Set random seeds np . random . seed ( Config . random_seed ) torch . manual_seed ( Config . random_seed ) # Create directories os . makedirs ( Config . model_dir , exist_ok = True ) os . makedirs ( Config . results_dir , exist_ok = True ) # Load data print ( \"Loading data...\" ) df = pd . read_csv ( Config . data_path ) smiles = df [ Config . smiles_column ] . values targets = df [ Config . target_column ] . values # Split data print ( \"Splitting data...\" ) smiles_train , smiles_temp , y_train , y_temp = train_test_split ( smiles , targets , test_size = 0.3 , random_state = Config . random_seed ) smiles_val , smiles_test , y_val , y_test = train_test_split ( smiles_temp , y_temp , test_size = 0.5 , random_state = Config . random_seed ) print ( f \"Train: { len ( smiles_train ) } , Val: { len ( smiles_val ) } , Test: { len ( smiles_test ) } \" ) # Create datasets print ( \"Creating datasets...\" ) train_dataset = MolecularDataset ( smiles_train , y_train , Config ) val_dataset = MolecularDataset ( smiles_val , y_val , Config , scaler = train_dataset . scaler ) test_dataset = MolecularDataset ( smiles_test , y_test , Config , scaler = train_dataset . scaler ) # Create data loaders train_loader = DataLoader ( train_dataset , batch_size = Config . batch_size , shuffle = True ) val_loader = DataLoader ( val_dataset , batch_size = Config . batch_size ) test_loader = DataLoader ( test_dataset , batch_size = Config . batch_size ) # Create model print ( \"Creating model...\" ) model = MolecularNN ( Config ) # Create trainer trainer = Trainer ( model , Config ) # Train print ( \" \\n Starting training...\" ) trainer . train ( train_loader , val_loader ) # Save model timestamp = datetime . now () . strftime ( \"%Y%m %d _%H%M%S\" ) model_path = os . path . join ( Config . model_dir , f 'model_ { timestamp } .pth' ) trainer . save_model ( model_path ) # Evaluate on test set print ( \" \\n Evaluating on test set...\" ) predictions , metrics = evaluate_model ( model , test_loader , trainer . device ) # Plot results results_path = os . path . join ( Config . results_dir , f 'results_ { timestamp } .png' ) plot_results ( trainer . history , predictions , y_test , results_path ) # Save metrics metrics_path = os . path . join ( Config . results_dir , f 'metrics_ { timestamp } .json' ) with open ( metrics_path , 'w' ) as f : json . dump ( metrics , f , indent = 2 ) print ( \" \\n Pipeline complete!\" ) if __name__ == \"__main__\" : main ()","title":"Appendix A: Complete Feedforward NN Template"},{"location":"day2/#appendix__b__smiles__processing__utilities","text":"\"\"\" SMILES Processing Utilities Complete toolkit for SMILES handling, tokenization, and augmentation \"\"\" import numpy as np from rdkit import Chem from rdkit.Chem import AllChem import re # ============================================================================ # SMILES TOKENIZATION # ============================================================================ class SMILESTokenizer : \"\"\"Advanced SMILES tokenizer with comprehensive vocabulary\"\"\" def __init__ ( self ): # Define token patterns (order matters!) self . token_patterns = [ r 'Br' , # Bromine (must come before 'r') r 'Cl' , # Chlorine (must come before 'l') r '@@' , # Chirality r '@' , # Chirality r '\\[([^\\]]+)\\]' , # Bracketed atoms r '[A-Z][a-z]?' , # Elements r '[#%\\(\\)\\+\\-0-9= \\\\ /]' , # Other tokens ] self . regex = re . compile ( '|' . join ( self . token_patterns )) # Special tokens self . pad_token = '<PAD>' self . unk_token = '<UNK>' self . start_token = '<START>' self . end_token = '<END>' self . mask_token = '<MASK>' # Build vocabulary from common SMILES self . _build_vocabulary () def _build_vocabulary ( self ): \"\"\"Build comprehensive vocabulary\"\"\" # Common SMILES tokens common_tokens = [ # Elements 'C' , 'N' , 'O' , 'S' , 'P' , 'F' , 'Cl' , 'Br' , 'I' , 'B' , 'Si' , 'Se' , 'As' , # Aromatic 'c' , 'n' , 'o' , 's' , 'p' , # Bonds '-' , '=' , '#' , '/' , ' \\\\ ' , # Branches '(' , ')' , # Rings '1' , '2' , '3' , '4' , '5' , '6' , '7' , '8' , '9' , '%10' , '%11' , # Chirality '@' , '@@' , # Brackets '[' , ']' , # Charges '+' , '-' , '++' , '--' , # Hydrogens 'H' , ] # Special tokens special_tokens = [ self . pad_token , self . unk_token , self . start_token , self . end_token , self . mask_token ] # Combined vocabulary self . vocab = special_tokens + common_tokens self . token_to_idx = { token : idx for idx , token in enumerate ( self . vocab )} self . idx_to_token = { idx : token for token , idx in self . token_to_idx . items ()} def tokenize ( self , smiles ): \"\"\"Tokenize SMILES string\"\"\" tokens = self . regex . findall ( smiles ) return tokens def encode ( self , smiles , max_length = None , add_special_tokens = True ): \"\"\"Convert SMILES to token indices\"\"\" tokens = self . tokenize ( smiles ) if add_special_tokens : tokens = [ self . start_token ] + tokens + [ self . end_token ] # Convert to indices indices = [ self . token_to_idx . get ( token , self . token_to_idx [ self . unk_token ]) for token in tokens ] # Pad or truncate if max_length is not None : if len ( indices ) < max_length : indices += [ self . token_to_idx [ self . pad_token ]] * ( max_length - len ( indices )) else : indices = indices [: max_length ] return indices def decode ( self , indices ): \"\"\"Convert token indices back to SMILES\"\"\" tokens = [ self . idx_to_token . get ( idx , self . unk_token ) for idx in indices ] # Remove special tokens and padding tokens = [ t for t in tokens if t not in [ self . pad_token , self . start_token , self . end_token ]] return '' . join ( tokens ) def batch_encode ( self , smiles_list , max_length = None , add_special_tokens = True ): \"\"\"Encode batch of SMILES\"\"\" return [ self . encode ( s , max_length , add_special_tokens ) for s in smiles_list ] # ============================================================================ # SMILES VALIDATION AND CANONICALIZATION # ============================================================================ def validate_smiles ( smiles ): \"\"\"Check if SMILES is valid\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) return mol is not None except : return False def canonicalize_smiles ( smiles ): \"\"\"Convert to canonical SMILES\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None return Chem . MolToSmiles ( mol , canonical = True ) except : return None def remove_stereochemistry ( smiles ): \"\"\"Remove stereochemistry information\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) if mol is None : return None Chem . RemoveStereochemistry ( mol ) return Chem . MolToSmiles ( mol ) except : return None # ============================================================================ # SMILES AUGMENTATION # ============================================================================ def enumerate_smiles ( smiles , n_variants = 10 ): \"\"\"Generate different SMILES representations of same molecule\"\"\" try : mol = Chem . MolFromSmiles ( smiles ) if mol is None : return [ smiles ] variants = set () for _ in range ( n_variants * 2 ): # Generate more, then sample variant = Chem . MolToSmiles ( mol , doRandom = True ) variants . add ( variant ) variants = list ( variants ) # Ensure original is included if smiles not in variants : variants . append ( smiles ) return variants [: n_variants ] except : return [ smiles ] def augment_smiles_dataset ( smiles_list , labels , augmentation_factor = 5 ): \"\"\"Augment SMILES dataset with enumeration\"\"\" augmented_smiles = [] augmented_labels = [] for smiles , label in zip ( smiles_list , labels ): variants = enumerate_smiles ( smiles , n_variants = augmentation_factor ) augmented_smiles . extend ( variants ) augmented_labels . extend ([ label ] * len ( variants )) return augmented_smiles , augmented_labels # ============================================================================ # SMILES FILTERING # ============================================================================ def filter_smiles_dataset ( smiles_list , labels , remove_invalid = True , remove_duplicates = True , max_length = None , min_atoms = None , max_atoms = None ): \"\"\"Filter SMILES dataset based on criteria\"\"\" filtered_smiles = [] filtered_labels = [] seen_canonical = set () for smiles , label in zip ( smiles_list , labels ): # Check validity if remove_invalid : mol = Chem . MolFromSmiles ( smiles ) if mol is None : continue # Check duplicates if remove_duplicates : canonical = canonicalize_smiles ( smiles ) if canonical in seen_canonical : continue seen_canonical . add ( canonical ) # Check length if max_length is not None and len ( smiles ) > max_length : continue # Check atom count if min_atoms is not None or max_atoms is not None : mol = Chem . MolFromSmiles ( smiles ) n_atoms = mol . GetNumHeavyAtoms () if min_atoms is not None and n_atoms < min_atoms : continue if max_atoms is not None and n_atoms > max_atoms : continue filtered_smiles . append ( smiles ) filtered_labels . append ( label ) return filtered_smiles , filtered_labels # ============================================================================ # SMILES STATISTICS # ============================================================================ def compute_smiles_statistics ( smiles_list ): \"\"\"Compute statistics about SMILES dataset\"\"\" stats = { 'total_molecules' : len ( smiles_list ), 'valid_molecules' : 0 , 'length_stats' : {}, 'atom_stats' : {}, 'ring_stats' : {}, } lengths = [] atom_counts = [] ring_counts = [] for smiles in smiles_list : mol = Chem . MolFromSmiles ( smiles ) if mol is not None : stats [ 'valid_molecules' ] += 1 lengths . append ( len ( smiles )) atom_counts . append ( mol . GetNumHeavyAtoms ()) ring_counts . append ( Chem . GetSSSR ( mol )) # Length statistics stats [ 'length_stats' ] = { 'mean' : np . mean ( lengths ), 'std' : np . std ( lengths ), 'min' : np . min ( lengths ), 'max' : np . max ( lengths ), 'median' : np . median ( lengths ) } # Atom statistics stats [ 'atom_stats' ] = { 'mean' : np . mean ( atom_counts ), 'std' : np . std ( atom_counts ), 'min' : np . min ( atom_counts ), 'max' : np . max ( atom_counts ), 'median' : np . median ( atom_counts ) } # Ring statistics stats [ 'ring_stats' ] = { 'mean' : np . mean ( ring_counts ), 'std' : np . std ( ring_counts ), 'min' : np . min ( ring_counts ), 'max' : np . max ( ring_counts ) } return stats # ============================================================================ # EXAMPLE USAGE # ============================================================================ if __name__ == \"__main__\" : # Example SMILES smiles_examples = [ \"CCO\" , # Ethanol \"CC(=O)OC1=CC=CC=C1C(=O)O\" , # Aspirin \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\" , # Caffeine ] # Initialize tokenizer tokenizer = SMILESTokenizer () # Tokenize for smiles in smiles_examples : tokens = tokenizer . tokenize ( smiles ) encoded = tokenizer . encode ( smiles , max_length = 50 ) decoded = tokenizer . decode ( encoded ) print ( f \" \\n SMILES: { smiles } \" ) print ( f \"Tokens: { tokens } \" ) print ( f \"Encoded: { encoded [: 10 ] } ...\" ) print ( f \"Decoded: { decoded } \" ) # Augmentation print ( \" \\n\\n SMILES Enumeration:\" ) variants = enumerate_smiles ( smiles_examples [ 1 ], n_variants = 5 ) for i , variant in enumerate ( variants , 1 ): print ( f \" { i } . { variant } \" ) # Statistics print ( \" \\n\\n Dataset Statistics:\" ) stats = compute_smiles_statistics ( smiles_examples ) print ( f \"Total molecules: { stats [ 'total_molecules' ] } \" ) print ( f \"Valid molecules: { stats [ 'valid_molecules' ] } \" ) print ( f \"Average length: { stats [ 'length_stats' ][ 'mean' ] : .1f } \" ) print ( f \"Average atoms: { stats [ 'atom_stats' ][ 'mean' ] : .1f } \" )","title":"Appendix B: SMILES Processing Utilities"},{"location":"day2/#appendix__c__model__evaluation__suite","text":"\"\"\" Comprehensive Model Evaluation Suite Tools for thorough model performance analysis \"\"\" import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.metrics import ( mean_squared_error , mean_absolute_error , r2_score , confusion_matrix , roc_auc_score , precision_recall_curve , roc_curve , classification_report ) from scipy import stats import pandas as pd # ============================================================================ # REGRESSION METRICS # ============================================================================ class RegressionEvaluator : \"\"\"Comprehensive regression model evaluation\"\"\" def __init__ ( self , y_true , y_pred ): self . y_true = np . array ( y_true ) . flatten () self . y_pred = np . array ( y_pred ) . flatten () self . residuals = self . y_true - self . y_pred def compute_metrics ( self ): \"\"\"Compute all regression metrics\"\"\" metrics = {} # Basic metrics metrics [ 'MSE' ] = mean_squared_error ( self . y_true , self . y_pred ) metrics [ 'RMSE' ] = np . sqrt ( metrics [ 'MSE' ]) metrics [ 'MAE' ] = mean_absolute_error ( self . y_true , self . y_pred ) metrics [ 'R2' ] = r2_score ( self . y_true , self . y_pred ) # Additional metrics metrics [ 'Max_Error' ] = np . max ( np . abs ( self . residuals )) metrics [ 'Mean_Residual' ] = np . mean ( self . residuals ) metrics [ 'Std_Residual' ] = np . std ( self . residuals ) # Relative metrics metrics [ 'MAPE' ] = np . mean ( np . abs ( self . residuals / ( self . y_true + 1e-10 ))) * 100 # Correlation metrics [ 'Pearson_r' ], metrics [ 'Pearson_p' ] = stats . pearsonr ( self . y_true , self . y_pred ) metrics [ 'Spearman_r' ], metrics [ 'Spearman_p' ] = stats . spearmanr ( self . y_true , self . y_pred ) return metrics def plot_results ( self , save_path = 'regression_evaluation.png' ): \"\"\"Create comprehensive visualization\"\"\" fig , axes = plt . subplots ( 2 , 3 , figsize = ( 18 , 12 )) # 1. Predictions vs True axes [ 0 , 0 ] . scatter ( self . y_true , self . y_pred , alpha = 0.5 ) axes [ 0 , 0 ] . plot ([ self . y_true . min (), self . y_true . max ()], [ self . y_true . min (), self . y_true . max ()], 'r--' , lw = 2 ) axes [ 0 , 0 ] . set_xlabel ( 'True Values' ) axes [ 0 , 0 ] . set_ylabel ( 'Predicted Values' ) axes [ 0 , 0 ] . set_title ( 'Predictions vs True Values' ) axes [ 0 , 0 ] . grid ( True ) # Add R\u00b2 to plot r2 = r2_score ( self . y_true , self . y_pred ) axes [ 0 , 0 ] . text ( 0.05 , 0.95 , f 'R\u00b2 = { r2 : .3f } ' , transform = axes [ 0 , 0 ] . transAxes , verticalalignment = 'top' , bbox = dict ( boxstyle = 'round' , facecolor = 'wheat' , alpha = 0.5 )) # 2. Residuals vs Predicted axes [ 0 , 1 ] . scatter ( self . y_pred , self . residuals , alpha = 0.5 ) axes [ 0 , 1 ] . axhline ( y = 0 , color = 'r' , linestyle = '--' ) axes [ 0 , 1 ] . set_xlabel ( 'Predicted Values' ) axes [ 0 , 1 ] . set_ylabel ( 'Residuals' ) axes [ 0 , 1 ] . set_title ( 'Residual Plot' ) axes [ 0 , 1 ] . grid ( True ) # 3. Residuals Distribution axes [ 0 , 2 ] . hist ( self . residuals , bins = 50 , edgecolor = 'black' ) axes [ 0 , 2 ] . set_xlabel ( 'Residuals' ) axes [ 0 , 2 ] . set_ylabel ( 'Frequency' ) axes [ 0 , 2 ] . set_title ( 'Residual Distribution' ) axes [ 0 , 2 ] . axvline ( x = 0 , color = 'r' , linestyle = '--' ) axes [ 0 , 2 ] . grid ( True ) # 4. Q-Q Plot stats . probplot ( self . residuals , dist = \"norm\" , plot = axes [ 1 , 0 ]) axes [ 1 , 0 ] . set_title ( 'Q-Q Plot' ) axes [ 1 , 0 ] . grid ( True ) # 5. Absolute Error vs True abs_errors = np . abs ( self . residuals ) axes [ 1 , 1 ] . scatter ( self . y_true , abs_errors , alpha = 0.5 ) axes [ 1 , 1 ] . set_xlabel ( 'True Values' ) axes [ 1 , 1 ] . set_ylabel ( 'Absolute Error' ) axes [ 1 , 1 ] . set_title ( 'Absolute Error vs True Values' ) axes [ 1 , 1 ] . grid ( True ) # 6. Error Distribution by Range # Bin true values and compute error statistics bins = np . linspace ( self . y_true . min (), self . y_true . max (), 10 ) bin_indices = np . digitize ( self . y_true , bins ) bin_means = [] bin_stds = [] bin_centers = [] for i in range ( 1 , len ( bins )): mask = bin_indices == i if np . sum ( mask ) > 0 : bin_means . append ( np . mean ( abs_errors [ mask ])) bin_stds . append ( np . std ( abs_errors [ mask ])) bin_centers . append (( bins [ i - 1 ] + bins [ i ]) / 2 ) axes [ 1 , 2 ] . errorbar ( bin_centers , bin_means , yerr = bin_stds , fmt = 'o-' , capsize = 5 ) axes [ 1 , 2 ] . set_xlabel ( 'True Value Range' ) axes [ 1 , 2 ] . set_ylabel ( 'Mean Absolute Error' ) axes [ 1 , 2 ] . set_title ( 'Error by Value Range' ) axes [ 1 , 2 ] . grid ( True ) plt . tight_layout () plt . savefig ( save_path , dpi = 150 , bbox_inches = 'tight' ) plt . show () print ( f \"Evaluation plots saved to { save_path } \" ) def print_report ( self ): \"\"\"Print detailed evaluation report\"\"\" metrics = self . compute_metrics () print ( \" \\n \" + \"=\" * 70 ) print ( \"REGRESSION EVALUATION REPORT\" ) print ( \"=\" * 70 ) print ( \" \\n 1. Basic Metrics:\" ) print ( f \" MSE: { metrics [ 'MSE' ] : .4f } \" ) print ( f \" RMSE: { metrics [ 'RMSE' ] : .4f } \" ) print ( f \" MAE: { metrics [ 'MAE' ] : .4f } \" ) print ( f \" R\u00b2: { metrics [ 'R2' ] : .4f } \" ) print ( \" \\n 2. Error Statistics:\" ) print ( f \" Max Error: { metrics [ 'Max_Error' ] : .4f } \" ) print ( f \" Mean Residual: { metrics [ 'Mean_Residual' ] : .4f } \" ) print ( f \" Std Residual: { metrics [ 'Std_Residual' ] : .4f } \" ) print ( f \" MAPE: { metrics [ 'MAPE' ] : .2f } %\" ) print ( \" \\n 3. Correlation:\" ) print ( f \" Pearson r: { metrics [ 'Pearson_r' ] : .4f } (p= { metrics [ 'Pearson_p' ] : .4e } )\" ) print ( f \" Spearman r: { metrics [ 'Spearman_r' ] : .4f } (p= { metrics [ 'Spearman_p' ] : .4e } )\" ) print ( \" \\n 4. Data Statistics:\" ) print ( f \" True values: Mean= { np . mean ( self . y_true ) : .4f } , \" f \"Std= { np . std ( self . y_true ) : .4f } \" ) print ( f \" Predictions: Mean= { np . mean ( self . y_pred ) : .4f } , \" f \"Std= { np . std ( self . y_pred ) : .4f } \" ) print ( f \" N samples: { len ( self . y_true ) } \" ) print ( \" \\n \" + \"=\" * 70 ) # ============================================================================ # MODEL COMPARISON # ============================================================================ def compare_models ( results_dict , metric = 'RMSE' ): \"\"\" Compare multiple models Args: results_dict: Dict of {model_name: {'y_true': ..., 'y_pred': ...}} metric: Metric to use for comparison \"\"\" comparison_data = [] for model_name , results in results_dict . items (): evaluator = RegressionEvaluator ( results [ 'y_true' ], results [ 'y_pred' ]) metrics = evaluator . compute_metrics () comparison_data . append ({ 'Model' : model_name , 'RMSE' : metrics [ 'RMSE' ], 'MAE' : metrics [ 'MAE' ], 'R2' : metrics [ 'R2' ], 'Pearson_r' : metrics [ 'Pearson_r' ] }) df = pd . DataFrame ( comparison_data ) df = df . sort_values ( by = metric ) print ( \" \\n \" + \"=\" * 70 ) print ( \"MODEL COMPARISON\" ) print ( \"=\" * 70 ) print ( df . to_string ( index = False )) # Plot comparison fig , axes = plt . subplots ( 1 , 3 , figsize = ( 15 , 5 )) metrics_to_plot = [ 'RMSE' , 'MAE' , 'R2' ] for idx , metric in enumerate ( metrics_to_plot ): axes [ idx ] . bar ( df [ 'Model' ], df [ metric ]) axes [ idx ] . set_xlabel ( 'Model' ) axes [ idx ] . set_ylabel ( metric ) axes [ idx ] . set_title ( f ' { metric } Comparison' ) axes [ idx ] . tick_params ( axis = 'x' , rotation = 45 ) axes [ idx ] . grid ( True , axis = 'y' ) plt . tight_layout () plt . savefig ( 'model_comparison.png' , dpi = 150 , bbox_inches = 'tight' ) plt . show () return df # ============================================================================ # EXAMPLE USAGE # ============================================================================ if __name__ == \"__main__\" : # Generate synthetic data np . random . seed ( 42 ) n_samples = 1000 y_true = np . random . randn ( n_samples ) * 2 + 5 y_pred = y_true + np . random . randn ( n_samples ) * 0.5 # Evaluate evaluator = RegressionEvaluator ( y_true , y_pred ) evaluator . print_report () evaluator . plot_results () # Compare models results_dict = { 'Model A' : { 'y_true' : y_true , 'y_pred' : y_pred }, 'Model B' : { 'y_true' : y_true , 'y_pred' : y_true + np . random . randn ( n_samples ) * 0.7 }, 'Model C' : { 'y_true' : y_true , 'y_pred' : y_true + np . random . randn ( n_samples ) * 0.3 }, } compare_models ( results_dict )","title":"Appendix C: Model Evaluation Suite"},{"location":"day3/","text":"Day 3: Graph Neural Networks and Geometric Deep Learning \u00b6 Overview \u00b6 Day 3 focuses on Graph Neural Networks (GNNs) and their applications in molecular and materials science. We\u2019ll explore how graphs can represent molecular structures and how specialized neural network architectures can learn from these representations while respecting physical symmetries and constraints. Topics Covered \u00b6 1. Message Passing Neural Networks \u00b6 Message Passing Neural Networks (MPNNs) form the foundation of modern graph neural networks for molecular property prediction. They provide a unified framework for understanding how information flows through graph-structured data, making them particularly well-suited for molecular modeling where atoms and bonds naturally form graph structures. Core Concepts \u00b6 Graph Representation: In molecular graphs, the structure naturally maps to graph representations: - Nodes represent atoms with rich feature vectors including: - Atomic number (element identity) - Formal charge and partial charge - Hybridization state (sp, sp2, sp3) - Number of hydrogen atoms - Aromaticity - Chirality - Atomic mass - Degree (number of connections) Edges represent bonds with attributes such as: Bond type (single, double, triple, aromatic) Bond length/distance Stereochemistry (cis/trans, E/Z) Conjugation Ring membership The graph structure encodes molecular connectivity, preserving the topological relationships that determine chemical properties Why Message Passing? The key insight behind MPNNs is that the properties of an atom in a molecule depend not just on the atom itself, but on its chemical environment. Message passing mimics how chemical effects propagate through molecular structures: - Inductive effects travel through sigma bonds - Resonance effects propagate through conjugated systems - Steric effects depend on spatial arrangements of neighboring groups Message Passing Framework: The MPNN framework consists of three main phases that iterate to build increasingly sophisticated representations: Message Phase (T iterations): m_v^(t+1) = \u03a3_{u\u2208N(v)} M_t(h_v^t, h_u^t, e_{uv}) In this phase: - Each node v receives messages from its neighbors N(v) - The message function M_t is a learnable neural network that combines: - h_v^t: the current node\u2019s hidden state - h_u^t: each neighbor\u2019s hidden state - e_{uv}: edge features connecting the nodes - Messages are computed for all edges simultaneously - The superscript t indicates the iteration number Intuition : Think of this as each atom \u201clistening\u201d to what its bonded neighbors are telling it about their local chemical environment. Update Phase : h_v^(t+1) = U_t(h_v^t, m_v^(t+1)) In this phase: - Each node updates its representation using the aggregated messages - U_t is typically a GRU, LSTM, or feedforward network - The update combines the node\u2019s previous state with new information - This preserves information from earlier iterations while incorporating new context Intuition : After listening to neighbors, each atom updates its own representation to reflect what it learned about its environment. Aggregation Within Messages : The summation in the message phase is just one choice. Other aggregation functions include: - Sum : \u03a3_{u\u2208N(v)} M_t(...) - sensitive to neighborhood size - Mean : (1/|N(v)|) \u03a3_{u\u2208N(v)} M_t(...) - normalizes by degree - Max : max_{u\u2208N(v)} M_t(...) - captures strongest signal - Attention : \u03a3_{u\u2208N(v)} \u03b1_{vu} M_t(...) - learnable weights (GAT) Readout Phase: After T message passing steps, we have node-level representations h_v^T for each atom. To predict molecular properties, we need a graph-level representation: y = R({h_v^T | v \u2208 G}) Common readout functions include: Sum pooling : R = \u03a3_v h_v^T Captures total contributions from all atoms Sensitive to molecule size Good for extensive properties (like mass, number of electrons) Mean pooling : R = (1/|V|) \u03a3_v h_v^T Normalizes by number of atoms Better for intensive properties (like density, stability per atom) Size-invariant representation Max pooling : R = max_v h_v^T (element-wise) Captures most significant features Can miss important distributed information Set2Set : A learnable attention-based aggregation Uses LSTM to iteratively attend to nodes More expressive but computationally expensive Can capture complex relationships between atoms Depth and Receptive Fields: The number of message passing iterations T determines the receptive field: - T=1: Each node sees only immediate neighbors (1-hop) - T=2: Each node sees neighbors and neighbors-of-neighbors (2-hop) - T=k: Each node sees all nodes within k bonds For molecular graphs: - Small molecules (QM9): T=3-5 is usually sufficient - Proteins: T=5-10 may be needed for long-range interactions - Trade-off: More iterations = larger receptive field but risk of over-smoothing Key Variants \u00b6 Graph Convolutional Networks (GCN): GCNs simplify message passing using a spectral approach: H^(t+1) = \u03c3(D^(-1/2) \u00c3 D^(-1/2) H^(t) W^(t)) Where: - \u00c3 = A + I (adjacency matrix with self-loops) - D is the degree matrix - This is equivalent to message passing with normalized averaging - Very efficient for semi-supervised learning on large graphs - Less flexible for edge features than general MPNNs Benefits for chemistry: - Fast computation on molecular graphs - Symmetric normalization prevents exploding/vanishing gradients - Can be stacked deeply with residual connections Limitations: - Doesn\u2019t naturally incorporate edge features - Fixed aggregation (normalized sum) - May struggle with distinguishing certain graph structures (limited expressivity) GraphSAGE (Sample and Aggregate): GraphSAGE introduces sampling for scalability: h_v^(t+1) = \u03c3(W \u00b7 CONCAT(h_v^t, AGG({h_u^t | u \u2208 Sample(N(v))}))) Key innovations: - Sampling : Instead of aggregating from all neighbors, sample a fixed number - Multiple aggregators : - Mean: AGG = (1/|S|) \u03a3_{u\u2208S} h_u - LSTM: Process neighbors sequentially (requires ordering) - Pooling: AGG = max(\u03c3(W_pool h_u + b)) - Concatenation : Explicitly preserves self-information Benefits for chemistry: - Handles variable-sized neighborhoods efficiently - Inductive learning: can generalize to new molecules not seen during training - Scalable to very large molecular databases Neural Message Passing for Quantum Chemistry (MPNN): The original MPNN paper specialized the framework for molecules: Architecture: m_v^(t+1) = \u03a3_{u\u2208N(v)} M_t(h_v^t, h_u^t, e_{uv}) = \u03a3_{u\u2208N(v)} A_t(e_{uv}) \u00b7 h_u^t h_v^(t+1) = U_t(h_v^t, m_v^(t+1)) = GRU(h_v^t, m_v^(t+1)) Key design choices: - Edge networks : A_t(e_{uv}) is a neural network that produces edge-specific matrices - Allows different bond types to transform neighbor information differently - Captures the idea that single/double/triple bonds transmit information differently GRU updates : Using Gated Recurrent Units for the update function Helps with gradient flow through multiple message passing steps Gates control what information to keep vs. update More stable than simple MLPs for deep message passing Virtual edges : Can add edges between non-bonded atoms within a distance cutoff Captures through-space interactions Important for conformational effects and weak interactions Master equations: # Initialize h_v^0 = embedding(x_v) # x_v are input features # Message passing for t in range(T): for each edge (v,u): m_vu = EdgeNetwork(e_vu) @ h_u^t m_v = \u03a3_u m_vu h_v^(t+1) = GRU(h_v^t, m_v) # Readout h_G = Set2Set({h_v^T | v \u2208 G}) y = MLP(h_G) Training considerations: - Typically T=3-6 message passing steps - Hidden dimensions: 64-256 depending on task complexity - Batch normalization or layer normalization helps training stability - Dropout between layers prevents overfitting Applications in Chemistry \u00b6 Molecular Property Prediction: MPNNs excel at predicting quantum mechanical and physical properties: Quantum properties (QM9 dataset): HOMO/LUMO energies (frontier orbitals) Internal energy and enthalpy Free energy and heat capacity Electronic spatial extent Zero-point vibrational energy Atomization energy Why MPNNs work : These properties depend on electronic structure, which is determined by how atoms and bonds are connected. Message passing naturally captures these structural effects. Physical properties : Solubility (important for drug absorption) Melting/boiling points Density and refractive index Viscosity Challenge : These properties can depend on 3D conformations, so incorporating geometry helps. Biological activity : Toxicity predictions (hERG, Ames, hepatotoxicity) Binding affinity to target proteins ADMET properties (Absorption, Distribution, Metabolism, Excretion, Toxicity) Blood-brain barrier penetration Application : Early-stage drug filtering, reducing costly experimental screening. Reaction Outcome Prediction: Given reactants and conditions, predict the major products: - Graph of reactants \u2192 MPNN \u2192 Product distribution - Can incorporate reaction conditions (temperature, solvent, catalysts) as global features - Attention mechanisms can identify reactive sites Retrosynthesis Planning: Predict how to synthesize a target molecule: - Target molecule \u2192 MPNN \u2192 Likely precursors - Can be formulated as a translation problem (product graph \u2192 reactant graphs) - Helps chemists find synthetic routes for complex molecules Drug Discovery and Virtual Screening: Screen millions of compounds against target proteins: - Fast prediction once model is trained (~1000 molecules/second) - Can be combined with active learning to guide experimental efforts - Multi-task learning: predict multiple properties simultaneously - Transfer learning: pre-train on large databases, fine-tune on specific targets De Novo Molecular Design: Use MPNNs as discriminators or reward functions in generative models Guide molecular generation toward desired properties Combine with optimization algorithms (genetic algorithms, reinforcement learning) Limitations and Challenges \u00b6 Over-smoothing : With many message passing steps, node representations become too similar - Solution : Residual connections, jumping knowledge networks Limited expressivity : Some graphs are indistinguishable by message passing - Solution : Add higher-order structural features, use more sophisticated aggregation Scalability : Large molecules or protein graphs can be computationally expensive - Solution : Sampling (GraphSAGE), hierarchical approaches, graph coarsening 3D structure : Basic MPNNs ignore 3D geometry - Solution : Add distance as edge features, use equivariant networks (next sections) 2. Graph Attention Networks \u00b6 Graph Attention Networks (GATs) introduce attention mechanisms to graph learning, allowing the model to learn which neighbors are most important for each node. This is a significant advancement over basic message passing, where all neighbors contribute equally to a node\u2019s update. Motivation and Intuition \u00b6 Why Attention for Graphs? In molecular contexts, not all bonds are equally important for determining a property: - In a large molecule, a specific functional group might dominate reactivity - Some atoms are in the \u201ccore\u201d structure while others are in peripheral substituents - Certain bonds participate in conjugation or resonance, making them more significant - In protein-ligand binding, only residues near the binding site matter Analogy to NLP : Just as in language, where \u201cbank\u201d means different things in \u201criver bank\u201d vs \u201csavings bank\u201d depending on context, an atom\u2019s role depends on which neighbors are most relevant. GATs allow the network to automatically learn these context-dependent importance weights. Attention Mechanism \u00b6 Core Idea: Unlike MPNNs that use fixed aggregation (sum, mean) or hand-crafted edge weights, GATs learn attention coefficients \u03b1_{ij} that adaptively weigh the importance of each neighbor j for node i. Mathematical Framework: The attention mechanism consists of three steps: Step 1: Compute Attention Logits e_{ij} = LeakyReLU(a^T [W h_i || W h_j]) Breaking this down: - W h_i and W h_j : First, transform node features through a shared weight matrix W - This projects features into a common space where comparisons are meaningful - Dimension: [d_in] \u2192 [d_out] [W h_i || W h_j] : Concatenate transformed features of node i and neighbor j Creates a pairwise feature vector Dimension: [2 \u00d7 d_out] a^T [\u2026] : Apply learned attention vector a Reduces to scalar attention logit e_{ij} The attention vector a learns what feature combinations indicate importance Dimension: [2 \u00d7 d_out] \u2192 [1] LeakyReLU : Non-linearity with slope \u03b1 for negative values (typically \u03b1=0.2) Allows negative attention scores (before softmax) Prevents dead neurons from ReLU saturation Intuition : The attention logit e_{ij} measures how relevant neighbor j is to node i, based on their feature compatibility. Step 2: Normalize to Attention Coefficients \u03b1_{ij} = softmax_j(e_{ij}) = exp(e_{ij}) / \u03a3_{k\u2208N(i)} exp(e_{ik}) Softmax normalization : Ensures attention weights sum to 1 over all neighbors Comparison : Only neighbors compete for attention (not all nodes in graph) Interpretation : \u03b1_{ij} \u2208 (0, 1) represents the probability-like importance of neighbor j Why softmax? - Preserves differentiability (can backpropagate) - Creates sharp distinctions (high e_{ij} \u2192 high \u03b1_{ij}) - Normalized weights prevent exploding values in aggregation Step 3: Aggregate with Attention Weights h_i' = \u03c3(\u03a3_{j\u2208N(i)} \u03b1_{ij} W h_j) Weighted sum : Each neighbor contributes proportionally to its attention weight W h_j : Uses the same transformation from step 1 (parameter sharing) \u03c3 : Activation function (typically ELU or ReLU) Complete Forward Pass Example: For an atom with 3 neighbors: 1. Compute logits: e_{i1} = 0.8, e_{i2} = 0.3, e_{i3} = -0.2 2. Apply softmax: \u03b1_{i1} = 0.52, \u03b1_{i2} = 0.31, \u03b1_{i3} = 0.17 3. Aggregate: h_i\u2019 = \u03c3(0.52 \u00d7 W h_1 + 0.31 \u00d7 W h_2 + 0.17 \u00d7 W h_3) Key Properties: Self-attention : Can include self-loops (i,i) so nodes attend to themselves Asymmetric : \u03b1_{ij} \u2260 \u03b1_{ji} (attention from i\u2192j differs from j\u2192i) Local : Only attends to direct neighbors (preserves graph structure) Permutation invariant : Order of neighbors doesn\u2019t matter Multi-Head Attention \u00b6 To stabilize learning and capture different types of relationships simultaneously, GATs employ multiple independent attention mechanisms (heads). Multi-Head Aggregation (Hidden Layers): h_i' = ||_{k=1}^K \u03c3(\u03a3_{j\u2208N(i)} \u03b1_{ij}^k W^k h_j) Where: - K = number of attention heads - Each head k has its own parameters: W^k and a^k - || denotes concatenation of head outputs - Output dimension: K \u00d7 d_out Why Multiple Heads? Different heads can learn complementary attention patterns: - Head 1 : Might focus on electronegative atoms (for polarity) - Head 2 : Might focus on aromatic neighbors (for conjugation) - Head 3 : Might focus on steric bulk (for size effects) - Head 4 : Might attend to formal charges Intuition from Chemistry : Just as a chemist considers multiple factors simultaneously (electronics, sterics, orbital interactions), multiple heads capture different aspects of molecular structure. Multi-Head Averaging (Output Layer): h_i' = \u03c3((1/K) \u03a3_{k=1}^K \u03a3_{j\u2208N(i)} \u03b1_{ij}^k W^k h_j) For the final layer, averaging instead of concatenation: - Keeps output dimension consistent with target - Ensembles the predictions from different heads - More stable for final predictions Implementation Considerations: # Typical hyperparameters num_heads = 4 - 8 # More heads = more capacity but more parameters hidden_dim = 64 - 256 # Per-head dimension dropout = 0.1 - 0.3 # On attention coefficients (attention dropout) Computational Complexity: - Attention computation: O(|E| \u00d7 d_out) - linear in edges - Memory for attention: O(|E| \u00d7 K) - stores attention per head - Highly parallelizable (all attention coefficients computed independently) Advantages of GATs \u00b6 1. Adaptive Neighborhoods Unlike fixed aggregation: # Fixed (GCN-style) h_i' = \u03a3_{j\u2208N(i)} (1/\u221a(d_i \u00d7 d_j)) W h_j # predetermined weights # Adaptive (GAT) h_i' = \u03a3_{j\u2208N(i)} \u03b1_{ij} W h_j # learned weights Benefits: - Automatically focuses on relevant neighbors - Can ignore uninformative connections - Adapts to different chemical contexts Example : In a drug molecule, GAT can learn to focus on: - Pharmacophore groups (active parts) - Hydrogen bond donors/acceptors - Hydrophobic regions While downweighting inert carbon chains. 2. Interpretability Attention weights \u03b1_{ij} can be visualized and interpreted: # Extract attention weights attention_weights = model . get_attention () # shape: [num_edges, num_heads] # Visualize for a molecule mol_graph = molecule . to_graph () highlight_edges_by_weight ( mol_graph , attention_weights , threshold = 0.3 ) What we can learn: - Which atoms influence predictions most - How information flows through the molecule - Whether the model learned chemically meaningful patterns - Debugging: Are attention patterns reasonable? Example insights: - High attention on C=O bonds for carbonyl chemistry - Focus on aromatic rings for \u03c0-stacking predictions - Attention following conjugation pathways 3. Parallelizability All attention coefficients for all edges can be computed in parallel: # Pseudo-code edge_features = concat ( h [ edges [:, 0 ]], h [ edges [:, 1 ]]) # All edges at once attention_logits = attention_network ( edge_features ) # Parallel attention_weights = softmax_per_node ( attention_logits ) # Parallel within neighbors Speed advantages: - GPU-friendly (matrix operations) - Scales well to large molecules - No sequential dependencies (unlike RNNs) 4. Inductive Learning GATs can generalize to completely new graph structures: - Train on small molecules, test on large ones - Learn patterns that transfer across different molecular classes - No fixed graph structure required during training This is critical for: - Generalizing to novel drug candidates - Transfer learning across datasets - Handling molecules with varying sizes GAT Variants and Extensions \u00b6 GATv2: More Expressive Attention Original GAT limitation: Attention is somewhat static # GAT: Transform then attend e_{ij} = a^T [W h_i || W h_j] # a can only linearly combine features GATv2 improvement: Dynamic attention # GATv2: Attend then transform e_{ij} = a^T LeakyReLU(W [h_i || h_j]) # Non-linearity before attention Why it\u2019s better: - The non-linearity is applied AFTER concatenation - Allows more complex attention functions - Empirically: 10-30% better performance on many benchmarks - Fixes theoretical expressivity limitations of original GAT When to use GATv2: - Complex molecules with subtle structural differences - When original GAT plateaus in performance - Tasks requiring fine-grained attention distinctions Molecular GAT: Edge Features Challenge: Chemical bonds have important attributes (single/double/triple, stereochemistry) that basic GAT ignores. Solution : Incorporate edge features into attention e_{ij} = a^T [W h_i || W h_j || E e_{ij}] Where: - e_{ij} = edge feature vector (bond type, distance, ring membership) - E = edge embedding matrix - Now attention depends on both node features AND edge features Applications: - Distinguishing single vs double bonds - Incorporating 3D distances - Using bond order information - Representing stereochemistry Example : In conjugated systems, \u03c0-bonds should have higher attention than \u03c3-bonds: # Single bond: e_{ij} = [1,0,0] \u2192 lower attention # Double bond: e_{ij} = [0,1,0] \u2192 higher attention (for delocalization) # Triple bond: e_{ij} = [0,0,1] \u2192 highest attention Graph Transformer Networks: Extension to full graph attention (not just neighbors): \u03b1_{ij} for all pairs (i,j) in graph # Not just edges Trade-offs: - Pro : Can capture long-range interactions - Pro : More flexible attention patterns - Con : O(|V|\u00b2) complexity (vs O(|E|) for GAT) - Con : May lose graph structural bias Use when: Long-range interactions matter (large molecules, proteins) Practical Tips for Using GATs \u00b6 Hyperparameter Selection: # Good starting points num_layers = 3 - 5 # Deeper than this risks over-smoothing num_heads = 4 - 8 # More heads for complex tasks hidden_dim = 64 - 128 per head dropout = 0.1 - 0.3 # Higher dropout for small datasets learning_rate = 0.001 # Adam optimizer # For QM9 dataset config = { 'num_layers' : 4 , 'num_heads' : 4 , 'hidden_dim' : 128 , 'dropout' : 0.2 , 'attention_dropout' : 0.1 # Separate dropout on attention weights } Common Pitfalls: Forgetting self-loops : Add explicit (i,i) edges or include h_i in aggregation Over-smoothing : Too many layers \u2192 all nodes become similar Attention collapse : All attention goes to one neighbor (use attention dropout) Memory issues : K heads \u00d7 L layers can use lots of GPU memory Best Practices: # 1. Add residual connections h_i ' = h_i + GAT(h_i) # Prevents over-smoothing # 2. Use layer normalization h_i ' = LayerNorm(h_i + GAT(h_i)) # Stabilizes training # 3. Attention dropout \u03b1_ { ij } = Dropout ( softmax ( e_ { ij })) # Regularizes attention # 4. Edge features when available e_ { ij } = [ bond_type , distance , ring_membership ] # Richer edges Visualization Strategies: # 1. Attention heatmaps plot_attention_matrix ( attention_weights , molecule ) # 2. Highlight important edges highlight_edges_above_threshold ( molecule , attention_weights > 0.3 ) # 3. Track attention across layers for layer in model . layers : visualize_attention_distribution ( layer . attention ) # 4. Compare heads for head in range ( num_heads ): visualize_attention_head ( head , attention_weights ) 3. Equivariant Networks for 3D Structures \u00b6 Geometric deep learning architectures respect the symmetries and geometric properties of 3D molecular structures, making them particularly powerful for computational chemistry and materials science. These networks go beyond simple graph connectivity to leverage the full 3D geometry of molecules. Motivation: Why Geometry Matters \u00b6 The 3D Problem: Traditional GNNs treat molecular graphs as topological structures, ignoring spatial arrangements: - Two molecules with the same connectivity but different 3D shapes (stereoisomers) would be treated identically - Bond angles and dihedral angles contain crucial information - 3D distance-based interactions (van der Waals, electrostatics) are not captured - Forces and other vector properties require 3D information Example : Consider two stereoisomers: cis-2-butene: H\u2083C-CH=CH-CH\u2083 (groups on same side) trans-2-butene: H\u2083C-CH=CH-CH\u2083 (groups on opposite sides) These have: - Identical graph connectivity - Different 3D structures - Different physical properties (melting point, boiling point, reactivity) A topology-only GNN cannot distinguish them, but a geometry-aware network can. Understanding Symmetries \u00b6 Physical Symmetries in Molecular Systems: Molecular properties must respect fundamental physical symmetries: Translation Invariance : Property(molecule) = Property(molecule + translation vector) Moving the entire molecule in space doesn\u2019t change its energy or properties Only relative positions matter, not absolute coordinates Mathematically: E(R + t) = E(R) for any translation t Rotation Invariance : Property(molecule) = Property(rotate(molecule, \u03b8)) Rotating the molecule doesn\u2019t change scalar properties (energy, dipole magnitude) Physical measurements don\u2019t depend on orientation in space Mathematically: E(QR) = E(R) for any rotation matrix Q Permutation Invariance : Property(atoms[1,2,3,...]) = Property(atoms[permutation]) Labeling atoms 1,2,3 vs 3,1,2 shouldn\u2019t change properties Physical reality has no preferred ordering Node ordering is an artifact of representation Reflection (for some properties) : Chiral molecules break reflection symmetry Achiral molecules maintain E(R) = E(mirror(R)) Invariance vs Equivariance: Invariant : Output doesn\u2019t change under transformation Example: Energy is rotation-invariant scalar E(rotate(molecule)) = E(molecule) Equivariant : Output transforms consistently with input Example: Forces are rotation-equivariant vectors F(rotate(molecule)) = rotate(F(molecule)) If you rotate the molecule, forces rotate the same way Why This Matters: Networks that violate these symmetries will: - Learn to recognize rotated versions as different molecules (inefficient) - Require much more training data to learn all orientations - Fail to generalize to new orientations - Predict unphysical results (energy changing with rotation) Networks that respect symmetries: - Are more data-efficient (one orientation teaches all) - Generalize better to new configurations - Satisfy physical laws by construction - Often achieve better accuracy with fewer parameters SchNet (Continuous-filter Convolutional Neural Network) \u00b6 SchNet pioneered the use of continuous convolutions for molecular modeling, treating molecules as continuous 3D objects rather than discrete graphs. Core Philosophy: Instead of learning fixed filters for discrete bond types, SchNet learns continuous functions that depend smoothly on interatomic distances. This mirrors physics: interactions depend on distance in a continuous way (not discrete jumps). Architecture Overview: Input: - Atomic numbers Z = [Z\u2081, Z\u2082, ..., Z_N] - 3D coordinates R = [r\u2081, r\u2082, ..., r_N] Output: - Molecular property (energy, HOMO, etc.) Step 1: Atomic Embeddings Each atom type is embedded into a feature space: x_i^(0) = Embedding(Z_i) # Lookup table: atomic number \u2192 d-dimensional vector For example: - Carbon (Z=6) \u2192 [0.12, -0.34, 0.56, \u2026] - Nitrogen (Z=7) \u2192 [0.08, -0.29, 0.61, \u2026] - Oxygen (Z=8) \u2192 [0.15, -0.41, 0.48, \u2026] Step 2: Continuous Filter Generation The innovation of SchNet: filters are functions of distance, not learned for discrete bins. # Compute all pairwise distances d_ij = ||r_i - r_j|| # Euclidean distance # Expand distances using radial basis functions (RBFs) e_ij = [exp(-(d_ij - \u03bc_k)\u00b2 / \u03c3\u00b2) for k in range(K)] # Generate filter weights from expanded distances W_ij = MLP(e_ij) # Neural network: \u211d^K \u2192 \u211d^(d\u00d7d) Radial Basis Functions (RBFs): RBFs create a smooth representation of distances: # Example: Gaussian RBFs centered at different distances \u03bc = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, ...] # Centers in \u00c5ngstr\u00f6ms \u03c3 = 0.1 # Width For distance d_ij = 1.8 \u00c5: RBF\u2081(1.8) = exp(-(1.8-0.5)\u00b2/0.01) \u2248 0.00 # Far from center RBF\u2082(1.8) = exp(-(1.8-1.0)\u00b2/0.01) \u2248 0.00 RBF\u2083(1.8) = exp(-(1.8-1.5)\u00b2/0.01) \u2248 0.74 # Close to center RBF\u2084(1.8) = exp(-(1.8-2.0)\u00b2/0.01) \u2248 0.82 # Closest center ... This creates a smooth, continuous representation that: - Captures nuances between bond lengths - Allows interpolation to unseen distances - Provides smooth gradients for optimization Step 3: Interaction Blocks (Message Passing) # For each interaction layer t: x_i^(t+1) = x_i^(t) + \u03a3_{j\u2260i} x_j^(t) \u2299 W_ij^(t) Where: \u2299 is element-wise multiplication (Hadamard product) W_ij^(t) = FilterNetwork_t(d_ij) is the distance-dependent filter Detailed Breakdown: Cutoff Function : Only interact with nearby atoms if d_ij > r_cutoff (typically 5-10 \u00c5): W_ij = 0 # No interaction else: W_ij = FilterNetwork(d_ij) \u00d7 smooth_cutoff(d_ij) Smooth cutoff prevents discontinuities: f_cutoff(d) = 0.5 \u00d7 [cos(\u03c0 \u00d7 d / r_cutoff) + 1] if d < r_cutoff 0 otherwise Filter Network Architecture : e = RBF_expansion(d_ij) # [K] vector h = Dense(e) # [K] \u2192 [hidden_dim] h = ShiftedSoftplus(h) # Smooth non-linearity W = Dense(h) # [hidden_dim] \u2192 [d\u00d7d] W = W \u00d7 f_cutoff(d_ij) # Apply cutoff Atom-wise Update : # Aggregate filtered neighbor features m_i = \u03a3_{j\u2208neighbors(i)} x_j \u2299 W_ij # Update with residual connection x_i = x_i + m_i Step 4: Output Modules After T interaction blocks: # Atom-wise energy contributions E_i = MLP_atom(x_i^(T)) # Scalar per atom # Sum for total energy (size-extensive property) E_total = \u03a3_i E_i # Or average for intensive properties Property = (1/N) \u03a3_i MLP(x_i^(T)) Key Design Choices: Only distances, not coordinates : Distances are rotation and translation invariant ||r_i - r_j|| is the same regardless of overall position/orientation This guarantees the network is invariant Continuous filters : Smoothly adapts to any distance Better than discrete binning (1.0-1.5 \u00c5, 1.5-2.0 \u00c5, \u2026) Allows accurate interpolation Shifted Softplus activation : ShiftedSoftplus(x) = log(0.5 \u00d7 exp(x) + 0.5) Smooth everywhere (unlike ReLU) Important for force prediction: forces = -\u2207E Provides smooth gradients Strengths: - Efficient : Linear in number of atoms (with cutoff) - Scalable : Handles molecules from 10 to 1000+ atoms - Accurate : State-of-the-art on QM9 and other benchmarks - End-to-end differentiable : Can predict forces via backpropagation - Physically motivated : Design mirrors physics of interactions Limitations: - Distance-only : Doesn\u2019t explicitly capture angles - Isotropic : Treats all directions equally (no directionality) - Cannot predict vector properties directly : Forces require gradients Implementation Notes: # Typical hyperparameters num_interactions = 6 # Depth of network num_gaussians = 50 # Number of RBF centers cutoff = 5.0 # Interaction cutoff (\u00c5) hidden_dim = 128 # Feature dimension num_filters = 128 # Filter network dimension DimeNet (Directional Message Passing Neural Network) \u00b6 DimeNet extends SchNet by incorporating angular information, making it significantly more expressive for molecular modeling. Key Innovation: Beyond Distances While distances capture bond lengths, many properties depend on: - Bond angles : H-O-H angle in water determines properties - Dihedral angles : Rotations around single bonds affect conformation - Triplet interactions : Three-body terms in force fields Physics Analogy : - SchNet \u2248 Lennard-Jones potential (pairwise, distance-only) - DimeNet \u2248 Force fields with angle terms (CHARMM, AMBER) Architecture: Directional Messages DimeNet introduces messages that depend on triplets of atoms (i,j,k): k / /\u03b8_{ijk} / j -------- i d_jk d_ij Step 1: Distance and Angle Embeddings # Distance embedding (like SchNet) e_dist(d_ij) = RBF_expansion(d_ij) # NEW: Angle embedding \u03b8_{ijk} = angle between vectors (r_j - r_i) and (r_k - r_j) e_angle(\u03b8_{ijk}) = SphericalBasisFunctions(\u03b8_{ijk}) Spherical Basis Functions: Instead of Gaussians (for distances), use spherical harmonics for angles: # Angles are periodic: 0\u00b0 = 360\u00b0 # Use basis that respects this periodicity SBF_k(\u03b8) = \u03a3_n c_{nk} \u00d7 exp(-(n - n_0)\u00b2/\u03c3\u00b2) \u00d7 sin(n\u03b8) # These form a complete basis for representing angular functions Step 2: Directional Message Passing The crucial innovation - messages depend on geometric triplets: # For each atom i: m_ij = \u03a3_{k\u2208N(j), k\u2260i} MessageBlock(d_ij, d_jk, \u03b8_{ijk}) \u2299 x_k Let\u2019s break down MessageBlock(d_ij, d_jk, \u03b8_{ijk}) : def MessageBlock ( d_ij , d_jk , theta_ijk ): # Embed distances rbf_ij = RBF ( d_ij ) # [n_rbf] rbf_jk = RBF ( d_jk ) # [n_rbf] # Embed angle sbf = SphericalBasis ( theta_ijk ) # [n_sbf] # Combine using bilinear layer # This learns correlations between distances and angles W = BilinearLayer ( rbf_ij , rbf_jk , sbf ) # [d \u00d7 d] matrix return W Bilinear Layer Explained: The bilinear layer is crucial for combining geometric features: W = \u03a3_m \u03a3_n \u03a3_l U_{mnl} \u00d7 rbf_ij[m] \u00d7 rbf_jk[n] \u00d7 sbf[l] Where U is a learned tensor of parameters This allows the network to learn patterns like: - \u201cWhen d_ij \u2248 1.5 \u00c5 AND d_jk \u2248 1.2 \u00c5 AND \u03b8 \u2248 120\u00b0\u201d \u2192 strong interaction - Captures correlations between geometric features - More expressive than treating features independently Step 3: Update with Directional Information # Aggregate directional messages m_i = \u03a3_{j\u2208N(i)} m_ij # Update node features x_i = Update(x_i, m_i) # Typically a residual network Why This Works Better: Angular Information : Distinguishes linear vs bent vs tetrahedral Example: CO\u2082 (linear, 180\u00b0) vs H\u2082O (bent, 104.5\u00b0) Same atom types, different angles \u2192 different properties Dihedral Sensitivity : Captures rotational barriers Ethane: staggered vs eclipsed conformations Different angles \u2192 different energies Three-Body Interactions : More realistic physics Many quantum effects involve three atoms Necessary for accurate force fields Maintaining Rotational Invariance: Despite using angles, DimeNet remains rotationally invariant because: - Angles are invariant: \u03b8_{ijk} doesn\u2019t change under rotation - Only distances and angles used (not absolute coordinates) - No preferred orientation in space DimeNet++ Improvements: The original DimeNet was slow. DimeNet++ optimized: # DimeNet: T-shaped message passing Message: k \u2192 j \u2192 i (sequential) # DimeNet++: Optimized message aggregation Message: All k \u2192 all j \u2192 all i (more parallel) Optimization strategies: 1. Shared bilinear layers : Reduce parameters 2. Efficient triplet enumeration : Better data structures 3. Grouped convolutions : Reduce computational cost 4. Memory-efficient attention : Lower memory footprint Performance: - DimeNet++: 2-5\u00d7 faster than DimeNet - Same or better accuracy - Can handle larger molecules (100+ atoms) Advantages: - Higher accuracy : 20-40% better MAE on QM9 vs SchNet - Captures geometry : Angles and dihedrals encoded - Physics-aware : Mirrors force field design - Still rotationally invariant : Maintains symmetries Disadvantages: - Computational cost : O(N \u00d7 k\u00b2) where k is coordination number - Memory : Stores triplets, not just pairs - Complexity : More hyperparameters to tune Use Cases: - High-accuracy property prediction - Conformational energy differences - Transition state geometries - Systems where angles matter (chelates, rings, etc.) PaiNN (Polarizable Atom Interaction Neural Network) \u00b6 PaiNN represents a paradigm shift: instead of just invariant features, it maintains both scalar (invariant) and vector (equivariant) representations. Motivation: Vector Properties Many important properties are vectors (have direction): - Forces : F = -\u2207E (gradient of energy) - Dipole moments : \u03bc = \u03a3_i q_i r_i - Magnetic moments : Direction matters - Polarizability : Tensorial response to fields Previous models (SchNet, DimeNet): - Can only predict scalar outputs directly - Forces require numerical differentiation: F = -dE/dR - Inefficient and sometimes inaccurate PaiNN: - Predicts vectors directly - Learns force fields end-to-end - Truly equivariant architecture Equivariance Explained: For a rotation matrix Q: Invariant (scalar): f(QR) = f(R) Example: ||v|| = ||Qv|| (length unchanged) Equivariant (vector): f(QR) = Q f(R) Example: Qv rotates the same way as input Equivariant (rank-2 tensor): f(QR) = Q f(R) Q^T Example: Stress tensor transforms Feature Representation: Each atom i has TWO types of features: Scalar features s_i \u2208 \u211d^d: Rotation invariant Examples: atomic charge, energy contribution, electron density Transforms: s_i \u2192 s_i (unchanged under rotation) Vector features v_i \u2208 \u211d^(d\u00d73): Rotation equivariant Examples: dipole moment, force vector, polarization Transforms: v_i \u2192 Q v_i (rotates with molecule) Architecture Overview: Input: (s_i^(0), v_i^(0)) for each atom s_i^(0) = embedding(Z_i) # Initial: just element type v_i^(0) = 0 # Initial: no directional info Message Passing Layers: (s_i, v_i) \u2192 MessagePass \u2192 (s_i', v_i') Output: Scalar: energy = \u03a3_i MLP(s_i^(T)) Vector: forces = \u03a3_i v_i^(T) # Or per-atom force contribution Message Passing in PaiNN: Each layer consists of three parts: Part 1: Scalar Message Passing # Compute scalar messages from neighbors for j in neighbors ( i ): d_ij = || r_j - r_i || # Distance dir_ij = ( r_j - r_i ) / d_ij # Unit direction vector # Filter based on distance \u03c6_ij = FilterNetwork ( d_ij ) # Like SchNet # Scalar message (rotation invariant) m_s_ij = \u03c6_ij \u2299 s_j # Also incorporate magnitude of vector features m_s_ij += \u03c6_ij \u2299 || v_j || \u00b2 # Invariant: length squared # Aggregate m_s_i = \u03a3_j m_s_ij # Update scalars s_i = s_i + MLP ( m_s_i ) Part 2: Vector Message Passing This is where equivariance happens: # Compute vector messages for j in neighbors ( i ): d_ij = || r_j - r_i || dir_ij = ( r_j - r_i ) / d_ij # \u2190 KEY: Direction vector \u03c6_ij = FilterNetwork ( d_ij ) # Vector message (rotation equivariant!) # Multiply by direction to make equivariant m_v_ij = \u03c6_ij \u2299 v_j # Element-wise filter m_v_ij += ( \u03c6_ij \u2299 s_j ) \u00d7 dir_ij # Scalar-to-vector term # Aggregate vectors m_v_i = \u03a3_j m_v_ij # Update vectors v_i = v_i + m_v_i Why this is equivariant: Under rotation Q: dir_ij \u2192 Q dir_ij (direction rotates) v_j \u2192 Q v_j (vector features rotate) m_v_ij = \u03c6_ij \u2299 v_j + (\u03c6_ij \u2299 s_j) \u00d7 dir_ij \u2192 \u03c6_ij \u2299 (Q v_j) + (\u03c6_ij \u2299 s_j) \u00d7 (Q dir_ij) = Q [\u03c6_ij \u2299 v_j + (\u03c6_ij \u2299 s_j) \u00d7 dir_ij] = Q m_v_ij \u2713 Part 3: Mixing Scalars and Vectors Cross-interactions between scalar and vector features: # Vector \u2192 Scalar: Extract invariant info from vectors s_i = s_i + MLP ( || v_i || ) # Length is invariant s_i = s_i + MLP ( v_i \u00b7 v_i ) # Dot product is invariant # Scalar \u2192 Vector: Modulate vectors by scalars v_i = v_i \u2299 \u03c3 ( U s_i ) # Element-wise gating Complete Update Equations: # Scalar update \u0394s_i = \u03a3_j [W_s(d_ij) \u2299 s_j + W_vs(d_ij) \u2299 ||v_j||\u00b2] s_i = s_i + MLP(\u0394s_i + ||v_i||\u00b2) # Vector update \u0394v_i = \u03a3_j [W_v(d_ij) \u2299 v_j + W_sv(d_ij) \u2299 s_j \u2299 (r_j - r_i) / d_ij] v_i = v_i + \u0394v_i # Mix scalar and vector s_i = s_i + U_vs ||v_i|| v_i = (W_vv v_i) \u2299 \u03c3(U_sv s_i) Output Predictions: # Energy (scalar invariant) E_i = MLP_scalar ( s_i ) E_total = \u03a3_i E_i # Forces (vector equivariant) F_i = v_i # Already in correct format! # Or: F_i = Linear(v_i) for learned scaling # Other vector properties dipole = \u03a3_i q_i \u00d7 v_i # If q_i are charges Advantages of PaiNN: Direct vector prediction : Forces without numerical differentiation More accurate and efficient Can predict multiple vector properties True equivariance : Guarantees physical consistency Rotated inputs \u2192 correctly rotated outputs No violation of physics Expressive representations : Vectors encode directional information Richer than scalar-only features Better for anisotropic systems Force field learning : Can be trained on forces directly Learns better potential energy surfaces Useful for molecular dynamics Training Considerations: # Loss function combining energy and forces loss = w_E || E_pred - E_true || \u00b2 + w_F || F_pred - F_true || \u00b2 # Typical weights w_E = 1.0 # Energy in eV or kcal/mol w_F = 100.0 # Forces in eV/\u00c5 (forces are smaller, need higher weight) Applications: Molecular Dynamics : Learn accurate force fields from DFT 1000\u00d7 faster than ab initio MD Maintains accuracy of quantum calculations Transition State Search : Accurate forces guide optimization Find saddle points efficiently Predict reaction barriers Dipole Moment Prediction : Important for spectroscopy Drug-like properties Solvent effects Polarizability : Response to external fields Optical properties Intermolecular interactions Implementation Notes: # Hyperparameters num_layers = 5 hidden_dim_scalar = 128 hidden_dim_vector = 64 # Vectors have 3\u00d7 more parameters (x,y,z) num_rbf = 20 cutoff = 5.0 # Vector features typically smaller dimension to save memory # v_i \u2208 \u211d^(d\u00d73) uses 3\u00d7 memory of s_i \u2208 \u211d^d Comparison of Approaches \u00b6 Model Distance Angles Equivariance Outputs Complexity Use Case SchNet \u2713 \u2717 Invariant Scalars O(N\u00d7k) Fast, general purpose, good baseline DimeNet \u2713 \u2713 Invariant Scalars O(N\u00d7k\u00b2) High accuracy, angle-dependent properties DimeNet++ \u2713 \u2713 Invariant Scalars O(N\u00d7k\u00b2)* DimeNet with 3-5\u00d7 speedup PaiNN \u2713 Implicit Equivariant Scalars + Vectors O(N\u00d7k) Forces, vector properties, MD *DimeNet++ optimized but same computational complexity When to Choose Each: SchNet: - Need fast inference - Large molecules (>100 atoms) - Don\u2019t need highest accuracy - Conformational search (many evaluations) - X Highly angle-dependent properties - X Need force predictions DimeNet/DimeNet++: - Need highest accuracy - Angle and dihedral effects important - Small to medium molecules (<50 atoms) - Property prediction only - X Computational budget limited - X Need force predictions - X Very large molecules PaiNN: - Need force predictions - Molecular dynamics simulations - Vector property prediction - Want equivariant representations - Good accuracy/speed trade-off - X Only need scalar properties - X Maximum simplicity desired Practical Decision Tree: Do you need vector outputs (forces, dipoles)? \u251c\u2500 Yes \u2192 Use PaiNN \u2514\u2500 No \u2502 \u2514\u2500 Is accuracy critical and dataset small? \u251c\u2500 Yes \u2192 Use DimeNet++ \u2514\u2500 No \u2192 Use SchNet (fastest) Benchmarks (QM9 dataset, HOMO energy): Method MAE (meV) Time/molecule Parameters SchNet 41 0.5 ms 600K DimeNet 33 3.0 ms 2M DimeNet++ 29 1.2 ms 2M PaiNN 35 0.8 ms 800K Note: Exact numbers vary by implementation and hardware 4. Protein-Ligand Interaction Modeling \u00b6 Understanding how small molecules (ligands) bind to proteins is crucial for drug discovery. GNNs provide powerful tools for modeling these complex interactions, potentially accelerating the drug development pipeline from years to months. The Drug Discovery Challenge \u00b6 Traditional Drug Discovery: Target Identification : Identify disease-related protein (months-years) Hit Discovery : Screen 10\u2075-10\u2076 compounds experimentally (months, $$$) Lead Optimization : Iteratively improve binding (years, $$$$) Clinical Trials : Test in humans (years, $$$$$) Total : 10-15 years, $1-2 billion per drug AI-Accelerated Discovery: GNNs can predict binding without experiments: - Virtual screening: 10\u2076 compounds in hours - Structure-based optimization - Reduced experimental testing - Faster iteration cycles Impact : Several AI-discovered drugs now in clinical trials Problem Formulation \u00b6 Key Tasks: Binding Affinity Prediction : Predict the strength of protein-ligand binding Metrics: IC\u2085\u2080, Ki, Kd, \u0394G_bind Range: nM (strong) to mM (weak) Applications: Virtual screening, lead optimization Binding Pose Prediction : Determine the 3D orientation of ligand in binding pocket Must satisfy spatial constraints Account for protein flexibility Applications: Structure-based drug design Virtual Screening : Rank large libraries of compounds Prioritize for experimental testing Requires fast inference (<1s per compound) Applications: Hit discovery, library filtering Selectivity Prediction : Binding to target vs off-targets Crucial for drug safety Multi-protein modeling Applications: Toxicity prediction, side effect profiling Input Data: Protein: - Sequence: MKTAYIAKQRQ... (amino acid sequence) - Structure: 3D coordinates of atoms or residues - Features: Secondary structure, surface accessibility, physicochemical properties Ligand: - SMILES: CC(C)CC1=CC=C(C=C1)C(C)C(=O)O (structure string) - 3D Conformation: Atomic coordinates - Features: Atom types, charges, pharmacophore points Complex: - Binding pose (if known from X-ray crystallography) - Interaction types (H-bonds, hydrophobic, \u03c0-stacking) Representation Strategies \u00b6 Challenge : How to represent a protein-ligand complex as a graph? Strategy 1: Separate Protein and Ligand Graphs [Protein Graph] [Ligand Graph] Nodes: Residues Nodes: Atoms Edges: Contacts Edges: Bonds \u2193 \u2193 [GNN_prot] [GNN_lig] \u2193 \u2193 h_protein h_ligand \u2514\u2500\u2500\u2500\u2500\u2500\u2192 [Concatenate] \u2190\u2500\u2500\u2500\u2500\u2500\u2518 \u2193 [MLP head] \u2193 Affinity Protein Graph Construction: # Option A: Residue-level (coarse-grained) for residue in protein . residues : node_features = [ residue . amino_acid_type , # One-hot: 20 amino acids residue . secondary_structure , # Helix, sheet, coil residue . surface_accessibility , residue . charge , residue . hydrophobicity ] # Edges: spatial proximity for i , j in combinations ( residues , 2 ): if distance ( i . CA , j . CA ) < 10.0 : # C-alpha distance add_edge ( i , j , distance = distance ( i . CA , j . CA )) # Option B: Atom-level (fine-grained) for atom in protein . atoms : node_features = [ atom . element , atom . charge , atom . in_backbone , atom . in_sidechain ] Ligand Graph Construction: # Atoms as nodes for atom in ligand . atoms : node_features = [ atom . atomic_number , atom . formal_charge , atom . hybridization , # sp, sp2, sp3 atom . is_aromatic , atom . num_hydrogens , atom . degree , atom . chirality ] # Bonds as edges for bond in ligand . bonds : edge_features = [ bond . bond_type , # Single, double, triple, aromatic bond . is_conjugated , bond . is_in_ring , bond . stereo # E/Z, cis/trans ] Pros: - Simple architecture - Can pre-train on protein/ligand datasets separately - Modular: easy to swap GNN architectures Cons: - No explicit inter-molecular interactions - Late fusion may miss important binding details - Less interpretable (black box combination) Strategy 2: Joint Protein-Ligand Interaction Graph Protein nodes + Ligand nodes \u2193 Intra-molecular edges (protein bonds, ligand bonds) + Inter-molecular edges (binding interactions) \u2193 [Joint GNN] \u2193 [Global pooling] \u2193 Affinity Interaction Graph Construction: # Combine protein and ligand into one graph G = Graph () # Add protein nodes G . add_nodes ( protein_atoms , type = 'protein' ) # Add ligand nodes G . add_nodes ( ligand_atoms , type = 'ligand' ) # Add intra-molecular edges G . add_edges ( protein_bonds ) G . add_edges ( ligand_bonds ) # Add inter-molecular edges (KEY!) for p_atom in protein_atoms : for l_atom in ligand_atoms : dist = distance ( p_atom , l_atom ) if dist < 5.0 : # Interaction cutoff interaction_type = classify_interaction ( p_atom , l_atom , dist ) G . add_edge ( p_atom , l_atom , distance = dist , interaction = interaction_type ) Interaction Types: Classify inter-molecular edges by interaction: def classify_interaction ( p_atom , l_atom , distance ): interactions = [] # Hydrogen bond if is_h_bond_donor ( p_atom ) and is_h_bond_acceptor ( l_atom ): if distance < 3.5 and angle_ok : interactions . append ( 'H-bond' ) # Hydrophobic if is_hydrophobic ( p_atom ) and is_hydrophobic ( l_atom ): if distance < 4.5 : interactions . append ( 'hydrophobic' ) # Pi-stacking if is_aromatic ( p_atom ) and is_aromatic ( l_atom ): if 3.5 < distance < 4.5 : interactions . append ( 'pi-stacking' ) # Electrostatic if charge ( p_atom ) * charge ( l_atom ) < 0 : # Opposite charges interactions . append ( 'electrostatic' ) # Salt bridge if is_charged_residue ( p_atom ) and is_charged_group ( l_atom ): if distance < 4.0 : interactions . append ( 'salt-bridge' ) return interactions Pros: - Explicit interaction modeling - Message passing directly between protein and ligand - More interpretable (can visualize key interactions) - Better captures binding geometry Cons: - Larger graphs (more nodes and edges) - Requires 3D structure (binding pose) - More complex to implement Strategy 3: Attention-Based Cross-Attention [Protein GNN] \u2192 h_protein_nodes [Ligand GNN] \u2192 h_ligand_nodes \u2193 \u2193 [Cross-Attention Layer] \u2193 Attended features \u2193 [Prediction head] Cross-Attention Mechanism: # Protein attends to ligand for p_node in protein_nodes : # Compute attention to all ligand nodes attention = [] for l_node in ligand_nodes : # Attention score based on features and geometry score = attention_function ( h_protein [ p_node ], h_ligand [ l_node ], distance ( p_node , l_node ) ) attention . append ( score ) # Softmax normalize attention = softmax ( attention ) # Attended ligand features h_protein [ p_node ] += weighted_sum ( attention , h_ligand ) # Ligand attends to protein (symmetric) for l_node in ligand_nodes : # Similar process in reverse ... Attention Function: def attention_function ( h_p , h_l , distance ): # Feature similarity feat_sim = dot_product ( W_p @ h_p , W_l @ h_l ) # Distance penalty (closer = more attention) dist_weight = exp ( - distance / sigma ) # Combined score score = feat_sim * dist_weight return score Pros: - Learns which protein-ligand pairs interact - Flexible: works without predefined interaction edges - Can handle multiple binding modes - Interpretable attention weights Cons: - O(N_protein \u00d7 N_ligand) complexity - May overfit on small datasets - Requires careful regularization Architecture Patterns \u00b6 Pattern 1: Separate Encoding with Late Fusion class SeparateFusionModel ( nn . Module ): def __init__ ( self ): self . protein_gnn = GNN ( protein_features , hidden_dim ) self . ligand_gnn = GNN ( ligand_features , hidden_dim ) self . fusion_mlp = MLP ( 2 * hidden_dim , output_dim ) def forward ( self , protein_graph , ligand_graph ): # Encode separately h_prot = self . protein_gnn ( protein_graph ) h_lig = self . ligand_gnn ( ligand_graph ) # Global pooling z_prot = global_mean_pool ( h_prot , protein_graph . batch ) z_lig = global_mean_pool ( h_lig , ligand_graph . batch ) # Concatenate and predict z = torch . cat ([ z_prot , z_lig ], dim =- 1 ) affinity = self . fusion_mlp ( z ) return affinity Pattern 2: Joint Encoding class JointGraphModel ( nn . Module ): def __init__ ( self ): self . gnn = GNN ( node_features , hidden_dim , num_layers = 5 ) self . interaction_embedding = nn . Embedding ( num_interactions , edge_dim ) self . readout = Set2Set ( hidden_dim ) self . predictor = MLP ( hidden_dim , 1 ) def forward ( self , complex_graph ): # Embed interactions edge_attr = self . interaction_embedding ( complex_graph . edge_type ) # Joint message passing h = self . gnn ( complex_graph . x , complex_graph . edge_index , edge_attr ) # Global pooling z = self . readout ( h , complex_graph . batch ) # Predict affinity affinity = self . predictor ( z ) return affinity Pattern 3: Cross-Attention class CrossAttentionModel ( nn . Module ): def __init__ ( self ): self . protein_encoder = GNN ( protein_features , hidden_dim ) self . ligand_encoder = GNN ( ligand_features , hidden_dim ) self . cross_attention = CrossAttentionLayer ( hidden_dim ) self . predictor = MLP ( hidden_dim , 1 ) def forward ( self , protein_graph , ligand_graph , distances ): # Encode separately h_prot = self . protein_encoder ( protein_graph ) h_lig = self . ligand_encoder ( ligand_graph ) # Cross-attention h_prot_updated , h_lig_updated = self . cross_attention ( h_prot , h_lig , distances ) # Pool both z_prot = global_attention_pool ( h_prot_updated ) z_lig = global_attention_pool ( h_lig_updated ) # Predict from combined representation affinity = self . predictor ( z_prot + z_lig ) return affinity Key Considerations \u00b6 1. Geometric Information 3D coordinates are essential for accurate binding prediction: # Distance features (crucial!) edge_features = [] for edge in edges : i , j = edge d = distance ( coords [ i ], coords [ j ]) # Distance encoding rbf = gaussian_rbf ( d , centers = [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]) edge_features . append ( rbf ) # Direction (for equivariant models) direction = ( coords [ j ] - coords [ i ]) / d # Use in GNN h = GNN ( nodes , edges , edge_features ) Why geometry matters: - Binding pocket shape determines complementarity - Hydrogen bonds have geometric constraints (distance + angle) - Hydrophobic interactions are distance-dependent - Steric clashes prevent binding 2. Data Challenges Limited Experimental Data: Available binding data: - PDBbind: ~20,000 protein-ligand complexes - ChEMBL: ~2M bioactivity measurements (but many without structures) - BindingDB: ~2M Ki/Kd values Compare to: - ImageNet: 14M images - GPT training: trillions of tokens Challenge: Deep learning typically needs more data Solutions: a) Data Augmentation: # Rotation augmentation for angle in [ 0 , 90 , 180 , 270 ]: rotated_complex = rotate ( complex , angle ) train_on ( rotated_complex ) # Conformational sampling for conf in generate_conformations ( ligand , n = 10 ): augmented_complex = ( protein , conf ) train_on ( augmented_complex ) # Noise injection for noise_level in [ 0.1 , 0.2 ]: noisy_coords = coords + noise_level * random_normal () train_on ( noisy_coords ) b) Transfer Learning: # Pre-train on easier tasks model . pretrain ( task = 'molecular_property_prediction' , dataset = 'QM9' , # Millions of molecules epochs = 100 ) # Fine-tune on binding model . finetune ( task = 'binding_affinity' , dataset = 'PDBbind' , # Thousands of complexes epochs = 50 , learning_rate = 1e-4 # Lower learning rate ) c) Multi-Task Learning: # Learn related tasks simultaneously loss = ( w1 * loss_binding_affinity + w2 * loss_binding_pose + w3 * loss_protein_function + w4 * loss_ligand_properties ) # Shared representations benefit all tasks # More efficient use of limited data Missing Structures: Many bioactivity measurements lack 3D structures: # Sequence-only prediction (when no structure) if protein_structure is None : # Use sequence-based protein representation h_prot = ProteinLanguageModel ( protein_sequence ) else : # Use structure-based GNN h_prot = ProteinGNN ( protein_structure ) 3. Interpretability Understanding WHY a compound binds is as important as predicting IF it binds: Attention Visualization: # Extract attention weights attentions = model . get_attention_weights () # Identify key protein residues important_residues = [] for residue , attention in zip ( residues , attentions ): if attention > threshold : important_residues . append ( residue ) # Visualize in 3D visualize_protein_ligand ( protein , ligand , highlight_residues = important_residues , highlight_atoms = high_attention_ligand_atoms ) Interaction Decomposition: # Analyze contribution of each interaction type for interaction_type in [ 'H-bond' , 'hydrophobic' , 'pi-stack' ]: # Ablation: remove this interaction type affinity_without = model . predict ( complex , exclude_interaction = interaction_type ) contribution = affinity_full - affinity_without print ( f \" { interaction_type } : { contribution : .2f } kcal/mol\" ) Gradient-Based Explanations: # Which atoms matter most? ligand . requires_grad = True affinity = model ( protein , ligand ) affinity . backward () # Atoms with large gradients are important importance = ligand . grad . norm ( dim =- 1 ) visualize_atom_importance ( ligand , importance ) Applications: SAR (Structure-Activity Relationship) : \u201cThis hydrogen bond donor is crucial\u201d \u201cHydrophobic tail can be modified\u201d Guides medicinal chemistry Failure Analysis : \u201cModel focuses on wrong pocket\u201d \u201cMissed key water-mediated H-bond\u201d Improves model training Knowledge Discovery : Identify novel binding motifs Understand selectivity patterns Generate hypotheses for experiments State-of-the-Art Models \u00b6 EquiBind (2021) Key Innovation: SE(3)-equivariant blind docking Architecture: - Separate protein and ligand encoders - Equivariant graph neural network (EGNN) - Predicts keypoint matches - Direct coordinate prediction (no search) Performance: - 38% success rate (<2\u00c5 RMSD) on PDBbind - 1000\u00d7 faster than traditional docking - Fully differentiable GraphDTA / DeepDTA (2018) Key Innovation: End-to-end learning from sequences Architecture: - CNN for protein sequences - GNN for ligand graphs - Concatenation + MLP - Trained on drug-target affinity Performance: - Competitive on Davis and KIBA datasets - Works without 3D structures - Fast inference for virtual screening ATOM3D (2021) Key Innovation: 3D structure benchmarks Datasets: - Protein-ligand binding (PDB) - Protein structure prediction - RNA structure - Molecular dynamics Models: - SchNet, DimeNet applied to biomolecules - Transformers with 3D positional encoding - Graph transformers TANKBind (2023) Key Innovation: Equivariant blind docking with diffusion Architecture: - Diffusion model for pose generation - Equivariant score matching - Iterative refinement - Confidence estimation Performance: - State-of-the-art on blind docking - Handles protein flexibility - Generalizes to unseen proteins Practical Workflow \u00b6 # 1. Data preparation from biopandas.pdb import PandasPdb # Load protein protein = PandasPdb () . fetch_pdb ( '1a2b' ) protein_graph = protein_to_graph ( protein ) # Load ligand ligand = Chem . MolFromMol2File ( 'ligand.mol2' ) ligand_graph = mol_to_graph ( ligand ) # 2. Model training model = ProteinLigandGNN ( protein_features = 37 , ligand_features = 11 , hidden_dim = 128 , num_layers = 4 ) optimizer = torch . optim . Adam ( model . parameters (), lr = 1e-3 ) for epoch in range ( 100 ): for protein_graph , ligand_graph , affinity in dataloader : pred_affinity = model ( protein_graph , ligand_graph ) loss = F . mse_loss ( pred_affinity , affinity ) optimizer . zero_grad () loss . backward () optimizer . step () # 3. Virtual screening candidates = load_library ( 'drugbank.sdf' ) # 10,000 compounds predictions = [] for ligand in tqdm ( candidates ): ligand_graph = mol_to_graph ( ligand ) affinity = model ( protein_graph , ligand_graph ) predictions . append (( ligand , affinity )) # Sort by predicted affinity predictions . sort ( key = lambda x : x [ 1 ], reverse = True ) # Test top 100 experimentally top_hits = predictions [: 100 ] Future Directions \u00b6 Physics-Informed Neural Networks : Incorporate electrostatics, solvation Constrain predictions with physical laws Hybrid quantum/classical approaches Generative Models : Generate ligands for target protein Optimize for multiple objectives De novo drug design Allostery and Dynamics : Model protein conformational changes Long-range allosteric effects Molecular dynamics integration Multi-Target Modeling : Selectivity across protein families Polypharmacology Side effect prediction 5. Crystal Structures for Materials Science \u00b6 GNNs are revolutionizing materials science by predicting properties of crystalline materials from their atomic structures. This enables high-throughput screening of millions of hypothetical materials, dramatically accelerating materials discovery. Understanding Crystalline Materials \u00b6 What Makes Crystals Different? Unlike molecules, crystals are: - Infinite : Periodic repetition in 3D space - Ordered : Atoms arranged in regular lattices - Defined by symmetry : Space groups and point groups - Bulk properties : Properties of the infinite system, not just a cluster Real-World Impact: Materials with specific properties enable technology: - Batteries : Li-ion conductors (electric vehicles) - Solar cells : High-efficiency photovoltaics - Catalysts : Green chemical production - Semiconductors : Computing and electronics - Superconductors : Lossless power transmission The Discovery Challenge: Possible stable materials: ~10^50 (combinatorial explosion!) Known materials: ~200,000 (Materials Project, ICSD) Fully characterized: ~50,000 Currently used: ~10,000 DFT calculation: 1-1000 CPU-hours per material ML prediction: 0.001 seconds per material Speed-up: 10^6\u00d7 faster! Crystal Representation \u00b6 Unit Cell Description: A crystal is fully specified by: Lattice Vectors : Define the unit cell a\u20d7 = [a_x, a_y, a_z] # First lattice vector b\u20d7 = [b_x, b_y, b_z] # Second lattice vector c\u20d7 = [c_x, c_y, c_z] # Third lattice vector Lattice matrix: L = [a\u20d7 | b\u20d7 | c\u20d7] Lattice Parameters : a, b, c = lengths of vectors (\u00c5ngstr\u00f6ms) \u03b1, \u03b2, \u03b3 = angles between vectors (degrees) Basis : Atomic positions within the unit cell Fractional coordinates: (u, v, w) where 0 \u2264 u,v,w < 1 Cartesian coordinates: r\u20d7 = u\u00b7a\u20d7 + v\u00b7b\u20d7 + w\u00b7c\u20d7 Space Group : Symmetry operations 230 possible space groups in 3D Examples: P21/c (monoclinic), Fm3\u0304m (cubic), P63/mmc (hexagonal) Example: Diamond (Carbon) Lattice: Face-centered cubic (FCC) a = b = c = 3.567 \u00c5 \u03b1 = \u03b2 = \u03b3 = 90\u00b0 Basis: Two carbon atoms at C\u2081: (0, 0, 0) C\u2082: (0.25, 0.25, 0.25) Space group: Fd3\u0304m (227) Infinite crystal: All positions (n\u2081, n\u2082, n\u2083) + basis where n\u2081, n\u2082, n\u2083 \u2208 \u2124 Periodic Boundary Conditions \u00b6 The Periodicity Challenge: For graph construction, we need neighbors, but: - Atoms near cell boundaries have neighbors in adjacent cells - Must account for periodic images - Same atom appears infinitely many times Graph Construction Algorithm: def construct_crystal_graph ( atoms , lattice , cutoff_radius ): \"\"\" Build graph respecting periodic boundaries \"\"\" graph = Graph () # Add nodes for atoms in unit cell for atom in atoms : graph . add_node ( element = atom . element , position = atom . frac_coords , # Fractional coordinates features = get_atom_features ( atom ) ) # Find neighbors using minimum image convention for i , atom_i in enumerate ( atoms ): for j , atom_j in enumerate ( atoms ): # Check all periodic images of atom_j for n1 in [ - 1 , 0 , 1 ]: for n2 in [ - 1 , 0 , 1 ]: for n3 in [ - 1 , 0 , 1 ]: # Skip self-loops (unless different cells) if i == j and ( n1 , n2 , n3 ) == ( 0 , 0 , 0 ): continue # Compute distance through periodic boundaries image_position = atom_j . frac_coords + [ n1 , n2 , n3 ] cart_position = lattice . to_cartesian ( image_position ) distance = np . linalg . norm ( cart_position - lattice . to_cartesian ( atom_i . frac_coords ) ) if distance < cutoff_radius : graph . add_edge ( i , j , distance = distance , cell_offset = ( n1 , n2 , n3 ), # Which periodic image direction = cart_position - atom_i . cart_coords ) return graph Minimum Image Convention: For each pair of atoms, consider all periodic images Choose the closest one (minimum distance) Example: 1D crystal with cell size 10 \u00c5 Atom A at x=1 Atom B at x=9 Direct distance: |9-1| = 8 \u00c5 Through boundary: |9-1-10| = 2 \u00c5 \u2190 MINIMUM (use this!) This handles wrapped distances correctly Challenges: Variable coordination : Atoms may have different numbers of neighbors Long-range order : Some properties depend on distant atoms Supercell construction : May need to replicate unit cell for larger cutoffs Property Prediction Tasks \u00b6 Electronic Properties These determine conducting and optical behavior: 1. Band Gap (E_g) Definition: Energy difference between valence and conduction bands Units: eV Range: 0 (metal) to >10 eV (insulator) Applications: - E_g \u2248 1.3 eV: Solar cells (optimal for sunlight) - E_g > 5 eV: Transparent insulators (windows) - E_g = 0: Metals (wires, electrodes) Prediction challenge: - DFT often underestimates (by 30-50%) - GNNs can learn correction factors 2. Formation Energy (E_f) Definition: Energy to form compound from elements Formula: E_f = E_compound - \u03a3_i n_i \u00d7 E_element(i) Units: eV/atom Applications: - E_f < 0: Thermodynamically stable - E_f > 0.1 eV/atom: Likely unstable - Guides synthesis feasibility Prediction: Critical for materials discovery 3. Energy Above Hull (E_hull) Definition: Energy above stable composition convex hull Measures: Thermodynamic stability relative to competing phases E_hull = 0: On convex hull (thermodynamically stable) E_hull < 0.025 eV/atom: Potentially synthesizable E_hull > 0.1 eV/atom: Very unlikely to be stable Applications: - Virtual materials screening - Stability prediction before synthesis Mechanical Properties Determine material strength and elasticity: 1. Bulk Modulus (B) Definition: Resistance to uniform compression Formula: B = -V (\u2202P/\u2202V)_T Units: GPa Range: 1 GPa (soft) to 400 GPa (diamond) Applications: - High B: Armor, cutting tools - Low B: Flexible substrates, cushioning 2. Shear Modulus (G) Definition: Resistance to shear deformation Units: GPa Applications: - G/B ratio indicates ductility - High G: Stiff materials - Critical for structural applications 3. Elastic Constants (C_ij) Tensor: 6\u00d76 matrix (21 independent components for general case) Symmetry: Reduces components based on crystal system - Cubic: 3 independent constants - Hexagonal: 5 constants - Triclinic: 21 constants Applications: - Full mechanical characterization - Anisotropic behavior prediction Thermodynamic Properties 1. Phonon Properties Frequency spectrum: Vibrational modes Heat capacity: C_v(T) from phonons Thermal expansion: \u03b1(T) Thermal conductivity: \u03ba Challenge: Requires dynamical matrix (expensive!) GNN potential: Fast phonon calculations 2. Free Energy Temperature-dependent stability Phase transitions Chemical potential Applications: - Phase diagram prediction - High-temperature materials Specialized Architectures \u00b6 CGCNN (Crystal Graph Convolutional Neural Networks) One of the first GNN architectures specifically designed for crystals. Architecture: class CGCNNConv ( nn . Module ): \"\"\"CGCNN convolution layer\"\"\" def __init__ ( self , node_features , edge_features , hidden ): self . node_fc = nn . Linear ( 2 * node_features + edge_features , hidden ) self . bn = nn . BatchNorm1d ( hidden ) def forward ( self , x , edge_index , edge_attr ): # For each edge (i \u2192 j) i , j = edge_index # Concatenate: [h_i || h_j || e_ij] messages = torch . cat ([ x [ i ], x [ j ], edge_attr ], dim =- 1 ) # Transform and normalize messages = self . bn ( self . node_fc ( messages )) messages = F . softplus ( messages ) # Aggregate to each node # Sum over all incoming edges x_new = scatter_add ( messages , i , dim = 0 ) return x + x_new # Residual connection Key Features: Distance-based edge weights : def edge_features ( distance , cutoff = 8.0 ): # Gaussian distance encoding centers = torch . linspace ( 0 , cutoff , 100 ) gamma = ( centers [ 1 ] - centers [ 0 ]) return torch . exp ( - gamma * ( distance - centers ) ** 2 ) Pooling for crystal-level properties : # Average over all atoms h_crystal = global_mean_pool ( h_atoms , batch ) # Or weighted by composition h_crystal = \u03a3_i ( n_i / N_total ) \u00d7 h_i Handling variable composition : # Works for any stoichiometry # Na\u2082Cl\u2082: 2 Na nodes, 2 Cl nodes # Na\u2081\u2080Cl\u2081\u2080: 10 Na nodes, 10 Cl nodes # Same model, different graph sizes Training: # Dataset: Materials Project dataset = CrystalDataset ( 'materials_project' ) model = CGCNN ( atom_features = 92 , # One-hot: 92 elements edge_features = 100 , # Gaussian RBFs hidden_dim = 128 , num_layers = 4 ) # Predict formation energy for crystal_graph in dataloader : pred_energy = model ( crystal_graph ) loss = F . mse_loss ( pred_energy , crystal_graph . y ) Performance: - MAE \u2248 0.04 eV/atom for formation energy - Handles diverse chemistries (metals, semiconductors, insulators) - Fast: 1000\u00d7 faster than DFT MEGNet (MatErials Graph Network) Multi-level graph network with atom, bond, and global state. Three-Level Architecture: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Global State\u2502 (lattice, volume, composition) \u2502 g \u2208 \u211d\u1d48 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Atom States\u2502 \u2502 Bond States \u2502 \u2502 v \u2208 \u211d\u1d48 \u2502\u25c4\u2500\u2500\u25ba\u2502 e \u2208 \u211d\u1d48 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Update Rules: # Bond update: use atoms + global e_ij ' = \u03c6_e([e_ij || v_i || v_j || g]) # Atom update: aggregate bonds + global v_i ' = \u03c6_v([v_i || \u03a3_j e_ij' || g ]) # Global update: aggregate atoms + bonds g ' = \u03c6_g([g || \u03a3_i v_i' || \u03a3_ij e_ij ']) Advantages: Multi-scale information : Local: Atom and bond features Global: Crystal-level properties (volume, space group) Richer representations : Bonds have their own learned features Global state captures extensive properties Better for complex properties : Properties that depend on global structure Example: Thermal conductivity (collective behavior) Applications: # Multi-task learning model = MEGNet ( tasks = [ 'formation_energy' , 'band_gap' , 'bulk_modulus' , 'shear_modulus' ]) # Shared encoder, separate heads h_shared = model . encode ( crystal ) E_f = model . head_energy ( h_shared ) E_g = model . head_gap ( h_shared ) B = model . head_bulk ( h_shared ) G = model . head_shear ( h_shared ) SchNet for Crystals Adapting continuous filters for periodic systems. Modifications for Periodicity: class SchNetCrystal ( nn . Module ): def __init__ ( self ): self . rbf_expansion = GaussianRBF ( cutoff = 5.0 ) self . interaction_blocks = nn . ModuleList ([ InteractionBlock () for _ in range ( 6 ) ]) def forward ( self , crystal ): # Handle periodic images distances , cell_offsets = get_neighbor_distances ( crystal . frac_coords , crystal . lattice , cutoff = 5.0 ) # Distance features (same as molecular SchNet) edge_features = self . rbf_expansion ( distances ) # Message passing with periodic edges h = self . embed ( crystal . atomic_numbers ) for block in self . interaction_blocks : h = block ( h , crystal . edge_index , edge_features ) # Crystal-level pooling return global_mean_pool ( h , crystal . batch ) Handling Variable Unit Cell Size: # Challenge: Different crystals have different numbers of atoms # Solution: Normalization # Per-atom properties \u2192 extensive energy_per_atom = total_energy / num_atoms # Or use composition-weighted features composition = get_composition ( crystal ) # {'Na': 2, 'Cl': 2} weights = [ composition [ atom ] / sum ( composition . values ()) for atom in atoms ] h_crystal = \u03a3_i weights [ i ] \u00d7 h_i Allegro / NequIP State-of-the-art equivariant models for materials. E(3)-Equivariant Architecture: class E3NN_Crystal ( nn . Module ): \"\"\"Based on e3nn library\"\"\" def __init__ ( self ): # Irreducible representations (irreps) # l=0: scalars (rotation invariant) # l=1: vectors (rotation equivariant) # l=2: rank-2 tensors self . irreps_in = \"92x0e\" # 92 elements, scalar features self . irreps_hidden = \"128x0e + 64x1o + 32x2e\" # Mixed irreps self . irreps_out = \"1x0e\" # Energy (scalar) self . convolution = E3Convolution ( self . irreps_in , self . irreps_hidden ) def forward ( self , crystal ): # Features transform correctly under rotations # Scalars: unchanged # Vectors: rotate with crystal # Tensors: rotate as rank-2 tensors ... Applications: Force Field Learning : # Train on energy and forces energy_pred = model ( crystal ) forces_pred = - autograd . grad ( energy_pred , crystal . coords ) loss = ( w_E * ( energy_pred - energy_true ) ** 2 + w_F * ( forces_pred - forces_true ) ** 2 ) Molecular Dynamics : Learned potential: 1000-10000\u00d7 faster than DFT Accuracy: Near DFT quality Application: Simulate nanoseconds to microseconds Stress Tensor Prediction : # Stress tensor: rank-2 equivariant quantity stress = model . predict_stress ( crystal ) # Transforms as: \u03c3 \u2192 Q \u03c3 Q^T under rotation Q Handling Periodicity: Technical Details \u00b6 Minimum Image Convention in Practice: def minimum_image_distance ( frac_coords_i , frac_coords_j , lattice ): \"\"\" Compute minimum distance through periodic boundaries \"\"\" # Difference in fractional coordinates d_frac = frac_coords_j - frac_coords_i # Wrap to [-0.5, 0.5] (minimum image) d_frac = d_frac - np . round ( d_frac ) # Convert to Cartesian d_cart = lattice @ d_frac # Distance return np . linalg . norm ( d_cart ) Edge Attributes for Periodic Systems: edge_attr = { 'distance' : d_ij , 'direction' : ( r_j - r_i ) / d_ij , # Unit vector 'cell_offset' : ( n1 , n2 , n3 ), # Which periodic image 'is_boundary' : ( n1 != 0 or n2 != 0 or n3 != 0 ) } Lattice as Global Feature: # Include lattice parameters as global features lattice_features = torch . tensor ([ a , b , c , # Lengths alpha , beta , gamma , # Angles volume , # Cell volume np . log ( volume ), # Log volume (more uniform distribution) ]) # Broadcast to all atoms for atom in crystal : atom . features = torch . cat ([ atom . features , lattice_features ]) Applications \u00b6 Materials Discovery Workflow: # 1. Generate candidate structures from pymatgen.io.cif import CifWriter from pymatgen.core import Structure candidates = [] for composition in element_combinations : for prototype in common_structures : structure = substitute ( prototype , composition ) if is_reasonable ( structure ): # Basic filters candidates . append ( structure ) print ( f \"Generated { len ( candidates ) } candidates\" ) # ~10\u2076 # 2. Screen with GNN model = load_model ( 'megnet_bandgap.pt' ) promising = [] for structure in tqdm ( candidates ): graph = structure_to_graph ( structure ) E_g = model . predict ( graph ) if 1.0 < E_g < 1.5 : # Target range for solar cells promising . append (( structure , E_g )) print ( f \"Found { len ( promising ) } promising candidates\" ) # ~10\u00b3 # 3. Refine with DFT for structure , E_g_predicted in promising [: 100 ]: # Top 100 E_g_dft = run_dft ( structure ) # Expensive but accurate if abs ( E_g_dft - E_g_predicted ) < 0.2 : # GNN was accurate! save_for_synthesis ( structure ) # 4. Experimental synthesis (top 10) Process Optimization: # Predict stability under different conditions def predict_stability_map ( composition ): structures = generate_polymorphs ( composition ) temperatures = np . linspace ( 300 , 2000 , 100 ) # K pressures = np . linspace ( 1 , 100 , 50 ) # GPa phase_diagram = np . zeros (( len ( temperatures ), len ( pressures ))) for i , T in enumerate ( temperatures ): for j , P in enumerate ( pressures ): energies = [ model . predict_free_energy ( s , T , P ) for s in structures ] phase_diagram [ i , j ] = np . argmin ( energies ) return phase_diagram Doping and Substitution: # Explore chemical substitutions base_structure = Structure . from_file ( 'LiFePO4.cif' ) for dopant in [ 'Mn' , 'Co' , 'Ni' ]: for site in iron_sites : doped = base_structure . copy () doped . replace ( site , dopant ) # Predict properties of doped material E_g = model . predict ( doped , property = 'band_gap' ) conductivity = model . predict ( doped , property = 'ionic_conductivity' ) print ( f \"Li { dopant } PO4: E_g= { E_g : .2f } eV, \u03c3= { conductivity : .2e } S/cm\" ) Catalysis Applications: # Surface models for catalysis def model_surface ( bulk_structure , miller_indices = ( 1 , 1 , 1 )): # Create surface slab slab = bulk_structure . get_surface ( miller_indices , thickness = 4 ) # Add adsorbate adsorbate = Molecule ([ 'H' , 'H' ], [[ 0 , 0 , 0 ], [ 0 , 0 , 0.74 ]]) slab_with_ads = add_adsorbate ( slab , adsorbate , site = 'ontop' ) # Predict adsorption energy E_slab = model . predict_energy ( slab ) E_slab_ads = model . predict_energy ( slab_with_ads ) E_H2 = model . predict_energy ( adsorbate ) E_ads = E_slab_ads - E_slab - 0.5 * E_H2 return E_ads Challenges and Future Directions \u00b6 Current Limitations: Size limitations : Most models use small unit cells Typical: 10-50 atoms Real systems: Can have 100-1000 atoms (supercells, defects) Accuracy vs speed trade-off : GNNs: Fast but ~10% error DFT: Slow but ~1% error Need: Fast AND accurate Out-of-distribution generalization : Models trained on known materials May fail on truly novel chemistries Active learning can help Emerging Directions: Foundation Models : Pre-train on ALL known structures (~200K) Transfer to specific tasks Few-shot learning for new properties Inverse Design : Input: Desired properties (E_g=1.3 eV, stable, earth-abundant) Output: Candidate structures Challenge: Generative models for crystals Multi-fidelity Learning : Low-fidelity: GNN predictions (fast, many) High-fidelity: DFT calculations (slow, few) Combine: Bayesian optimization, active learning Uncertainty Quantification : prediction , uncertainty = model . predict_with_uncertainty ( structure ) if uncertainty < threshold : # High confidence, trust prediction use_prediction () else : # Low confidence, run DFT run_dft_calculation () 6. Practical: Building a GAT for QM9 Dataset \u00b6 In this hands-on session, we\u2019ll build a Graph Attention Network to predict molecular properties on the QM9 dataset. This practical will cover the complete workflow from data loading to model evaluation and interpretation. Dataset Overview \u00b6 QM9 Dataset: The QM9 dataset is a quantum chemistry benchmark containing: - 134,000 small organic molecules (subset of GDB-17 database) - Up to 9 heavy atoms (C, O, N, F) - excludes hydrogen in counting - 19 regression targets computed with Density Functional Theory (DFT) - 3D molecular geometries with optimized coordinates Target Properties: Index Property Units Typical Range 0 Dipole moment D 0-5 1 Isotropic polarizability a\u2080\u00b3 10-100 2 HOMO energy eV -12 to -5 3 LUMO energy eV -5 to 5 4 HOMO-LUMO gap eV 2-15 5 Electronic spatial extent a\u2080\u00b2 200-1500 6 Zero-point vibrational energy eV 0.5-3.0 7 Internal energy (0K) eV -100 to 0 8 Internal energy (298K) eV -100 to 0 9 Enthalpy (298K) eV -100 to 0 10 Free energy (298K) eV -100 to 0 11 Heat capacity (298K) cal/mol/K 10-40 12 Atomization energy (0K) eV -100 to 0 13 Atomization energy (298K) eV -100 to 0 14-18 Rotational constants A,B,C GHz Various Why QM9 is Important: Benchmark standard : Most papers report QM9 results Diverse chemistry : Various functional groups and structures Ground truth quality : High-level DFT calculations (B3LYP/6-31G(2df,p)) Moderate size : Large enough to train deep models, small enough to iterate quickly Multi-task learning : 19 targets allow testing generalization Data Format: # Each molecule has: molecule = { 'num_atoms' : 18 , 'atomic_numbers' : [ 6 , 6 , 6 , 1 , 1 , ... ], # Element types 'positions' : [[ x1 , y1 , z1 ], [ x2 , y2 , z2 ], ... ], # 3D coordinates (\u00c5) 'edge_index' : [[ 0 , 1 ], [ 1 , 2 ], ... ], # Bond connectivity 'edge_attr' : [[ 1 ], [ 2 ], ... ], # Bond types (1=single, 2=double, 3=triple) 'y' : [ 2.7 , 48.2 , - 7.8 , ... ], # 19 target properties } Implementation Steps \u00b6 Step 1: Data Loading and Preprocessing import torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.datasets import QM9 from torch_geometric.loader import DataLoader from torch_geometric.nn import GATConv , global_mean_pool , global_add_pool import numpy as np from sklearn.metrics import mean_absolute_error , r2_score import matplotlib.pyplot as plt # Set random seeds for reproducibility torch . manual_seed ( 42 ) np . random . seed ( 42 ) # Load QM9 dataset # This will download ~200MB on first run print ( \"Loading QM9 dataset...\" ) dataset = QM9 ( root = './data/QM9' ) print ( f \"Total molecules: { len ( dataset ) } \" ) print ( f \"Number of features: { dataset . num_features } \" ) print ( f \"Number of targets: { dataset . num_tasks } \" ) # Select target property to predict # Let's predict HOMO energy (index 2) target_idx = 2 target_name = 'HOMO_energy' print ( f \" \\n Target: { target_name } (index { target_idx } )\" ) print ( f \"Mean: { dataset . data . y [:, target_idx ] . mean () : .4f } \" ) print ( f \"Std: { dataset . data . y [:, target_idx ] . std () : .4f } \" ) # Examine a sample molecule sample = dataset [ 0 ] print ( f \" \\n Sample molecule:\" ) print ( f \" Number of atoms: { sample . num_nodes } \" ) print ( f \" Number of bonds: { sample . num_edges } \" ) print ( f \" Node features shape: { sample . x . shape } \" ) print ( f \" Edge indices shape: { sample . edge_index . shape } \" ) print ( f \" Target values: { sample . y . shape } \" ) # Split dataset (80% train, 10% validation, 10% test) train_size = int ( 0.8 * len ( dataset )) val_size = int ( 0.1 * len ( dataset )) test_size = len ( dataset ) - train_size - val_size # Use specific indices to ensure consistent splits train_dataset = dataset [: train_size ] val_dataset = dataset [ train_size : train_size + val_size ] test_dataset = dataset [ train_size + val_size :] print ( f \" \\n Dataset splits:\" ) print ( f \" Train: { len ( train_dataset ) } molecules\" ) print ( f \" Val: { len ( val_dataset ) } molecules\" ) print ( f \" Test: { len ( test_dataset ) } molecules\" ) # Compute normalization statistics from training set only # This prevents data leakage y_train = torch . stack ([ data . y [ target_idx ] for data in train_dataset ]) target_mean = y_train . mean () target_std = y_train . std () print ( f \" \\n Normalization (from train set):\" ) print ( f \" Mean: { target_mean : .4f } \" ) print ( f \" Std: { target_std : .4f } \" ) # Create data loaders batch_size = 32 train_loader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True , # Shuffle training data num_workers = 4 # Parallel data loading ) val_loader = DataLoader ( val_dataset , batch_size = batch_size , shuffle = False , num_workers = 4 ) test_loader = DataLoader ( test_dataset , batch_size = batch_size , shuffle = False , num_workers = 4 ) print ( f \" \\n Batch information:\" ) print ( f \" Batch size: { batch_size } \" ) print ( f \" Train batches: { len ( train_loader ) } \" ) print ( f \" Val batches: { len ( val_loader ) } \" ) print ( f \" Test batches: { len ( test_loader ) } \" ) Understanding the Data: # Visualize feature distributions def analyze_dataset ( dataset , name = 'Dataset' ): \"\"\"Analyze and visualize dataset statistics\"\"\" # Collect statistics num_atoms = [ data . num_nodes for data in dataset ] num_bonds = [ data . num_edges // 2 for data in dataset ] # Divide by 2 (undirected) print ( f \" \\n { name } Statistics:\" ) print ( f \" Molecules with 5 atoms: { sum ( n == 5 for n in num_atoms ) } \" ) print ( f \" Molecules with 9 atoms: { sum ( n == 9 for n in num_atoms ) } \" ) print ( f \" Average atoms: { np . mean ( num_atoms ) : .2f } \" ) print ( f \" Average bonds: { np . mean ( num_bonds ) : .2f } \" ) # Plot distributions fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) axes [ 0 ] . hist ( num_atoms , bins = range ( 5 , 11 ), edgecolor = 'black' , alpha = 0.7 ) axes [ 0 ] . set_xlabel ( 'Number of Atoms' ) axes [ 0 ] . set_ylabel ( 'Frequency' ) axes [ 0 ] . set_title ( 'Molecule Size Distribution' ) axes [ 0 ] . grid ( alpha = 0.3 ) # Get target values targets = [ data . y [ target_idx ] . item () for data in dataset ] axes [ 1 ] . hist ( targets , bins = 50 , edgecolor = 'black' , alpha = 0.7 , color = 'green' ) axes [ 1 ] . set_xlabel ( f ' { target_name } (eV)' ) axes [ 1 ] . set_ylabel ( 'Frequency' ) axes [ 1 ] . set_title ( 'Target Distribution' ) axes [ 1 ] . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( f ' { name . lower () } _analysis.png' , dpi = 150 ) print ( f \" Saved: { name . lower () } _analysis.png\" ) analyze_dataset ( train_dataset , 'Training Set' ) Step 2: Define GAT Model class GATForQM9 ( nn . Module ): \"\"\" Graph Attention Network for molecular property prediction Architecture: - Initial embedding layer - Multiple GAT layers with multi-head attention - Batch normalization and residual connections - Global pooling - MLP prediction head \"\"\" def __init__ ( self , num_node_features , hidden_dim = 64 , num_heads = 4 , num_layers = 3 , dropout = 0.2 , target_mean = 0.0 , target_std = 1.0 ): super ( GATForQM9 , self ) . __init__ () # Store normalization parameters self . register_buffer ( 'target_mean' , torch . tensor ( target_mean )) self . register_buffer ( 'target_std' , torch . tensor ( target_std )) # Initial node embedding # Maps input features to hidden dimension self . embedding = nn . Linear ( num_node_features , hidden_dim ) self . embedding_bn = nn . BatchNorm1d ( hidden_dim ) # GAT layers with multi-head attention self . gat_layers = nn . ModuleList () self . batch_norms = nn . ModuleList () for i in range ( num_layers ): # Input dimension changes after concatenation if i == 0 : in_channels = hidden_dim else : in_channels = hidden_dim * num_heads # Last layer averages heads instead of concatenation if i == num_layers - 1 : self . gat_layers . append ( GATConv ( in_channels = in_channels , out_channels = hidden_dim , heads = num_heads , concat = False , # Average instead of concatenate dropout = dropout , add_self_loops = True , # Include self-attention bias = True ) ) self . batch_norms . append ( nn . BatchNorm1d ( hidden_dim )) else : self . gat_layers . append ( GATConv ( in_channels = in_channels , out_channels = hidden_dim , heads = num_heads , concat = True , # Concatenate attention heads dropout = dropout , add_self_loops = True , bias = True ) ) self . batch_norms . append ( nn . BatchNorm1d ( hidden_dim * num_heads )) # Prediction head # Two-layer MLP with ReLU activation self . fc1 = nn . Linear ( hidden_dim , hidden_dim // 2 ) self . bn_fc1 = nn . BatchNorm1d ( hidden_dim // 2 ) self . fc2 = nn . Linear ( hidden_dim // 2 , 1 ) self . dropout = nn . Dropout ( dropout ) self . relu = nn . ReLU () # Initialize weights self . _initialize_weights () def _initialize_weights ( self ): \"\"\"Kaiming initialization for better training\"\"\" for m in self . modules (): if isinstance ( m , nn . Linear ): nn . init . kaiming_normal_ ( m . weight , mode = 'fan_out' , nonlinearity = 'relu' ) if m . bias is not None : nn . init . constant_ ( m . bias , 0 ) def forward ( self , data ): \"\"\" Forward pass Args: data: PyG Batch object with x, edge_index, batch Returns: predictions: Tensor of shape [batch_size] \"\"\" x , edge_index , batch = data . x , data . edge_index , data . batch # Initial embedding x = self . embedding ( x ) x = self . embedding_bn ( x ) x = self . relu ( x ) x = self . dropout ( x ) # GAT layers with residual connections for i , ( gat , bn ) in enumerate ( zip ( self . gat_layers , self . batch_norms )): # Store input for residual connection x_residual = x # GAT convolution x = gat ( x , edge_index ) x = bn ( x ) x = self . relu ( x ) x = self . dropout ( x ) # Residual connection (with projection if dimensions don't match) if x_residual . size ( 1 ) == x . size ( 1 ): x = x + x_residual elif i > 0 : # Skip for first layer # Project residual to match dimensions x_residual_proj = self . _project_residual ( x_residual , x . size ( 1 )) x = x + x_residual_proj # Global pooling: aggregate node features to graph-level # Mean pooling: intensive property (per-atom average) x = global_mean_pool ( x , batch ) # Prediction head x = self . fc1 ( x ) x = self . bn_fc1 ( x ) x = self . relu ( x ) x = self . dropout ( x ) x = self . fc2 ( x ) # Denormalize predictions x = x * self . target_std + self . target_mean return x . squeeze ( - 1 ) # Remove last dimension [batch, 1] -> [batch] def _project_residual ( self , x_residual , target_dim ): \"\"\"Helper to project residual connection\"\"\" if not hasattr ( self , 'residual_projections' ): self . residual_projections = {} key = ( x_residual . size ( 1 ), target_dim ) if key not in self . residual_projections : self . residual_projections [ key ] = nn . Linear ( x_residual . size ( 1 ), target_dim , bias = False ) . to ( x_residual . device ) return self . residual_projections [ key ]( x_residual ) # Instantiate model device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) print ( f \" \\n Using device: { device } \" ) model = GATForQM9 ( num_node_features = dataset . num_features , hidden_dim = 128 , num_heads = 4 , num_layers = 4 , dropout = 0.2 , target_mean = target_mean . item (), target_std = target_std . item () ) . to ( device ) # Count parameters num_params = sum ( p . numel () for p in model . parameters () if p . requires_grad ) print ( f \"Number of trainable parameters: { num_params : , } \" ) # Print model architecture print ( \" \\n Model Architecture:\" ) print ( model ) Step 3: Training Loop with Best Practices def train_epoch ( model , loader , optimizer , criterion , device ): \"\"\"Train for one epoch\"\"\" model . train () total_loss = 0 num_samples = 0 for data in loader : data = data . to ( device ) # Forward pass pred = model ( data ) target = data . y [:, target_idx ] # Compute loss loss = criterion ( pred , target ) # Backward pass optimizer . zero_grad () loss . backward () # Gradient clipping to prevent explosion torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 1.0 ) optimizer . step () total_loss += loss . item () * data . num_graphs num_samples += data . num_graphs return total_loss / num_samples def evaluate ( model , loader , criterion , device ): \"\"\"Evaluate model\"\"\" model . eval () total_loss = 0 predictions = [] targets = [] with torch . no_grad (): for data in loader : data = data . to ( device ) pred = model ( data ) target = data . y [:, target_idx ] loss = criterion ( pred , target ) total_loss += loss . item () * data . num_graphs predictions . append ( pred . cpu ()) targets . append ( target . cpu ()) predictions = torch . cat ( predictions ) targets = torch . cat ( targets ) # Compute metrics mae = torch . mean ( torch . abs ( predictions - targets )) . item () rmse = torch . sqrt ( torch . mean (( predictions - targets ) ** 2 )) . item () r2 = r2_score ( targets . numpy (), predictions . numpy ()) return { 'loss' : total_loss / len ( loader . dataset ), 'mae' : mae , 'rmse' : rmse , 'r2' : r2 , 'predictions' : predictions , 'targets' : targets } # Setup training optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 , weight_decay = 1e-5 ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = 'min' , factor = 0.5 , # Reduce LR by half patience = 10 , # Wait 10 epochs before reducing verbose = True , min_lr = 1e-6 # Don't go below this ) criterion = nn . L1Loss () # MAE loss # Training loop num_epochs = 200 best_val_loss = float ( 'inf' ) patience_counter = 0 early_stop_patience = 30 # Track history history = { 'train_loss' : [], 'val_loss' : [], 'val_mae' : [], 'val_r2' : [], 'learning_rate' : [] } print ( f \" \\n Starting training for { num_epochs } epochs...\" ) print ( \"=\" * 70 ) for epoch in range ( num_epochs ): # Train train_loss = train_epoch ( model , train_loader , optimizer , criterion , device ) # Evaluate val_metrics = evaluate ( model , val_loader , criterion , device ) val_loss = val_metrics [ 'loss' ] val_mae = val_metrics [ 'mae' ] val_r2 = val_metrics [ 'r2' ] # Learning rate scheduling scheduler . step ( val_loss ) current_lr = optimizer . param_groups [ 0 ][ 'lr' ] # Store history history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_mae' ] . append ( val_mae ) history [ 'val_r2' ] . append ( val_r2 ) history [ 'learning_rate' ] . append ( current_lr ) # Save best model if val_loss < best_val_loss : best_val_loss = val_loss torch . save ({ 'epoch' : epoch , 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (), 'val_loss' : val_loss , 'val_mae' : val_mae , }, 'best_gat_qm9.pt' ) patience_counter = 0 else : patience_counter += 1 # Print progress if ( epoch + 1 ) % 10 == 0 or epoch == 0 : print ( f 'Epoch { epoch + 1 : 3d } / { num_epochs } :' ) print ( f ' Train Loss: { train_loss : .4f } ' ) print ( f ' Val Loss: { val_loss : .4f } | MAE: { val_mae : .4f } | R\u00b2: { val_r2 : .4f } ' ) print ( f ' LR: { current_lr : .2e } | Best Val: { best_val_loss : .4f } | Patience: { patience_counter } / { early_stop_patience } ' ) print ( \"-\" * 70 ) # Early stopping if patience_counter >= early_stop_patience : print ( f \" \\n Early stopping triggered at epoch { epoch + 1 } \" ) break print ( \"=\" * 70 ) print ( \"Training completed!\" ) Step 4: Model Evaluation and Analysis # Load best model print ( \" \\n Loading best model...\" ) checkpoint = torch . load ( 'best_gat_qm9.pt' ) model . load_state_dict ( checkpoint [ 'model_state_dict' ]) print ( f \"Best model from epoch { checkpoint [ 'epoch' ] + 1 } \" ) print ( f \"Best validation MAE: { checkpoint [ 'val_mae' ] : .4f } eV\" ) # Evaluate on test set print ( \" \\n Evaluating on test set...\" ) test_metrics = evaluate ( model , test_loader , criterion , device ) print ( \" \\n Test Set Results:\" ) print ( f \" Loss: { test_metrics [ 'loss' ] : .4f } \" ) print ( f \" MAE: { test_metrics [ 'mae' ] : .4f } eV\" ) print ( f \" RMSE: { test_metrics [ 'rmse' ] : .4f } eV\" ) print ( f \" R\u00b2: { test_metrics [ 'r2' ] : .4f } \" ) # Plot training curves fig , axes = plt . subplots ( 2 , 2 , figsize = ( 14 , 10 )) # Loss curves axes [ 0 , 0 ] . plot ( history [ 'train_loss' ], label = 'Train' , linewidth = 2 ) axes [ 0 , 0 ] . plot ( history [ 'val_loss' ], label = 'Validation' , linewidth = 2 ) axes [ 0 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 0 ] . set_ylabel ( 'Loss (MAE)' ) axes [ 0 , 0 ] . set_title ( 'Training and Validation Loss' ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( alpha = 0.3 ) # MAE over epochs axes [ 0 , 1 ] . plot ( history [ 'val_mae' ], color = 'green' , linewidth = 2 ) axes [ 0 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 1 ] . set_ylabel ( 'MAE (eV)' ) axes [ 0 , 1 ] . set_title ( 'Validation MAE' ) axes [ 0 , 1 ] . grid ( alpha = 0.3 ) # R\u00b2 over epochs axes [ 1 , 0 ] . plot ( history [ 'val_r2' ], color = 'purple' , linewidth = 2 ) axes [ 1 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 1 , 0 ] . set_ylabel ( 'R\u00b2 Score' ) axes [ 1 , 0 ] . set_title ( 'Validation R\u00b2' ) axes [ 1 , 0 ] . grid ( alpha = 0.3 ) # Learning rate schedule axes [ 1 , 1 ] . plot ( history [ 'learning_rate' ], color = 'red' , linewidth = 2 ) axes [ 1 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 1 , 1 ] . set_ylabel ( 'Learning Rate' ) axes [ 1 , 1 ] . set_title ( 'Learning Rate Schedule' ) axes [ 1 , 1 ] . set_yscale ( 'log' ) axes [ 1 , 1 ] . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( 'training_curves.png' , dpi = 150 ) print ( \" \\n Saved: training_curves.png\" ) Step 5: Visualization and Interpretation def plot_predictions ( predictions , targets , dataset_name = 'Test' ): \"\"\"Create comprehensive prediction plots\"\"\" predictions = predictions . numpy () targets = targets . numpy () fig , axes = plt . subplots ( 2 , 2 , figsize = ( 14 , 12 )) # Scatter plot: Predicted vs Actual axes [ 0 , 0 ] . scatter ( targets , predictions , alpha = 0.3 , s = 20 ) # Perfect prediction line min_val , max_val = min ( targets . min (), predictions . min ()), max ( targets . max (), predictions . max ()) axes [ 0 , 0 ] . plot ([ min_val , max_val ], [ min_val , max_val ], 'r--' , lw = 2 , label = 'Perfect Prediction' ) # Add statistics text mae = mean_absolute_error ( targets , predictions ) r2 = r2_score ( targets , predictions ) axes [ 0 , 0 ] . text ( 0.05 , 0.95 , f 'MAE = { mae : .4f } eV \\n R\u00b2 = { r2 : .4f } ' , transform = axes [ 0 , 0 ] . transAxes , fontsize = 12 , verticalalignment = 'top' , bbox = dict ( boxstyle = 'round' , facecolor = 'wheat' , alpha = 0.5 ) ) axes [ 0 , 0 ] . set_xlabel ( f 'Actual { target_name } (eV)' , fontsize = 12 ) axes [ 0 , 0 ] . set_ylabel ( f 'Predicted { target_name } (eV)' , fontsize = 12 ) axes [ 0 , 0 ] . set_title ( f ' { dataset_name } Set: Predictions vs Actual' , fontsize = 14 ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( alpha = 0.3 ) axes [ 0 , 0 ] . set_aspect ( 'equal' ) # Error distribution errors = predictions - targets axes [ 0 , 1 ] . hist ( errors , bins = 50 , edgecolor = 'black' , alpha = 0.7 , color = 'coral' ) axes [ 0 , 1 ] . axvline ( 0 , color = 'r' , linestyle = '--' , linewidth = 2 , label = 'Zero Error' ) axes [ 0 , 1 ] . set_xlabel ( 'Prediction Error (eV)' , fontsize = 12 ) axes [ 0 , 1 ] . set_ylabel ( 'Frequency' , fontsize = 12 ) axes [ 0 , 1 ] . set_title ( 'Error Distribution' , fontsize = 14 ) axes [ 0 , 1 ] . legend () axes [ 0 , 1 ] . grid ( alpha = 0.3 ) # Error vs actual value axes [ 1 , 0 ] . scatter ( targets , np . abs ( errors ), alpha = 0.3 , s = 20 , color = 'green' ) axes [ 1 , 0 ] . axhline ( mae , color = 'r' , linestyle = '--' , linewidth = 2 , label = f 'Mean MAE = { mae : .4f } ' ) axes [ 1 , 0 ] . set_xlabel ( f 'Actual { target_name } (eV)' , fontsize = 12 ) axes [ 1 , 0 ] . set_ylabel ( 'Absolute Error (eV)' , fontsize = 12 ) axes [ 1 , 0 ] . set_title ( 'Absolute Error vs Actual Value' , fontsize = 14 ) axes [ 1 , 0 ] . legend () axes [ 1 , 0 ] . grid ( alpha = 0.3 ) # Q-Q plot (Quantile-Quantile) from scipy import stats stats . probplot ( errors , dist = \"norm\" , plot = axes [ 1 , 1 ]) axes [ 1 , 1 ] . set_title ( 'Q-Q Plot (Error Normality)' , fontsize = 14 ) axes [ 1 , 1 ] . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( f ' { dataset_name . lower () } _predictions.png' , dpi = 150 ) print ( f \"Saved: { dataset_name . lower () } _predictions.png\" ) # Plot test set predictions plot_predictions ( test_metrics [ 'predictions' ], test_metrics [ 'targets' ], dataset_name = 'Test' ) # Analyze errors by molecule size def analyze_errors_by_size ( model , loader , device ): \"\"\"Analyze how prediction error varies with molecule size\"\"\" model . eval () errors_by_size = { i : [] for i in range ( 5 , 10 )} with torch . no_grad (): for data in loader : data = data . to ( device ) pred = model ( data ) target = data . y [:, target_idx ] errors = torch . abs ( pred - target ) . cpu () # Group by number of atoms for i , error in enumerate ( errors ): num_atoms = ( data . batch == i ) . sum () . item () if num_atoms in errors_by_size : errors_by_size [ num_atoms ] . append ( error . item ()) # Plot plt . figure ( figsize = ( 10 , 6 )) sizes = sorted ( errors_by_size . keys ()) mean_errors = [ np . mean ( errors_by_size [ size ]) if errors_by_size [ size ] else 0 for size in sizes ] std_errors = [ np . std ( errors_by_size [ size ]) if errors_by_size [ size ] else 0 for size in sizes ] plt . errorbar ( sizes , mean_errors , yerr = std_errors , marker = 'o' , linewidth = 2 , markersize = 8 , capsize = 5 ) plt . xlabel ( 'Number of Heavy Atoms' , fontsize = 12 ) plt . ylabel ( 'Mean Absolute Error (eV)' , fontsize = 12 ) plt . title ( 'Prediction Error vs Molecule Size' , fontsize = 14 ) plt . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( 'error_by_size.png' , dpi = 150 ) print ( \"Saved: error_by_size.png\" ) return errors_by_size errors_by_size = analyze_errors_by_size ( model , test_loader , device ) Expected Results \u00b6 Performance Benchmarks: For HOMO energy prediction (in meV): Method MAE RMSE R\u00b2 Parameters Baseline 300 420 0.00 - Linear 180 250 0.45 10K MLP 120 170 0.72 50K GCN 45 65 0.94 500K GAT (ours) 35-40 50-55 0.96 800K DimeNet++ 25-30 40-45 0.98 2M Note: Results vary by hyperparameters and random seed Training Time: Hardware: Single GPU (V100) Batch size: 32 Epochs: ~100-150 to convergence Time per epoch: ~30 seconds Total training: ~60-90 minutes Inference: ~1000 molecules/second Extension Ideas \u00b6 1. Multi-Task Learning class MultiTaskGAT ( nn . Module ): \"\"\"Predict multiple properties simultaneously\"\"\" def __init__ ( self , num_tasks = 19 ): super () . __init__ () # Shared encoder self . encoder = GATEncoder ( ... ) # Separate heads for each task self . task_heads = nn . ModuleList ([ nn . Linear ( hidden_dim , 1 ) for _ in range ( num_tasks ) ]) def forward ( self , data ): h = self . encoder ( data ) return torch . cat ([ head ( h ) for head in self . task_heads ], dim =- 1 ) # Train with multi-task loss loss = sum ( F . mse_loss ( pred [:, i ], target [:, i ]) for i in range ( num_tasks )) 2. Add 3D Distance Information # Add edge features based on 3D distances edge_attr = [] for edge in data . edge_index . t (): i , j = edge dist = torch . norm ( data . pos [ i ] - data . pos [ j ]) # Gaussian RBF encoding rbf = torch . exp ( - ( dist - centers ) ** 2 / gamma ) edge_attr . append ( rbf ) # Use in GAT model = GATConv ( edge_dim = num_rbf ) # Enable edge features 3. Attention Visualization def visualize_attention ( model , molecule_idx ): \"\"\"Visualize attention weights for a molecule\"\"\" data = dataset [ molecule_idx ] . to ( device ) model . eval () # Hook to capture attention attention_weights = [] def hook_fn ( module , input , output ): # GATConv returns (output, attention_weights) if isinstance ( output , tuple ) and len ( output ) == 2 : attention_weights . append ( output [ 1 ] . detach () . cpu ()) # Register hooks on GAT layers hooks = [] for layer in model . gat_layers : hooks . append ( layer . register_forward_hook ( hook_fn )) # Forward pass with torch . no_grad (): _ = model ( data ) # Remove hooks for hook in hooks : hook . remove () # Visualize (requires RDKit or py3Dmol) from rdkit import Chem from rdkit.Chem import Draw # Highlight high-attention bonds mol = Chem . MolFromSmiles ( data . smiles ) # If SMILES available highlight_bonds = [] for i , ( src , dst ) in enumerate ( data . edge_index . t ()): if attention_weights [ 0 ][ i ] > threshold : highlight_bonds . append ( mol . GetBondBetweenAtoms ( int ( src ), int ( dst )) . GetIdx ()) img = Draw . MolToImage ( mol , highlightBonds = highlight_bonds ) plt . imshow ( img ) plt . axis ( 'off' ) plt . savefig ( f 'attention_mol_ { molecule_idx } .png' ) 4. Hyperparameter Tuning import optuna def objective ( trial ): # Suggest hyperparameters hidden_dim = trial . suggest_int ( 'hidden_dim' , 64 , 256 ) num_heads = trial . suggest_int ( 'num_heads' , 2 , 8 ) num_layers = trial . suggest_int ( 'num_layers' , 2 , 6 ) dropout = trial . suggest_float ( 'dropout' , 0.1 , 0.5 ) lr = trial . suggest_loguniform ( 'lr' , 1e-4 , 1e-2 ) # Train model model = GATForQM9 ( hidden_dim = hidden_dim , num_heads = num_heads , ... ) # ... training loop ... return val_mae # Run optimization study = optuna . create_study ( direction = 'minimize' ) study . optimize ( objective , n_trials = 100 ) print ( f \"Best hyperparameters: { study . best_params } \" ) 5. Ensemble Methods # Train multiple models with different seeds models = [] for seed in range ( 5 ): torch . manual_seed ( seed ) model = GATForQM9 ( ... ) # ... train model ... models . append ( model ) # Ensemble prediction def ensemble_predict ( data ): predictions = [ model ( data ) for model in models ] return torch . stack ( predictions ) . mean ( dim = 0 ) # Usually 2-5% better than single model This practical provides a complete, production-ready implementation of GAT for molecular property prediction with extensive analysis and interpretation tools. Summary \u00b6 Today we covered the fundamentals of graph neural networks and their applications in computational chemistry and materials science: Message Passing Neural Networks provide a flexible framework for learning on graphs Graph Attention Networks learn adaptive edge weights through attention mechanisms Equivariant Networks (SchNet, DimeNet, PaiNN) respect 3D geometric symmetries Protein-Ligand Modeling enables AI-driven drug discovery Crystal Structure Prediction accelerates materials discovery Practical Implementation demonstrated how to build and train GATs on real molecular data Additional Resources \u00b6 Papers: - \u201cNeural Message Passing for Quantum Chemistry\u201d (Gilmer et al., 2017) - \u201cGraph Attention Networks\u201d (Veli\u010dkovi\u0107 et al., 2018) - \u201cSchNet: A continuous-filter convolutional neural network\u201d (Sch\u00fctt et al., 2017) - \u201cDirectional Message Passing for Molecular Graphs\u201d (Klicpera et al., 2020) - \u201cEquivariant message passing for the prediction of tensorial properties\u201d (Sch\u00fctt et al., 2021) Tutorials: - PyTorch Geometric documentation and tutorials - Open Catalyst Project tutorials - Deep Graph Library (DGL) examples Datasets: - QM9: Small organic molecules - PCQM4M: Large-scale quantum chemistry dataset - Materials Project: Inorganic crystals - PDBbind: Protein-ligand binding affinities - Open Catalyst: Catalytic materials","title":"Graph Neural Networks"},{"location":"day3/#day__3__graph__neural__networks__and__geometric__deep__learning","text":"","title":"Day 3: Graph Neural Networks and Geometric Deep Learning"},{"location":"day3/#overview","text":"Day 3 focuses on Graph Neural Networks (GNNs) and their applications in molecular and materials science. We\u2019ll explore how graphs can represent molecular structures and how specialized neural network architectures can learn from these representations while respecting physical symmetries and constraints.","title":"Overview"},{"location":"day3/#topics__covered","text":"","title":"Topics Covered"},{"location":"day3/#1__message__passing__neural__networks","text":"Message Passing Neural Networks (MPNNs) form the foundation of modern graph neural networks for molecular property prediction. They provide a unified framework for understanding how information flows through graph-structured data, making them particularly well-suited for molecular modeling where atoms and bonds naturally form graph structures.","title":"1. Message Passing Neural Networks"},{"location":"day3/#core__concepts","text":"Graph Representation: In molecular graphs, the structure naturally maps to graph representations: - Nodes represent atoms with rich feature vectors including: - Atomic number (element identity) - Formal charge and partial charge - Hybridization state (sp, sp2, sp3) - Number of hydrogen atoms - Aromaticity - Chirality - Atomic mass - Degree (number of connections) Edges represent bonds with attributes such as: Bond type (single, double, triple, aromatic) Bond length/distance Stereochemistry (cis/trans, E/Z) Conjugation Ring membership The graph structure encodes molecular connectivity, preserving the topological relationships that determine chemical properties Why Message Passing? The key insight behind MPNNs is that the properties of an atom in a molecule depend not just on the atom itself, but on its chemical environment. Message passing mimics how chemical effects propagate through molecular structures: - Inductive effects travel through sigma bonds - Resonance effects propagate through conjugated systems - Steric effects depend on spatial arrangements of neighboring groups Message Passing Framework: The MPNN framework consists of three main phases that iterate to build increasingly sophisticated representations: Message Phase (T iterations): m_v^(t+1) = \u03a3_{u\u2208N(v)} M_t(h_v^t, h_u^t, e_{uv}) In this phase: - Each node v receives messages from its neighbors N(v) - The message function M_t is a learnable neural network that combines: - h_v^t: the current node\u2019s hidden state - h_u^t: each neighbor\u2019s hidden state - e_{uv}: edge features connecting the nodes - Messages are computed for all edges simultaneously - The superscript t indicates the iteration number Intuition : Think of this as each atom \u201clistening\u201d to what its bonded neighbors are telling it about their local chemical environment. Update Phase : h_v^(t+1) = U_t(h_v^t, m_v^(t+1)) In this phase: - Each node updates its representation using the aggregated messages - U_t is typically a GRU, LSTM, or feedforward network - The update combines the node\u2019s previous state with new information - This preserves information from earlier iterations while incorporating new context Intuition : After listening to neighbors, each atom updates its own representation to reflect what it learned about its environment. Aggregation Within Messages : The summation in the message phase is just one choice. Other aggregation functions include: - Sum : \u03a3_{u\u2208N(v)} M_t(...) - sensitive to neighborhood size - Mean : (1/|N(v)|) \u03a3_{u\u2208N(v)} M_t(...) - normalizes by degree - Max : max_{u\u2208N(v)} M_t(...) - captures strongest signal - Attention : \u03a3_{u\u2208N(v)} \u03b1_{vu} M_t(...) - learnable weights (GAT) Readout Phase: After T message passing steps, we have node-level representations h_v^T for each atom. To predict molecular properties, we need a graph-level representation: y = R({h_v^T | v \u2208 G}) Common readout functions include: Sum pooling : R = \u03a3_v h_v^T Captures total contributions from all atoms Sensitive to molecule size Good for extensive properties (like mass, number of electrons) Mean pooling : R = (1/|V|) \u03a3_v h_v^T Normalizes by number of atoms Better for intensive properties (like density, stability per atom) Size-invariant representation Max pooling : R = max_v h_v^T (element-wise) Captures most significant features Can miss important distributed information Set2Set : A learnable attention-based aggregation Uses LSTM to iteratively attend to nodes More expressive but computationally expensive Can capture complex relationships between atoms Depth and Receptive Fields: The number of message passing iterations T determines the receptive field: - T=1: Each node sees only immediate neighbors (1-hop) - T=2: Each node sees neighbors and neighbors-of-neighbors (2-hop) - T=k: Each node sees all nodes within k bonds For molecular graphs: - Small molecules (QM9): T=3-5 is usually sufficient - Proteins: T=5-10 may be needed for long-range interactions - Trade-off: More iterations = larger receptive field but risk of over-smoothing","title":"Core Concepts"},{"location":"day3/#key__variants","text":"Graph Convolutional Networks (GCN): GCNs simplify message passing using a spectral approach: H^(t+1) = \u03c3(D^(-1/2) \u00c3 D^(-1/2) H^(t) W^(t)) Where: - \u00c3 = A + I (adjacency matrix with self-loops) - D is the degree matrix - This is equivalent to message passing with normalized averaging - Very efficient for semi-supervised learning on large graphs - Less flexible for edge features than general MPNNs Benefits for chemistry: - Fast computation on molecular graphs - Symmetric normalization prevents exploding/vanishing gradients - Can be stacked deeply with residual connections Limitations: - Doesn\u2019t naturally incorporate edge features - Fixed aggregation (normalized sum) - May struggle with distinguishing certain graph structures (limited expressivity) GraphSAGE (Sample and Aggregate): GraphSAGE introduces sampling for scalability: h_v^(t+1) = \u03c3(W \u00b7 CONCAT(h_v^t, AGG({h_u^t | u \u2208 Sample(N(v))}))) Key innovations: - Sampling : Instead of aggregating from all neighbors, sample a fixed number - Multiple aggregators : - Mean: AGG = (1/|S|) \u03a3_{u\u2208S} h_u - LSTM: Process neighbors sequentially (requires ordering) - Pooling: AGG = max(\u03c3(W_pool h_u + b)) - Concatenation : Explicitly preserves self-information Benefits for chemistry: - Handles variable-sized neighborhoods efficiently - Inductive learning: can generalize to new molecules not seen during training - Scalable to very large molecular databases Neural Message Passing for Quantum Chemistry (MPNN): The original MPNN paper specialized the framework for molecules: Architecture: m_v^(t+1) = \u03a3_{u\u2208N(v)} M_t(h_v^t, h_u^t, e_{uv}) = \u03a3_{u\u2208N(v)} A_t(e_{uv}) \u00b7 h_u^t h_v^(t+1) = U_t(h_v^t, m_v^(t+1)) = GRU(h_v^t, m_v^(t+1)) Key design choices: - Edge networks : A_t(e_{uv}) is a neural network that produces edge-specific matrices - Allows different bond types to transform neighbor information differently - Captures the idea that single/double/triple bonds transmit information differently GRU updates : Using Gated Recurrent Units for the update function Helps with gradient flow through multiple message passing steps Gates control what information to keep vs. update More stable than simple MLPs for deep message passing Virtual edges : Can add edges between non-bonded atoms within a distance cutoff Captures through-space interactions Important for conformational effects and weak interactions Master equations: # Initialize h_v^0 = embedding(x_v) # x_v are input features # Message passing for t in range(T): for each edge (v,u): m_vu = EdgeNetwork(e_vu) @ h_u^t m_v = \u03a3_u m_vu h_v^(t+1) = GRU(h_v^t, m_v) # Readout h_G = Set2Set({h_v^T | v \u2208 G}) y = MLP(h_G) Training considerations: - Typically T=3-6 message passing steps - Hidden dimensions: 64-256 depending on task complexity - Batch normalization or layer normalization helps training stability - Dropout between layers prevents overfitting","title":"Key Variants"},{"location":"day3/#applications__in__chemistry","text":"Molecular Property Prediction: MPNNs excel at predicting quantum mechanical and physical properties: Quantum properties (QM9 dataset): HOMO/LUMO energies (frontier orbitals) Internal energy and enthalpy Free energy and heat capacity Electronic spatial extent Zero-point vibrational energy Atomization energy Why MPNNs work : These properties depend on electronic structure, which is determined by how atoms and bonds are connected. Message passing naturally captures these structural effects. Physical properties : Solubility (important for drug absorption) Melting/boiling points Density and refractive index Viscosity Challenge : These properties can depend on 3D conformations, so incorporating geometry helps. Biological activity : Toxicity predictions (hERG, Ames, hepatotoxicity) Binding affinity to target proteins ADMET properties (Absorption, Distribution, Metabolism, Excretion, Toxicity) Blood-brain barrier penetration Application : Early-stage drug filtering, reducing costly experimental screening. Reaction Outcome Prediction: Given reactants and conditions, predict the major products: - Graph of reactants \u2192 MPNN \u2192 Product distribution - Can incorporate reaction conditions (temperature, solvent, catalysts) as global features - Attention mechanisms can identify reactive sites Retrosynthesis Planning: Predict how to synthesize a target molecule: - Target molecule \u2192 MPNN \u2192 Likely precursors - Can be formulated as a translation problem (product graph \u2192 reactant graphs) - Helps chemists find synthetic routes for complex molecules Drug Discovery and Virtual Screening: Screen millions of compounds against target proteins: - Fast prediction once model is trained (~1000 molecules/second) - Can be combined with active learning to guide experimental efforts - Multi-task learning: predict multiple properties simultaneously - Transfer learning: pre-train on large databases, fine-tune on specific targets De Novo Molecular Design: Use MPNNs as discriminators or reward functions in generative models Guide molecular generation toward desired properties Combine with optimization algorithms (genetic algorithms, reinforcement learning)","title":"Applications in Chemistry"},{"location":"day3/#limitations__and__challenges","text":"Over-smoothing : With many message passing steps, node representations become too similar - Solution : Residual connections, jumping knowledge networks Limited expressivity : Some graphs are indistinguishable by message passing - Solution : Add higher-order structural features, use more sophisticated aggregation Scalability : Large molecules or protein graphs can be computationally expensive - Solution : Sampling (GraphSAGE), hierarchical approaches, graph coarsening 3D structure : Basic MPNNs ignore 3D geometry - Solution : Add distance as edge features, use equivariant networks (next sections)","title":"Limitations and Challenges"},{"location":"day3/#2__graph__attention__networks","text":"Graph Attention Networks (GATs) introduce attention mechanisms to graph learning, allowing the model to learn which neighbors are most important for each node. This is a significant advancement over basic message passing, where all neighbors contribute equally to a node\u2019s update.","title":"2. Graph Attention Networks"},{"location":"day3/#motivation__and__intuition","text":"Why Attention for Graphs? In molecular contexts, not all bonds are equally important for determining a property: - In a large molecule, a specific functional group might dominate reactivity - Some atoms are in the \u201ccore\u201d structure while others are in peripheral substituents - Certain bonds participate in conjugation or resonance, making them more significant - In protein-ligand binding, only residues near the binding site matter Analogy to NLP : Just as in language, where \u201cbank\u201d means different things in \u201criver bank\u201d vs \u201csavings bank\u201d depending on context, an atom\u2019s role depends on which neighbors are most relevant. GATs allow the network to automatically learn these context-dependent importance weights.","title":"Motivation and Intuition"},{"location":"day3/#attention__mechanism","text":"Core Idea: Unlike MPNNs that use fixed aggregation (sum, mean) or hand-crafted edge weights, GATs learn attention coefficients \u03b1_{ij} that adaptively weigh the importance of each neighbor j for node i. Mathematical Framework: The attention mechanism consists of three steps: Step 1: Compute Attention Logits e_{ij} = LeakyReLU(a^T [W h_i || W h_j]) Breaking this down: - W h_i and W h_j : First, transform node features through a shared weight matrix W - This projects features into a common space where comparisons are meaningful - Dimension: [d_in] \u2192 [d_out] [W h_i || W h_j] : Concatenate transformed features of node i and neighbor j Creates a pairwise feature vector Dimension: [2 \u00d7 d_out] a^T [\u2026] : Apply learned attention vector a Reduces to scalar attention logit e_{ij} The attention vector a learns what feature combinations indicate importance Dimension: [2 \u00d7 d_out] \u2192 [1] LeakyReLU : Non-linearity with slope \u03b1 for negative values (typically \u03b1=0.2) Allows negative attention scores (before softmax) Prevents dead neurons from ReLU saturation Intuition : The attention logit e_{ij} measures how relevant neighbor j is to node i, based on their feature compatibility. Step 2: Normalize to Attention Coefficients \u03b1_{ij} = softmax_j(e_{ij}) = exp(e_{ij}) / \u03a3_{k\u2208N(i)} exp(e_{ik}) Softmax normalization : Ensures attention weights sum to 1 over all neighbors Comparison : Only neighbors compete for attention (not all nodes in graph) Interpretation : \u03b1_{ij} \u2208 (0, 1) represents the probability-like importance of neighbor j Why softmax? - Preserves differentiability (can backpropagate) - Creates sharp distinctions (high e_{ij} \u2192 high \u03b1_{ij}) - Normalized weights prevent exploding values in aggregation Step 3: Aggregate with Attention Weights h_i' = \u03c3(\u03a3_{j\u2208N(i)} \u03b1_{ij} W h_j) Weighted sum : Each neighbor contributes proportionally to its attention weight W h_j : Uses the same transformation from step 1 (parameter sharing) \u03c3 : Activation function (typically ELU or ReLU) Complete Forward Pass Example: For an atom with 3 neighbors: 1. Compute logits: e_{i1} = 0.8, e_{i2} = 0.3, e_{i3} = -0.2 2. Apply softmax: \u03b1_{i1} = 0.52, \u03b1_{i2} = 0.31, \u03b1_{i3} = 0.17 3. Aggregate: h_i\u2019 = \u03c3(0.52 \u00d7 W h_1 + 0.31 \u00d7 W h_2 + 0.17 \u00d7 W h_3) Key Properties: Self-attention : Can include self-loops (i,i) so nodes attend to themselves Asymmetric : \u03b1_{ij} \u2260 \u03b1_{ji} (attention from i\u2192j differs from j\u2192i) Local : Only attends to direct neighbors (preserves graph structure) Permutation invariant : Order of neighbors doesn\u2019t matter","title":"Attention Mechanism"},{"location":"day3/#multi-head__attention","text":"To stabilize learning and capture different types of relationships simultaneously, GATs employ multiple independent attention mechanisms (heads). Multi-Head Aggregation (Hidden Layers): h_i' = ||_{k=1}^K \u03c3(\u03a3_{j\u2208N(i)} \u03b1_{ij}^k W^k h_j) Where: - K = number of attention heads - Each head k has its own parameters: W^k and a^k - || denotes concatenation of head outputs - Output dimension: K \u00d7 d_out Why Multiple Heads? Different heads can learn complementary attention patterns: - Head 1 : Might focus on electronegative atoms (for polarity) - Head 2 : Might focus on aromatic neighbors (for conjugation) - Head 3 : Might focus on steric bulk (for size effects) - Head 4 : Might attend to formal charges Intuition from Chemistry : Just as a chemist considers multiple factors simultaneously (electronics, sterics, orbital interactions), multiple heads capture different aspects of molecular structure. Multi-Head Averaging (Output Layer): h_i' = \u03c3((1/K) \u03a3_{k=1}^K \u03a3_{j\u2208N(i)} \u03b1_{ij}^k W^k h_j) For the final layer, averaging instead of concatenation: - Keeps output dimension consistent with target - Ensembles the predictions from different heads - More stable for final predictions Implementation Considerations: # Typical hyperparameters num_heads = 4 - 8 # More heads = more capacity but more parameters hidden_dim = 64 - 256 # Per-head dimension dropout = 0.1 - 0.3 # On attention coefficients (attention dropout) Computational Complexity: - Attention computation: O(|E| \u00d7 d_out) - linear in edges - Memory for attention: O(|E| \u00d7 K) - stores attention per head - Highly parallelizable (all attention coefficients computed independently)","title":"Multi-Head Attention"},{"location":"day3/#advantages__of__gats","text":"1. Adaptive Neighborhoods Unlike fixed aggregation: # Fixed (GCN-style) h_i' = \u03a3_{j\u2208N(i)} (1/\u221a(d_i \u00d7 d_j)) W h_j # predetermined weights # Adaptive (GAT) h_i' = \u03a3_{j\u2208N(i)} \u03b1_{ij} W h_j # learned weights Benefits: - Automatically focuses on relevant neighbors - Can ignore uninformative connections - Adapts to different chemical contexts Example : In a drug molecule, GAT can learn to focus on: - Pharmacophore groups (active parts) - Hydrogen bond donors/acceptors - Hydrophobic regions While downweighting inert carbon chains. 2. Interpretability Attention weights \u03b1_{ij} can be visualized and interpreted: # Extract attention weights attention_weights = model . get_attention () # shape: [num_edges, num_heads] # Visualize for a molecule mol_graph = molecule . to_graph () highlight_edges_by_weight ( mol_graph , attention_weights , threshold = 0.3 ) What we can learn: - Which atoms influence predictions most - How information flows through the molecule - Whether the model learned chemically meaningful patterns - Debugging: Are attention patterns reasonable? Example insights: - High attention on C=O bonds for carbonyl chemistry - Focus on aromatic rings for \u03c0-stacking predictions - Attention following conjugation pathways 3. Parallelizability All attention coefficients for all edges can be computed in parallel: # Pseudo-code edge_features = concat ( h [ edges [:, 0 ]], h [ edges [:, 1 ]]) # All edges at once attention_logits = attention_network ( edge_features ) # Parallel attention_weights = softmax_per_node ( attention_logits ) # Parallel within neighbors Speed advantages: - GPU-friendly (matrix operations) - Scales well to large molecules - No sequential dependencies (unlike RNNs) 4. Inductive Learning GATs can generalize to completely new graph structures: - Train on small molecules, test on large ones - Learn patterns that transfer across different molecular classes - No fixed graph structure required during training This is critical for: - Generalizing to novel drug candidates - Transfer learning across datasets - Handling molecules with varying sizes","title":"Advantages of GATs"},{"location":"day3/#gat__variants__and__extensions","text":"GATv2: More Expressive Attention Original GAT limitation: Attention is somewhat static # GAT: Transform then attend e_{ij} = a^T [W h_i || W h_j] # a can only linearly combine features GATv2 improvement: Dynamic attention # GATv2: Attend then transform e_{ij} = a^T LeakyReLU(W [h_i || h_j]) # Non-linearity before attention Why it\u2019s better: - The non-linearity is applied AFTER concatenation - Allows more complex attention functions - Empirically: 10-30% better performance on many benchmarks - Fixes theoretical expressivity limitations of original GAT When to use GATv2: - Complex molecules with subtle structural differences - When original GAT plateaus in performance - Tasks requiring fine-grained attention distinctions Molecular GAT: Edge Features Challenge: Chemical bonds have important attributes (single/double/triple, stereochemistry) that basic GAT ignores. Solution : Incorporate edge features into attention e_{ij} = a^T [W h_i || W h_j || E e_{ij}] Where: - e_{ij} = edge feature vector (bond type, distance, ring membership) - E = edge embedding matrix - Now attention depends on both node features AND edge features Applications: - Distinguishing single vs double bonds - Incorporating 3D distances - Using bond order information - Representing stereochemistry Example : In conjugated systems, \u03c0-bonds should have higher attention than \u03c3-bonds: # Single bond: e_{ij} = [1,0,0] \u2192 lower attention # Double bond: e_{ij} = [0,1,0] \u2192 higher attention (for delocalization) # Triple bond: e_{ij} = [0,0,1] \u2192 highest attention Graph Transformer Networks: Extension to full graph attention (not just neighbors): \u03b1_{ij} for all pairs (i,j) in graph # Not just edges Trade-offs: - Pro : Can capture long-range interactions - Pro : More flexible attention patterns - Con : O(|V|\u00b2) complexity (vs O(|E|) for GAT) - Con : May lose graph structural bias Use when: Long-range interactions matter (large molecules, proteins)","title":"GAT Variants and Extensions"},{"location":"day3/#practical__tips__for__using__gats","text":"Hyperparameter Selection: # Good starting points num_layers = 3 - 5 # Deeper than this risks over-smoothing num_heads = 4 - 8 # More heads for complex tasks hidden_dim = 64 - 128 per head dropout = 0.1 - 0.3 # Higher dropout for small datasets learning_rate = 0.001 # Adam optimizer # For QM9 dataset config = { 'num_layers' : 4 , 'num_heads' : 4 , 'hidden_dim' : 128 , 'dropout' : 0.2 , 'attention_dropout' : 0.1 # Separate dropout on attention weights } Common Pitfalls: Forgetting self-loops : Add explicit (i,i) edges or include h_i in aggregation Over-smoothing : Too many layers \u2192 all nodes become similar Attention collapse : All attention goes to one neighbor (use attention dropout) Memory issues : K heads \u00d7 L layers can use lots of GPU memory Best Practices: # 1. Add residual connections h_i ' = h_i + GAT(h_i) # Prevents over-smoothing # 2. Use layer normalization h_i ' = LayerNorm(h_i + GAT(h_i)) # Stabilizes training # 3. Attention dropout \u03b1_ { ij } = Dropout ( softmax ( e_ { ij })) # Regularizes attention # 4. Edge features when available e_ { ij } = [ bond_type , distance , ring_membership ] # Richer edges Visualization Strategies: # 1. Attention heatmaps plot_attention_matrix ( attention_weights , molecule ) # 2. Highlight important edges highlight_edges_above_threshold ( molecule , attention_weights > 0.3 ) # 3. Track attention across layers for layer in model . layers : visualize_attention_distribution ( layer . attention ) # 4. Compare heads for head in range ( num_heads ): visualize_attention_head ( head , attention_weights )","title":"Practical Tips for Using GATs"},{"location":"day3/#3__equivariant__networks__for__3d__structures","text":"Geometric deep learning architectures respect the symmetries and geometric properties of 3D molecular structures, making them particularly powerful for computational chemistry and materials science. These networks go beyond simple graph connectivity to leverage the full 3D geometry of molecules.","title":"3. Equivariant Networks for 3D Structures"},{"location":"day3/#motivation__why__geometry__matters","text":"The 3D Problem: Traditional GNNs treat molecular graphs as topological structures, ignoring spatial arrangements: - Two molecules with the same connectivity but different 3D shapes (stereoisomers) would be treated identically - Bond angles and dihedral angles contain crucial information - 3D distance-based interactions (van der Waals, electrostatics) are not captured - Forces and other vector properties require 3D information Example : Consider two stereoisomers: cis-2-butene: H\u2083C-CH=CH-CH\u2083 (groups on same side) trans-2-butene: H\u2083C-CH=CH-CH\u2083 (groups on opposite sides) These have: - Identical graph connectivity - Different 3D structures - Different physical properties (melting point, boiling point, reactivity) A topology-only GNN cannot distinguish them, but a geometry-aware network can.","title":"Motivation: Why Geometry Matters"},{"location":"day3/#understanding__symmetries","text":"Physical Symmetries in Molecular Systems: Molecular properties must respect fundamental physical symmetries: Translation Invariance : Property(molecule) = Property(molecule + translation vector) Moving the entire molecule in space doesn\u2019t change its energy or properties Only relative positions matter, not absolute coordinates Mathematically: E(R + t) = E(R) for any translation t Rotation Invariance : Property(molecule) = Property(rotate(molecule, \u03b8)) Rotating the molecule doesn\u2019t change scalar properties (energy, dipole magnitude) Physical measurements don\u2019t depend on orientation in space Mathematically: E(QR) = E(R) for any rotation matrix Q Permutation Invariance : Property(atoms[1,2,3,...]) = Property(atoms[permutation]) Labeling atoms 1,2,3 vs 3,1,2 shouldn\u2019t change properties Physical reality has no preferred ordering Node ordering is an artifact of representation Reflection (for some properties) : Chiral molecules break reflection symmetry Achiral molecules maintain E(R) = E(mirror(R)) Invariance vs Equivariance: Invariant : Output doesn\u2019t change under transformation Example: Energy is rotation-invariant scalar E(rotate(molecule)) = E(molecule) Equivariant : Output transforms consistently with input Example: Forces are rotation-equivariant vectors F(rotate(molecule)) = rotate(F(molecule)) If you rotate the molecule, forces rotate the same way Why This Matters: Networks that violate these symmetries will: - Learn to recognize rotated versions as different molecules (inefficient) - Require much more training data to learn all orientations - Fail to generalize to new orientations - Predict unphysical results (energy changing with rotation) Networks that respect symmetries: - Are more data-efficient (one orientation teaches all) - Generalize better to new configurations - Satisfy physical laws by construction - Often achieve better accuracy with fewer parameters","title":"Understanding Symmetries"},{"location":"day3/#schnet__continuous-filter__convolutional__neural__network","text":"SchNet pioneered the use of continuous convolutions for molecular modeling, treating molecules as continuous 3D objects rather than discrete graphs. Core Philosophy: Instead of learning fixed filters for discrete bond types, SchNet learns continuous functions that depend smoothly on interatomic distances. This mirrors physics: interactions depend on distance in a continuous way (not discrete jumps). Architecture Overview: Input: - Atomic numbers Z = [Z\u2081, Z\u2082, ..., Z_N] - 3D coordinates R = [r\u2081, r\u2082, ..., r_N] Output: - Molecular property (energy, HOMO, etc.) Step 1: Atomic Embeddings Each atom type is embedded into a feature space: x_i^(0) = Embedding(Z_i) # Lookup table: atomic number \u2192 d-dimensional vector For example: - Carbon (Z=6) \u2192 [0.12, -0.34, 0.56, \u2026] - Nitrogen (Z=7) \u2192 [0.08, -0.29, 0.61, \u2026] - Oxygen (Z=8) \u2192 [0.15, -0.41, 0.48, \u2026] Step 2: Continuous Filter Generation The innovation of SchNet: filters are functions of distance, not learned for discrete bins. # Compute all pairwise distances d_ij = ||r_i - r_j|| # Euclidean distance # Expand distances using radial basis functions (RBFs) e_ij = [exp(-(d_ij - \u03bc_k)\u00b2 / \u03c3\u00b2) for k in range(K)] # Generate filter weights from expanded distances W_ij = MLP(e_ij) # Neural network: \u211d^K \u2192 \u211d^(d\u00d7d) Radial Basis Functions (RBFs): RBFs create a smooth representation of distances: # Example: Gaussian RBFs centered at different distances \u03bc = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0, ...] # Centers in \u00c5ngstr\u00f6ms \u03c3 = 0.1 # Width For distance d_ij = 1.8 \u00c5: RBF\u2081(1.8) = exp(-(1.8-0.5)\u00b2/0.01) \u2248 0.00 # Far from center RBF\u2082(1.8) = exp(-(1.8-1.0)\u00b2/0.01) \u2248 0.00 RBF\u2083(1.8) = exp(-(1.8-1.5)\u00b2/0.01) \u2248 0.74 # Close to center RBF\u2084(1.8) = exp(-(1.8-2.0)\u00b2/0.01) \u2248 0.82 # Closest center ... This creates a smooth, continuous representation that: - Captures nuances between bond lengths - Allows interpolation to unseen distances - Provides smooth gradients for optimization Step 3: Interaction Blocks (Message Passing) # For each interaction layer t: x_i^(t+1) = x_i^(t) + \u03a3_{j\u2260i} x_j^(t) \u2299 W_ij^(t) Where: \u2299 is element-wise multiplication (Hadamard product) W_ij^(t) = FilterNetwork_t(d_ij) is the distance-dependent filter Detailed Breakdown: Cutoff Function : Only interact with nearby atoms if d_ij > r_cutoff (typically 5-10 \u00c5): W_ij = 0 # No interaction else: W_ij = FilterNetwork(d_ij) \u00d7 smooth_cutoff(d_ij) Smooth cutoff prevents discontinuities: f_cutoff(d) = 0.5 \u00d7 [cos(\u03c0 \u00d7 d / r_cutoff) + 1] if d < r_cutoff 0 otherwise Filter Network Architecture : e = RBF_expansion(d_ij) # [K] vector h = Dense(e) # [K] \u2192 [hidden_dim] h = ShiftedSoftplus(h) # Smooth non-linearity W = Dense(h) # [hidden_dim] \u2192 [d\u00d7d] W = W \u00d7 f_cutoff(d_ij) # Apply cutoff Atom-wise Update : # Aggregate filtered neighbor features m_i = \u03a3_{j\u2208neighbors(i)} x_j \u2299 W_ij # Update with residual connection x_i = x_i + m_i Step 4: Output Modules After T interaction blocks: # Atom-wise energy contributions E_i = MLP_atom(x_i^(T)) # Scalar per atom # Sum for total energy (size-extensive property) E_total = \u03a3_i E_i # Or average for intensive properties Property = (1/N) \u03a3_i MLP(x_i^(T)) Key Design Choices: Only distances, not coordinates : Distances are rotation and translation invariant ||r_i - r_j|| is the same regardless of overall position/orientation This guarantees the network is invariant Continuous filters : Smoothly adapts to any distance Better than discrete binning (1.0-1.5 \u00c5, 1.5-2.0 \u00c5, \u2026) Allows accurate interpolation Shifted Softplus activation : ShiftedSoftplus(x) = log(0.5 \u00d7 exp(x) + 0.5) Smooth everywhere (unlike ReLU) Important for force prediction: forces = -\u2207E Provides smooth gradients Strengths: - Efficient : Linear in number of atoms (with cutoff) - Scalable : Handles molecules from 10 to 1000+ atoms - Accurate : State-of-the-art on QM9 and other benchmarks - End-to-end differentiable : Can predict forces via backpropagation - Physically motivated : Design mirrors physics of interactions Limitations: - Distance-only : Doesn\u2019t explicitly capture angles - Isotropic : Treats all directions equally (no directionality) - Cannot predict vector properties directly : Forces require gradients Implementation Notes: # Typical hyperparameters num_interactions = 6 # Depth of network num_gaussians = 50 # Number of RBF centers cutoff = 5.0 # Interaction cutoff (\u00c5) hidden_dim = 128 # Feature dimension num_filters = 128 # Filter network dimension","title":"SchNet (Continuous-filter Convolutional Neural Network)"},{"location":"day3/#dimenet__directional__message__passing__neural__network","text":"DimeNet extends SchNet by incorporating angular information, making it significantly more expressive for molecular modeling. Key Innovation: Beyond Distances While distances capture bond lengths, many properties depend on: - Bond angles : H-O-H angle in water determines properties - Dihedral angles : Rotations around single bonds affect conformation - Triplet interactions : Three-body terms in force fields Physics Analogy : - SchNet \u2248 Lennard-Jones potential (pairwise, distance-only) - DimeNet \u2248 Force fields with angle terms (CHARMM, AMBER) Architecture: Directional Messages DimeNet introduces messages that depend on triplets of atoms (i,j,k): k / /\u03b8_{ijk} / j -------- i d_jk d_ij Step 1: Distance and Angle Embeddings # Distance embedding (like SchNet) e_dist(d_ij) = RBF_expansion(d_ij) # NEW: Angle embedding \u03b8_{ijk} = angle between vectors (r_j - r_i) and (r_k - r_j) e_angle(\u03b8_{ijk}) = SphericalBasisFunctions(\u03b8_{ijk}) Spherical Basis Functions: Instead of Gaussians (for distances), use spherical harmonics for angles: # Angles are periodic: 0\u00b0 = 360\u00b0 # Use basis that respects this periodicity SBF_k(\u03b8) = \u03a3_n c_{nk} \u00d7 exp(-(n - n_0)\u00b2/\u03c3\u00b2) \u00d7 sin(n\u03b8) # These form a complete basis for representing angular functions Step 2: Directional Message Passing The crucial innovation - messages depend on geometric triplets: # For each atom i: m_ij = \u03a3_{k\u2208N(j), k\u2260i} MessageBlock(d_ij, d_jk, \u03b8_{ijk}) \u2299 x_k Let\u2019s break down MessageBlock(d_ij, d_jk, \u03b8_{ijk}) : def MessageBlock ( d_ij , d_jk , theta_ijk ): # Embed distances rbf_ij = RBF ( d_ij ) # [n_rbf] rbf_jk = RBF ( d_jk ) # [n_rbf] # Embed angle sbf = SphericalBasis ( theta_ijk ) # [n_sbf] # Combine using bilinear layer # This learns correlations between distances and angles W = BilinearLayer ( rbf_ij , rbf_jk , sbf ) # [d \u00d7 d] matrix return W Bilinear Layer Explained: The bilinear layer is crucial for combining geometric features: W = \u03a3_m \u03a3_n \u03a3_l U_{mnl} \u00d7 rbf_ij[m] \u00d7 rbf_jk[n] \u00d7 sbf[l] Where U is a learned tensor of parameters This allows the network to learn patterns like: - \u201cWhen d_ij \u2248 1.5 \u00c5 AND d_jk \u2248 1.2 \u00c5 AND \u03b8 \u2248 120\u00b0\u201d \u2192 strong interaction - Captures correlations between geometric features - More expressive than treating features independently Step 3: Update with Directional Information # Aggregate directional messages m_i = \u03a3_{j\u2208N(i)} m_ij # Update node features x_i = Update(x_i, m_i) # Typically a residual network Why This Works Better: Angular Information : Distinguishes linear vs bent vs tetrahedral Example: CO\u2082 (linear, 180\u00b0) vs H\u2082O (bent, 104.5\u00b0) Same atom types, different angles \u2192 different properties Dihedral Sensitivity : Captures rotational barriers Ethane: staggered vs eclipsed conformations Different angles \u2192 different energies Three-Body Interactions : More realistic physics Many quantum effects involve three atoms Necessary for accurate force fields Maintaining Rotational Invariance: Despite using angles, DimeNet remains rotationally invariant because: - Angles are invariant: \u03b8_{ijk} doesn\u2019t change under rotation - Only distances and angles used (not absolute coordinates) - No preferred orientation in space DimeNet++ Improvements: The original DimeNet was slow. DimeNet++ optimized: # DimeNet: T-shaped message passing Message: k \u2192 j \u2192 i (sequential) # DimeNet++: Optimized message aggregation Message: All k \u2192 all j \u2192 all i (more parallel) Optimization strategies: 1. Shared bilinear layers : Reduce parameters 2. Efficient triplet enumeration : Better data structures 3. Grouped convolutions : Reduce computational cost 4. Memory-efficient attention : Lower memory footprint Performance: - DimeNet++: 2-5\u00d7 faster than DimeNet - Same or better accuracy - Can handle larger molecules (100+ atoms) Advantages: - Higher accuracy : 20-40% better MAE on QM9 vs SchNet - Captures geometry : Angles and dihedrals encoded - Physics-aware : Mirrors force field design - Still rotationally invariant : Maintains symmetries Disadvantages: - Computational cost : O(N \u00d7 k\u00b2) where k is coordination number - Memory : Stores triplets, not just pairs - Complexity : More hyperparameters to tune Use Cases: - High-accuracy property prediction - Conformational energy differences - Transition state geometries - Systems where angles matter (chelates, rings, etc.)","title":"DimeNet (Directional Message Passing Neural Network)"},{"location":"day3/#painn__polarizable__atom__interaction__neural__network","text":"PaiNN represents a paradigm shift: instead of just invariant features, it maintains both scalar (invariant) and vector (equivariant) representations. Motivation: Vector Properties Many important properties are vectors (have direction): - Forces : F = -\u2207E (gradient of energy) - Dipole moments : \u03bc = \u03a3_i q_i r_i - Magnetic moments : Direction matters - Polarizability : Tensorial response to fields Previous models (SchNet, DimeNet): - Can only predict scalar outputs directly - Forces require numerical differentiation: F = -dE/dR - Inefficient and sometimes inaccurate PaiNN: - Predicts vectors directly - Learns force fields end-to-end - Truly equivariant architecture Equivariance Explained: For a rotation matrix Q: Invariant (scalar): f(QR) = f(R) Example: ||v|| = ||Qv|| (length unchanged) Equivariant (vector): f(QR) = Q f(R) Example: Qv rotates the same way as input Equivariant (rank-2 tensor): f(QR) = Q f(R) Q^T Example: Stress tensor transforms Feature Representation: Each atom i has TWO types of features: Scalar features s_i \u2208 \u211d^d: Rotation invariant Examples: atomic charge, energy contribution, electron density Transforms: s_i \u2192 s_i (unchanged under rotation) Vector features v_i \u2208 \u211d^(d\u00d73): Rotation equivariant Examples: dipole moment, force vector, polarization Transforms: v_i \u2192 Q v_i (rotates with molecule) Architecture Overview: Input: (s_i^(0), v_i^(0)) for each atom s_i^(0) = embedding(Z_i) # Initial: just element type v_i^(0) = 0 # Initial: no directional info Message Passing Layers: (s_i, v_i) \u2192 MessagePass \u2192 (s_i', v_i') Output: Scalar: energy = \u03a3_i MLP(s_i^(T)) Vector: forces = \u03a3_i v_i^(T) # Or per-atom force contribution Message Passing in PaiNN: Each layer consists of three parts: Part 1: Scalar Message Passing # Compute scalar messages from neighbors for j in neighbors ( i ): d_ij = || r_j - r_i || # Distance dir_ij = ( r_j - r_i ) / d_ij # Unit direction vector # Filter based on distance \u03c6_ij = FilterNetwork ( d_ij ) # Like SchNet # Scalar message (rotation invariant) m_s_ij = \u03c6_ij \u2299 s_j # Also incorporate magnitude of vector features m_s_ij += \u03c6_ij \u2299 || v_j || \u00b2 # Invariant: length squared # Aggregate m_s_i = \u03a3_j m_s_ij # Update scalars s_i = s_i + MLP ( m_s_i ) Part 2: Vector Message Passing This is where equivariance happens: # Compute vector messages for j in neighbors ( i ): d_ij = || r_j - r_i || dir_ij = ( r_j - r_i ) / d_ij # \u2190 KEY: Direction vector \u03c6_ij = FilterNetwork ( d_ij ) # Vector message (rotation equivariant!) # Multiply by direction to make equivariant m_v_ij = \u03c6_ij \u2299 v_j # Element-wise filter m_v_ij += ( \u03c6_ij \u2299 s_j ) \u00d7 dir_ij # Scalar-to-vector term # Aggregate vectors m_v_i = \u03a3_j m_v_ij # Update vectors v_i = v_i + m_v_i Why this is equivariant: Under rotation Q: dir_ij \u2192 Q dir_ij (direction rotates) v_j \u2192 Q v_j (vector features rotate) m_v_ij = \u03c6_ij \u2299 v_j + (\u03c6_ij \u2299 s_j) \u00d7 dir_ij \u2192 \u03c6_ij \u2299 (Q v_j) + (\u03c6_ij \u2299 s_j) \u00d7 (Q dir_ij) = Q [\u03c6_ij \u2299 v_j + (\u03c6_ij \u2299 s_j) \u00d7 dir_ij] = Q m_v_ij \u2713 Part 3: Mixing Scalars and Vectors Cross-interactions between scalar and vector features: # Vector \u2192 Scalar: Extract invariant info from vectors s_i = s_i + MLP ( || v_i || ) # Length is invariant s_i = s_i + MLP ( v_i \u00b7 v_i ) # Dot product is invariant # Scalar \u2192 Vector: Modulate vectors by scalars v_i = v_i \u2299 \u03c3 ( U s_i ) # Element-wise gating Complete Update Equations: # Scalar update \u0394s_i = \u03a3_j [W_s(d_ij) \u2299 s_j + W_vs(d_ij) \u2299 ||v_j||\u00b2] s_i = s_i + MLP(\u0394s_i + ||v_i||\u00b2) # Vector update \u0394v_i = \u03a3_j [W_v(d_ij) \u2299 v_j + W_sv(d_ij) \u2299 s_j \u2299 (r_j - r_i) / d_ij] v_i = v_i + \u0394v_i # Mix scalar and vector s_i = s_i + U_vs ||v_i|| v_i = (W_vv v_i) \u2299 \u03c3(U_sv s_i) Output Predictions: # Energy (scalar invariant) E_i = MLP_scalar ( s_i ) E_total = \u03a3_i E_i # Forces (vector equivariant) F_i = v_i # Already in correct format! # Or: F_i = Linear(v_i) for learned scaling # Other vector properties dipole = \u03a3_i q_i \u00d7 v_i # If q_i are charges Advantages of PaiNN: Direct vector prediction : Forces without numerical differentiation More accurate and efficient Can predict multiple vector properties True equivariance : Guarantees physical consistency Rotated inputs \u2192 correctly rotated outputs No violation of physics Expressive representations : Vectors encode directional information Richer than scalar-only features Better for anisotropic systems Force field learning : Can be trained on forces directly Learns better potential energy surfaces Useful for molecular dynamics Training Considerations: # Loss function combining energy and forces loss = w_E || E_pred - E_true || \u00b2 + w_F || F_pred - F_true || \u00b2 # Typical weights w_E = 1.0 # Energy in eV or kcal/mol w_F = 100.0 # Forces in eV/\u00c5 (forces are smaller, need higher weight) Applications: Molecular Dynamics : Learn accurate force fields from DFT 1000\u00d7 faster than ab initio MD Maintains accuracy of quantum calculations Transition State Search : Accurate forces guide optimization Find saddle points efficiently Predict reaction barriers Dipole Moment Prediction : Important for spectroscopy Drug-like properties Solvent effects Polarizability : Response to external fields Optical properties Intermolecular interactions Implementation Notes: # Hyperparameters num_layers = 5 hidden_dim_scalar = 128 hidden_dim_vector = 64 # Vectors have 3\u00d7 more parameters (x,y,z) num_rbf = 20 cutoff = 5.0 # Vector features typically smaller dimension to save memory # v_i \u2208 \u211d^(d\u00d73) uses 3\u00d7 memory of s_i \u2208 \u211d^d","title":"PaiNN (Polarizable Atom Interaction Neural Network)"},{"location":"day3/#comparison__of__approaches","text":"Model Distance Angles Equivariance Outputs Complexity Use Case SchNet \u2713 \u2717 Invariant Scalars O(N\u00d7k) Fast, general purpose, good baseline DimeNet \u2713 \u2713 Invariant Scalars O(N\u00d7k\u00b2) High accuracy, angle-dependent properties DimeNet++ \u2713 \u2713 Invariant Scalars O(N\u00d7k\u00b2)* DimeNet with 3-5\u00d7 speedup PaiNN \u2713 Implicit Equivariant Scalars + Vectors O(N\u00d7k) Forces, vector properties, MD *DimeNet++ optimized but same computational complexity When to Choose Each: SchNet: - Need fast inference - Large molecules (>100 atoms) - Don\u2019t need highest accuracy - Conformational search (many evaluations) - X Highly angle-dependent properties - X Need force predictions DimeNet/DimeNet++: - Need highest accuracy - Angle and dihedral effects important - Small to medium molecules (<50 atoms) - Property prediction only - X Computational budget limited - X Need force predictions - X Very large molecules PaiNN: - Need force predictions - Molecular dynamics simulations - Vector property prediction - Want equivariant representations - Good accuracy/speed trade-off - X Only need scalar properties - X Maximum simplicity desired Practical Decision Tree: Do you need vector outputs (forces, dipoles)? \u251c\u2500 Yes \u2192 Use PaiNN \u2514\u2500 No \u2502 \u2514\u2500 Is accuracy critical and dataset small? \u251c\u2500 Yes \u2192 Use DimeNet++ \u2514\u2500 No \u2192 Use SchNet (fastest) Benchmarks (QM9 dataset, HOMO energy): Method MAE (meV) Time/molecule Parameters SchNet 41 0.5 ms 600K DimeNet 33 3.0 ms 2M DimeNet++ 29 1.2 ms 2M PaiNN 35 0.8 ms 800K Note: Exact numbers vary by implementation and hardware","title":"Comparison of Approaches"},{"location":"day3/#4__protein-ligand__interaction__modeling","text":"Understanding how small molecules (ligands) bind to proteins is crucial for drug discovery. GNNs provide powerful tools for modeling these complex interactions, potentially accelerating the drug development pipeline from years to months.","title":"4. Protein-Ligand Interaction Modeling"},{"location":"day3/#the__drug__discovery__challenge","text":"Traditional Drug Discovery: Target Identification : Identify disease-related protein (months-years) Hit Discovery : Screen 10\u2075-10\u2076 compounds experimentally (months, $$$) Lead Optimization : Iteratively improve binding (years, $$$$) Clinical Trials : Test in humans (years, $$$$$) Total : 10-15 years, $1-2 billion per drug AI-Accelerated Discovery: GNNs can predict binding without experiments: - Virtual screening: 10\u2076 compounds in hours - Structure-based optimization - Reduced experimental testing - Faster iteration cycles Impact : Several AI-discovered drugs now in clinical trials","title":"The Drug Discovery Challenge"},{"location":"day3/#problem__formulation","text":"Key Tasks: Binding Affinity Prediction : Predict the strength of protein-ligand binding Metrics: IC\u2085\u2080, Ki, Kd, \u0394G_bind Range: nM (strong) to mM (weak) Applications: Virtual screening, lead optimization Binding Pose Prediction : Determine the 3D orientation of ligand in binding pocket Must satisfy spatial constraints Account for protein flexibility Applications: Structure-based drug design Virtual Screening : Rank large libraries of compounds Prioritize for experimental testing Requires fast inference (<1s per compound) Applications: Hit discovery, library filtering Selectivity Prediction : Binding to target vs off-targets Crucial for drug safety Multi-protein modeling Applications: Toxicity prediction, side effect profiling Input Data: Protein: - Sequence: MKTAYIAKQRQ... (amino acid sequence) - Structure: 3D coordinates of atoms or residues - Features: Secondary structure, surface accessibility, physicochemical properties Ligand: - SMILES: CC(C)CC1=CC=C(C=C1)C(C)C(=O)O (structure string) - 3D Conformation: Atomic coordinates - Features: Atom types, charges, pharmacophore points Complex: - Binding pose (if known from X-ray crystallography) - Interaction types (H-bonds, hydrophobic, \u03c0-stacking)","title":"Problem Formulation"},{"location":"day3/#representation__strategies","text":"Challenge : How to represent a protein-ligand complex as a graph? Strategy 1: Separate Protein and Ligand Graphs [Protein Graph] [Ligand Graph] Nodes: Residues Nodes: Atoms Edges: Contacts Edges: Bonds \u2193 \u2193 [GNN_prot] [GNN_lig] \u2193 \u2193 h_protein h_ligand \u2514\u2500\u2500\u2500\u2500\u2500\u2192 [Concatenate] \u2190\u2500\u2500\u2500\u2500\u2500\u2518 \u2193 [MLP head] \u2193 Affinity Protein Graph Construction: # Option A: Residue-level (coarse-grained) for residue in protein . residues : node_features = [ residue . amino_acid_type , # One-hot: 20 amino acids residue . secondary_structure , # Helix, sheet, coil residue . surface_accessibility , residue . charge , residue . hydrophobicity ] # Edges: spatial proximity for i , j in combinations ( residues , 2 ): if distance ( i . CA , j . CA ) < 10.0 : # C-alpha distance add_edge ( i , j , distance = distance ( i . CA , j . CA )) # Option B: Atom-level (fine-grained) for atom in protein . atoms : node_features = [ atom . element , atom . charge , atom . in_backbone , atom . in_sidechain ] Ligand Graph Construction: # Atoms as nodes for atom in ligand . atoms : node_features = [ atom . atomic_number , atom . formal_charge , atom . hybridization , # sp, sp2, sp3 atom . is_aromatic , atom . num_hydrogens , atom . degree , atom . chirality ] # Bonds as edges for bond in ligand . bonds : edge_features = [ bond . bond_type , # Single, double, triple, aromatic bond . is_conjugated , bond . is_in_ring , bond . stereo # E/Z, cis/trans ] Pros: - Simple architecture - Can pre-train on protein/ligand datasets separately - Modular: easy to swap GNN architectures Cons: - No explicit inter-molecular interactions - Late fusion may miss important binding details - Less interpretable (black box combination) Strategy 2: Joint Protein-Ligand Interaction Graph Protein nodes + Ligand nodes \u2193 Intra-molecular edges (protein bonds, ligand bonds) + Inter-molecular edges (binding interactions) \u2193 [Joint GNN] \u2193 [Global pooling] \u2193 Affinity Interaction Graph Construction: # Combine protein and ligand into one graph G = Graph () # Add protein nodes G . add_nodes ( protein_atoms , type = 'protein' ) # Add ligand nodes G . add_nodes ( ligand_atoms , type = 'ligand' ) # Add intra-molecular edges G . add_edges ( protein_bonds ) G . add_edges ( ligand_bonds ) # Add inter-molecular edges (KEY!) for p_atom in protein_atoms : for l_atom in ligand_atoms : dist = distance ( p_atom , l_atom ) if dist < 5.0 : # Interaction cutoff interaction_type = classify_interaction ( p_atom , l_atom , dist ) G . add_edge ( p_atom , l_atom , distance = dist , interaction = interaction_type ) Interaction Types: Classify inter-molecular edges by interaction: def classify_interaction ( p_atom , l_atom , distance ): interactions = [] # Hydrogen bond if is_h_bond_donor ( p_atom ) and is_h_bond_acceptor ( l_atom ): if distance < 3.5 and angle_ok : interactions . append ( 'H-bond' ) # Hydrophobic if is_hydrophobic ( p_atom ) and is_hydrophobic ( l_atom ): if distance < 4.5 : interactions . append ( 'hydrophobic' ) # Pi-stacking if is_aromatic ( p_atom ) and is_aromatic ( l_atom ): if 3.5 < distance < 4.5 : interactions . append ( 'pi-stacking' ) # Electrostatic if charge ( p_atom ) * charge ( l_atom ) < 0 : # Opposite charges interactions . append ( 'electrostatic' ) # Salt bridge if is_charged_residue ( p_atom ) and is_charged_group ( l_atom ): if distance < 4.0 : interactions . append ( 'salt-bridge' ) return interactions Pros: - Explicit interaction modeling - Message passing directly between protein and ligand - More interpretable (can visualize key interactions) - Better captures binding geometry Cons: - Larger graphs (more nodes and edges) - Requires 3D structure (binding pose) - More complex to implement Strategy 3: Attention-Based Cross-Attention [Protein GNN] \u2192 h_protein_nodes [Ligand GNN] \u2192 h_ligand_nodes \u2193 \u2193 [Cross-Attention Layer] \u2193 Attended features \u2193 [Prediction head] Cross-Attention Mechanism: # Protein attends to ligand for p_node in protein_nodes : # Compute attention to all ligand nodes attention = [] for l_node in ligand_nodes : # Attention score based on features and geometry score = attention_function ( h_protein [ p_node ], h_ligand [ l_node ], distance ( p_node , l_node ) ) attention . append ( score ) # Softmax normalize attention = softmax ( attention ) # Attended ligand features h_protein [ p_node ] += weighted_sum ( attention , h_ligand ) # Ligand attends to protein (symmetric) for l_node in ligand_nodes : # Similar process in reverse ... Attention Function: def attention_function ( h_p , h_l , distance ): # Feature similarity feat_sim = dot_product ( W_p @ h_p , W_l @ h_l ) # Distance penalty (closer = more attention) dist_weight = exp ( - distance / sigma ) # Combined score score = feat_sim * dist_weight return score Pros: - Learns which protein-ligand pairs interact - Flexible: works without predefined interaction edges - Can handle multiple binding modes - Interpretable attention weights Cons: - O(N_protein \u00d7 N_ligand) complexity - May overfit on small datasets - Requires careful regularization","title":"Representation Strategies"},{"location":"day3/#architecture__patterns","text":"Pattern 1: Separate Encoding with Late Fusion class SeparateFusionModel ( nn . Module ): def __init__ ( self ): self . protein_gnn = GNN ( protein_features , hidden_dim ) self . ligand_gnn = GNN ( ligand_features , hidden_dim ) self . fusion_mlp = MLP ( 2 * hidden_dim , output_dim ) def forward ( self , protein_graph , ligand_graph ): # Encode separately h_prot = self . protein_gnn ( protein_graph ) h_lig = self . ligand_gnn ( ligand_graph ) # Global pooling z_prot = global_mean_pool ( h_prot , protein_graph . batch ) z_lig = global_mean_pool ( h_lig , ligand_graph . batch ) # Concatenate and predict z = torch . cat ([ z_prot , z_lig ], dim =- 1 ) affinity = self . fusion_mlp ( z ) return affinity Pattern 2: Joint Encoding class JointGraphModel ( nn . Module ): def __init__ ( self ): self . gnn = GNN ( node_features , hidden_dim , num_layers = 5 ) self . interaction_embedding = nn . Embedding ( num_interactions , edge_dim ) self . readout = Set2Set ( hidden_dim ) self . predictor = MLP ( hidden_dim , 1 ) def forward ( self , complex_graph ): # Embed interactions edge_attr = self . interaction_embedding ( complex_graph . edge_type ) # Joint message passing h = self . gnn ( complex_graph . x , complex_graph . edge_index , edge_attr ) # Global pooling z = self . readout ( h , complex_graph . batch ) # Predict affinity affinity = self . predictor ( z ) return affinity Pattern 3: Cross-Attention class CrossAttentionModel ( nn . Module ): def __init__ ( self ): self . protein_encoder = GNN ( protein_features , hidden_dim ) self . ligand_encoder = GNN ( ligand_features , hidden_dim ) self . cross_attention = CrossAttentionLayer ( hidden_dim ) self . predictor = MLP ( hidden_dim , 1 ) def forward ( self , protein_graph , ligand_graph , distances ): # Encode separately h_prot = self . protein_encoder ( protein_graph ) h_lig = self . ligand_encoder ( ligand_graph ) # Cross-attention h_prot_updated , h_lig_updated = self . cross_attention ( h_prot , h_lig , distances ) # Pool both z_prot = global_attention_pool ( h_prot_updated ) z_lig = global_attention_pool ( h_lig_updated ) # Predict from combined representation affinity = self . predictor ( z_prot + z_lig ) return affinity","title":"Architecture Patterns"},{"location":"day3/#key__considerations","text":"1. Geometric Information 3D coordinates are essential for accurate binding prediction: # Distance features (crucial!) edge_features = [] for edge in edges : i , j = edge d = distance ( coords [ i ], coords [ j ]) # Distance encoding rbf = gaussian_rbf ( d , centers = [ 1.0 , 2.0 , 3.0 , 4.0 , 5.0 ]) edge_features . append ( rbf ) # Direction (for equivariant models) direction = ( coords [ j ] - coords [ i ]) / d # Use in GNN h = GNN ( nodes , edges , edge_features ) Why geometry matters: - Binding pocket shape determines complementarity - Hydrogen bonds have geometric constraints (distance + angle) - Hydrophobic interactions are distance-dependent - Steric clashes prevent binding 2. Data Challenges Limited Experimental Data: Available binding data: - PDBbind: ~20,000 protein-ligand complexes - ChEMBL: ~2M bioactivity measurements (but many without structures) - BindingDB: ~2M Ki/Kd values Compare to: - ImageNet: 14M images - GPT training: trillions of tokens Challenge: Deep learning typically needs more data Solutions: a) Data Augmentation: # Rotation augmentation for angle in [ 0 , 90 , 180 , 270 ]: rotated_complex = rotate ( complex , angle ) train_on ( rotated_complex ) # Conformational sampling for conf in generate_conformations ( ligand , n = 10 ): augmented_complex = ( protein , conf ) train_on ( augmented_complex ) # Noise injection for noise_level in [ 0.1 , 0.2 ]: noisy_coords = coords + noise_level * random_normal () train_on ( noisy_coords ) b) Transfer Learning: # Pre-train on easier tasks model . pretrain ( task = 'molecular_property_prediction' , dataset = 'QM9' , # Millions of molecules epochs = 100 ) # Fine-tune on binding model . finetune ( task = 'binding_affinity' , dataset = 'PDBbind' , # Thousands of complexes epochs = 50 , learning_rate = 1e-4 # Lower learning rate ) c) Multi-Task Learning: # Learn related tasks simultaneously loss = ( w1 * loss_binding_affinity + w2 * loss_binding_pose + w3 * loss_protein_function + w4 * loss_ligand_properties ) # Shared representations benefit all tasks # More efficient use of limited data Missing Structures: Many bioactivity measurements lack 3D structures: # Sequence-only prediction (when no structure) if protein_structure is None : # Use sequence-based protein representation h_prot = ProteinLanguageModel ( protein_sequence ) else : # Use structure-based GNN h_prot = ProteinGNN ( protein_structure ) 3. Interpretability Understanding WHY a compound binds is as important as predicting IF it binds: Attention Visualization: # Extract attention weights attentions = model . get_attention_weights () # Identify key protein residues important_residues = [] for residue , attention in zip ( residues , attentions ): if attention > threshold : important_residues . append ( residue ) # Visualize in 3D visualize_protein_ligand ( protein , ligand , highlight_residues = important_residues , highlight_atoms = high_attention_ligand_atoms ) Interaction Decomposition: # Analyze contribution of each interaction type for interaction_type in [ 'H-bond' , 'hydrophobic' , 'pi-stack' ]: # Ablation: remove this interaction type affinity_without = model . predict ( complex , exclude_interaction = interaction_type ) contribution = affinity_full - affinity_without print ( f \" { interaction_type } : { contribution : .2f } kcal/mol\" ) Gradient-Based Explanations: # Which atoms matter most? ligand . requires_grad = True affinity = model ( protein , ligand ) affinity . backward () # Atoms with large gradients are important importance = ligand . grad . norm ( dim =- 1 ) visualize_atom_importance ( ligand , importance ) Applications: SAR (Structure-Activity Relationship) : \u201cThis hydrogen bond donor is crucial\u201d \u201cHydrophobic tail can be modified\u201d Guides medicinal chemistry Failure Analysis : \u201cModel focuses on wrong pocket\u201d \u201cMissed key water-mediated H-bond\u201d Improves model training Knowledge Discovery : Identify novel binding motifs Understand selectivity patterns Generate hypotheses for experiments","title":"Key Considerations"},{"location":"day3/#state-of-the-art__models","text":"EquiBind (2021) Key Innovation: SE(3)-equivariant blind docking Architecture: - Separate protein and ligand encoders - Equivariant graph neural network (EGNN) - Predicts keypoint matches - Direct coordinate prediction (no search) Performance: - 38% success rate (<2\u00c5 RMSD) on PDBbind - 1000\u00d7 faster than traditional docking - Fully differentiable GraphDTA / DeepDTA (2018) Key Innovation: End-to-end learning from sequences Architecture: - CNN for protein sequences - GNN for ligand graphs - Concatenation + MLP - Trained on drug-target affinity Performance: - Competitive on Davis and KIBA datasets - Works without 3D structures - Fast inference for virtual screening ATOM3D (2021) Key Innovation: 3D structure benchmarks Datasets: - Protein-ligand binding (PDB) - Protein structure prediction - RNA structure - Molecular dynamics Models: - SchNet, DimeNet applied to biomolecules - Transformers with 3D positional encoding - Graph transformers TANKBind (2023) Key Innovation: Equivariant blind docking with diffusion Architecture: - Diffusion model for pose generation - Equivariant score matching - Iterative refinement - Confidence estimation Performance: - State-of-the-art on blind docking - Handles protein flexibility - Generalizes to unseen proteins","title":"State-of-the-Art Models"},{"location":"day3/#practical__workflow","text":"# 1. Data preparation from biopandas.pdb import PandasPdb # Load protein protein = PandasPdb () . fetch_pdb ( '1a2b' ) protein_graph = protein_to_graph ( protein ) # Load ligand ligand = Chem . MolFromMol2File ( 'ligand.mol2' ) ligand_graph = mol_to_graph ( ligand ) # 2. Model training model = ProteinLigandGNN ( protein_features = 37 , ligand_features = 11 , hidden_dim = 128 , num_layers = 4 ) optimizer = torch . optim . Adam ( model . parameters (), lr = 1e-3 ) for epoch in range ( 100 ): for protein_graph , ligand_graph , affinity in dataloader : pred_affinity = model ( protein_graph , ligand_graph ) loss = F . mse_loss ( pred_affinity , affinity ) optimizer . zero_grad () loss . backward () optimizer . step () # 3. Virtual screening candidates = load_library ( 'drugbank.sdf' ) # 10,000 compounds predictions = [] for ligand in tqdm ( candidates ): ligand_graph = mol_to_graph ( ligand ) affinity = model ( protein_graph , ligand_graph ) predictions . append (( ligand , affinity )) # Sort by predicted affinity predictions . sort ( key = lambda x : x [ 1 ], reverse = True ) # Test top 100 experimentally top_hits = predictions [: 100 ]","title":"Practical Workflow"},{"location":"day3/#future__directions","text":"Physics-Informed Neural Networks : Incorporate electrostatics, solvation Constrain predictions with physical laws Hybrid quantum/classical approaches Generative Models : Generate ligands for target protein Optimize for multiple objectives De novo drug design Allostery and Dynamics : Model protein conformational changes Long-range allosteric effects Molecular dynamics integration Multi-Target Modeling : Selectivity across protein families Polypharmacology Side effect prediction","title":"Future Directions"},{"location":"day3/#5__crystal__structures__for__materials__science","text":"GNNs are revolutionizing materials science by predicting properties of crystalline materials from their atomic structures. This enables high-throughput screening of millions of hypothetical materials, dramatically accelerating materials discovery.","title":"5. Crystal Structures for Materials Science"},{"location":"day3/#understanding__crystalline__materials","text":"What Makes Crystals Different? Unlike molecules, crystals are: - Infinite : Periodic repetition in 3D space - Ordered : Atoms arranged in regular lattices - Defined by symmetry : Space groups and point groups - Bulk properties : Properties of the infinite system, not just a cluster Real-World Impact: Materials with specific properties enable technology: - Batteries : Li-ion conductors (electric vehicles) - Solar cells : High-efficiency photovoltaics - Catalysts : Green chemical production - Semiconductors : Computing and electronics - Superconductors : Lossless power transmission The Discovery Challenge: Possible stable materials: ~10^50 (combinatorial explosion!) Known materials: ~200,000 (Materials Project, ICSD) Fully characterized: ~50,000 Currently used: ~10,000 DFT calculation: 1-1000 CPU-hours per material ML prediction: 0.001 seconds per material Speed-up: 10^6\u00d7 faster!","title":"Understanding Crystalline Materials"},{"location":"day3/#crystal__representation","text":"Unit Cell Description: A crystal is fully specified by: Lattice Vectors : Define the unit cell a\u20d7 = [a_x, a_y, a_z] # First lattice vector b\u20d7 = [b_x, b_y, b_z] # Second lattice vector c\u20d7 = [c_x, c_y, c_z] # Third lattice vector Lattice matrix: L = [a\u20d7 | b\u20d7 | c\u20d7] Lattice Parameters : a, b, c = lengths of vectors (\u00c5ngstr\u00f6ms) \u03b1, \u03b2, \u03b3 = angles between vectors (degrees) Basis : Atomic positions within the unit cell Fractional coordinates: (u, v, w) where 0 \u2264 u,v,w < 1 Cartesian coordinates: r\u20d7 = u\u00b7a\u20d7 + v\u00b7b\u20d7 + w\u00b7c\u20d7 Space Group : Symmetry operations 230 possible space groups in 3D Examples: P21/c (monoclinic), Fm3\u0304m (cubic), P63/mmc (hexagonal) Example: Diamond (Carbon) Lattice: Face-centered cubic (FCC) a = b = c = 3.567 \u00c5 \u03b1 = \u03b2 = \u03b3 = 90\u00b0 Basis: Two carbon atoms at C\u2081: (0, 0, 0) C\u2082: (0.25, 0.25, 0.25) Space group: Fd3\u0304m (227) Infinite crystal: All positions (n\u2081, n\u2082, n\u2083) + basis where n\u2081, n\u2082, n\u2083 \u2208 \u2124","title":"Crystal Representation"},{"location":"day3/#periodic__boundary__conditions","text":"The Periodicity Challenge: For graph construction, we need neighbors, but: - Atoms near cell boundaries have neighbors in adjacent cells - Must account for periodic images - Same atom appears infinitely many times Graph Construction Algorithm: def construct_crystal_graph ( atoms , lattice , cutoff_radius ): \"\"\" Build graph respecting periodic boundaries \"\"\" graph = Graph () # Add nodes for atoms in unit cell for atom in atoms : graph . add_node ( element = atom . element , position = atom . frac_coords , # Fractional coordinates features = get_atom_features ( atom ) ) # Find neighbors using minimum image convention for i , atom_i in enumerate ( atoms ): for j , atom_j in enumerate ( atoms ): # Check all periodic images of atom_j for n1 in [ - 1 , 0 , 1 ]: for n2 in [ - 1 , 0 , 1 ]: for n3 in [ - 1 , 0 , 1 ]: # Skip self-loops (unless different cells) if i == j and ( n1 , n2 , n3 ) == ( 0 , 0 , 0 ): continue # Compute distance through periodic boundaries image_position = atom_j . frac_coords + [ n1 , n2 , n3 ] cart_position = lattice . to_cartesian ( image_position ) distance = np . linalg . norm ( cart_position - lattice . to_cartesian ( atom_i . frac_coords ) ) if distance < cutoff_radius : graph . add_edge ( i , j , distance = distance , cell_offset = ( n1 , n2 , n3 ), # Which periodic image direction = cart_position - atom_i . cart_coords ) return graph Minimum Image Convention: For each pair of atoms, consider all periodic images Choose the closest one (minimum distance) Example: 1D crystal with cell size 10 \u00c5 Atom A at x=1 Atom B at x=9 Direct distance: |9-1| = 8 \u00c5 Through boundary: |9-1-10| = 2 \u00c5 \u2190 MINIMUM (use this!) This handles wrapped distances correctly Challenges: Variable coordination : Atoms may have different numbers of neighbors Long-range order : Some properties depend on distant atoms Supercell construction : May need to replicate unit cell for larger cutoffs","title":"Periodic Boundary Conditions"},{"location":"day3/#property__prediction__tasks","text":"Electronic Properties These determine conducting and optical behavior: 1. Band Gap (E_g) Definition: Energy difference between valence and conduction bands Units: eV Range: 0 (metal) to >10 eV (insulator) Applications: - E_g \u2248 1.3 eV: Solar cells (optimal for sunlight) - E_g > 5 eV: Transparent insulators (windows) - E_g = 0: Metals (wires, electrodes) Prediction challenge: - DFT often underestimates (by 30-50%) - GNNs can learn correction factors 2. Formation Energy (E_f) Definition: Energy to form compound from elements Formula: E_f = E_compound - \u03a3_i n_i \u00d7 E_element(i) Units: eV/atom Applications: - E_f < 0: Thermodynamically stable - E_f > 0.1 eV/atom: Likely unstable - Guides synthesis feasibility Prediction: Critical for materials discovery 3. Energy Above Hull (E_hull) Definition: Energy above stable composition convex hull Measures: Thermodynamic stability relative to competing phases E_hull = 0: On convex hull (thermodynamically stable) E_hull < 0.025 eV/atom: Potentially synthesizable E_hull > 0.1 eV/atom: Very unlikely to be stable Applications: - Virtual materials screening - Stability prediction before synthesis Mechanical Properties Determine material strength and elasticity: 1. Bulk Modulus (B) Definition: Resistance to uniform compression Formula: B = -V (\u2202P/\u2202V)_T Units: GPa Range: 1 GPa (soft) to 400 GPa (diamond) Applications: - High B: Armor, cutting tools - Low B: Flexible substrates, cushioning 2. Shear Modulus (G) Definition: Resistance to shear deformation Units: GPa Applications: - G/B ratio indicates ductility - High G: Stiff materials - Critical for structural applications 3. Elastic Constants (C_ij) Tensor: 6\u00d76 matrix (21 independent components for general case) Symmetry: Reduces components based on crystal system - Cubic: 3 independent constants - Hexagonal: 5 constants - Triclinic: 21 constants Applications: - Full mechanical characterization - Anisotropic behavior prediction Thermodynamic Properties 1. Phonon Properties Frequency spectrum: Vibrational modes Heat capacity: C_v(T) from phonons Thermal expansion: \u03b1(T) Thermal conductivity: \u03ba Challenge: Requires dynamical matrix (expensive!) GNN potential: Fast phonon calculations 2. Free Energy Temperature-dependent stability Phase transitions Chemical potential Applications: - Phase diagram prediction - High-temperature materials","title":"Property Prediction Tasks"},{"location":"day3/#specialized__architectures","text":"CGCNN (Crystal Graph Convolutional Neural Networks) One of the first GNN architectures specifically designed for crystals. Architecture: class CGCNNConv ( nn . Module ): \"\"\"CGCNN convolution layer\"\"\" def __init__ ( self , node_features , edge_features , hidden ): self . node_fc = nn . Linear ( 2 * node_features + edge_features , hidden ) self . bn = nn . BatchNorm1d ( hidden ) def forward ( self , x , edge_index , edge_attr ): # For each edge (i \u2192 j) i , j = edge_index # Concatenate: [h_i || h_j || e_ij] messages = torch . cat ([ x [ i ], x [ j ], edge_attr ], dim =- 1 ) # Transform and normalize messages = self . bn ( self . node_fc ( messages )) messages = F . softplus ( messages ) # Aggregate to each node # Sum over all incoming edges x_new = scatter_add ( messages , i , dim = 0 ) return x + x_new # Residual connection Key Features: Distance-based edge weights : def edge_features ( distance , cutoff = 8.0 ): # Gaussian distance encoding centers = torch . linspace ( 0 , cutoff , 100 ) gamma = ( centers [ 1 ] - centers [ 0 ]) return torch . exp ( - gamma * ( distance - centers ) ** 2 ) Pooling for crystal-level properties : # Average over all atoms h_crystal = global_mean_pool ( h_atoms , batch ) # Or weighted by composition h_crystal = \u03a3_i ( n_i / N_total ) \u00d7 h_i Handling variable composition : # Works for any stoichiometry # Na\u2082Cl\u2082: 2 Na nodes, 2 Cl nodes # Na\u2081\u2080Cl\u2081\u2080: 10 Na nodes, 10 Cl nodes # Same model, different graph sizes Training: # Dataset: Materials Project dataset = CrystalDataset ( 'materials_project' ) model = CGCNN ( atom_features = 92 , # One-hot: 92 elements edge_features = 100 , # Gaussian RBFs hidden_dim = 128 , num_layers = 4 ) # Predict formation energy for crystal_graph in dataloader : pred_energy = model ( crystal_graph ) loss = F . mse_loss ( pred_energy , crystal_graph . y ) Performance: - MAE \u2248 0.04 eV/atom for formation energy - Handles diverse chemistries (metals, semiconductors, insulators) - Fast: 1000\u00d7 faster than DFT MEGNet (MatErials Graph Network) Multi-level graph network with atom, bond, and global state. Three-Level Architecture: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Global State\u2502 (lattice, volume, composition) \u2502 g \u2208 \u211d\u1d48 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Atom States\u2502 \u2502 Bond States \u2502 \u2502 v \u2208 \u211d\u1d48 \u2502\u25c4\u2500\u2500\u25ba\u2502 e \u2208 \u211d\u1d48 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Update Rules: # Bond update: use atoms + global e_ij ' = \u03c6_e([e_ij || v_i || v_j || g]) # Atom update: aggregate bonds + global v_i ' = \u03c6_v([v_i || \u03a3_j e_ij' || g ]) # Global update: aggregate atoms + bonds g ' = \u03c6_g([g || \u03a3_i v_i' || \u03a3_ij e_ij ']) Advantages: Multi-scale information : Local: Atom and bond features Global: Crystal-level properties (volume, space group) Richer representations : Bonds have their own learned features Global state captures extensive properties Better for complex properties : Properties that depend on global structure Example: Thermal conductivity (collective behavior) Applications: # Multi-task learning model = MEGNet ( tasks = [ 'formation_energy' , 'band_gap' , 'bulk_modulus' , 'shear_modulus' ]) # Shared encoder, separate heads h_shared = model . encode ( crystal ) E_f = model . head_energy ( h_shared ) E_g = model . head_gap ( h_shared ) B = model . head_bulk ( h_shared ) G = model . head_shear ( h_shared ) SchNet for Crystals Adapting continuous filters for periodic systems. Modifications for Periodicity: class SchNetCrystal ( nn . Module ): def __init__ ( self ): self . rbf_expansion = GaussianRBF ( cutoff = 5.0 ) self . interaction_blocks = nn . ModuleList ([ InteractionBlock () for _ in range ( 6 ) ]) def forward ( self , crystal ): # Handle periodic images distances , cell_offsets = get_neighbor_distances ( crystal . frac_coords , crystal . lattice , cutoff = 5.0 ) # Distance features (same as molecular SchNet) edge_features = self . rbf_expansion ( distances ) # Message passing with periodic edges h = self . embed ( crystal . atomic_numbers ) for block in self . interaction_blocks : h = block ( h , crystal . edge_index , edge_features ) # Crystal-level pooling return global_mean_pool ( h , crystal . batch ) Handling Variable Unit Cell Size: # Challenge: Different crystals have different numbers of atoms # Solution: Normalization # Per-atom properties \u2192 extensive energy_per_atom = total_energy / num_atoms # Or use composition-weighted features composition = get_composition ( crystal ) # {'Na': 2, 'Cl': 2} weights = [ composition [ atom ] / sum ( composition . values ()) for atom in atoms ] h_crystal = \u03a3_i weights [ i ] \u00d7 h_i Allegro / NequIP State-of-the-art equivariant models for materials. E(3)-Equivariant Architecture: class E3NN_Crystal ( nn . Module ): \"\"\"Based on e3nn library\"\"\" def __init__ ( self ): # Irreducible representations (irreps) # l=0: scalars (rotation invariant) # l=1: vectors (rotation equivariant) # l=2: rank-2 tensors self . irreps_in = \"92x0e\" # 92 elements, scalar features self . irreps_hidden = \"128x0e + 64x1o + 32x2e\" # Mixed irreps self . irreps_out = \"1x0e\" # Energy (scalar) self . convolution = E3Convolution ( self . irreps_in , self . irreps_hidden ) def forward ( self , crystal ): # Features transform correctly under rotations # Scalars: unchanged # Vectors: rotate with crystal # Tensors: rotate as rank-2 tensors ... Applications: Force Field Learning : # Train on energy and forces energy_pred = model ( crystal ) forces_pred = - autograd . grad ( energy_pred , crystal . coords ) loss = ( w_E * ( energy_pred - energy_true ) ** 2 + w_F * ( forces_pred - forces_true ) ** 2 ) Molecular Dynamics : Learned potential: 1000-10000\u00d7 faster than DFT Accuracy: Near DFT quality Application: Simulate nanoseconds to microseconds Stress Tensor Prediction : # Stress tensor: rank-2 equivariant quantity stress = model . predict_stress ( crystal ) # Transforms as: \u03c3 \u2192 Q \u03c3 Q^T under rotation Q","title":"Specialized Architectures"},{"location":"day3/#handling__periodicity__technical__details","text":"Minimum Image Convention in Practice: def minimum_image_distance ( frac_coords_i , frac_coords_j , lattice ): \"\"\" Compute minimum distance through periodic boundaries \"\"\" # Difference in fractional coordinates d_frac = frac_coords_j - frac_coords_i # Wrap to [-0.5, 0.5] (minimum image) d_frac = d_frac - np . round ( d_frac ) # Convert to Cartesian d_cart = lattice @ d_frac # Distance return np . linalg . norm ( d_cart ) Edge Attributes for Periodic Systems: edge_attr = { 'distance' : d_ij , 'direction' : ( r_j - r_i ) / d_ij , # Unit vector 'cell_offset' : ( n1 , n2 , n3 ), # Which periodic image 'is_boundary' : ( n1 != 0 or n2 != 0 or n3 != 0 ) } Lattice as Global Feature: # Include lattice parameters as global features lattice_features = torch . tensor ([ a , b , c , # Lengths alpha , beta , gamma , # Angles volume , # Cell volume np . log ( volume ), # Log volume (more uniform distribution) ]) # Broadcast to all atoms for atom in crystal : atom . features = torch . cat ([ atom . features , lattice_features ])","title":"Handling Periodicity: Technical Details"},{"location":"day3/#applications","text":"Materials Discovery Workflow: # 1. Generate candidate structures from pymatgen.io.cif import CifWriter from pymatgen.core import Structure candidates = [] for composition in element_combinations : for prototype in common_structures : structure = substitute ( prototype , composition ) if is_reasonable ( structure ): # Basic filters candidates . append ( structure ) print ( f \"Generated { len ( candidates ) } candidates\" ) # ~10\u2076 # 2. Screen with GNN model = load_model ( 'megnet_bandgap.pt' ) promising = [] for structure in tqdm ( candidates ): graph = structure_to_graph ( structure ) E_g = model . predict ( graph ) if 1.0 < E_g < 1.5 : # Target range for solar cells promising . append (( structure , E_g )) print ( f \"Found { len ( promising ) } promising candidates\" ) # ~10\u00b3 # 3. Refine with DFT for structure , E_g_predicted in promising [: 100 ]: # Top 100 E_g_dft = run_dft ( structure ) # Expensive but accurate if abs ( E_g_dft - E_g_predicted ) < 0.2 : # GNN was accurate! save_for_synthesis ( structure ) # 4. Experimental synthesis (top 10) Process Optimization: # Predict stability under different conditions def predict_stability_map ( composition ): structures = generate_polymorphs ( composition ) temperatures = np . linspace ( 300 , 2000 , 100 ) # K pressures = np . linspace ( 1 , 100 , 50 ) # GPa phase_diagram = np . zeros (( len ( temperatures ), len ( pressures ))) for i , T in enumerate ( temperatures ): for j , P in enumerate ( pressures ): energies = [ model . predict_free_energy ( s , T , P ) for s in structures ] phase_diagram [ i , j ] = np . argmin ( energies ) return phase_diagram Doping and Substitution: # Explore chemical substitutions base_structure = Structure . from_file ( 'LiFePO4.cif' ) for dopant in [ 'Mn' , 'Co' , 'Ni' ]: for site in iron_sites : doped = base_structure . copy () doped . replace ( site , dopant ) # Predict properties of doped material E_g = model . predict ( doped , property = 'band_gap' ) conductivity = model . predict ( doped , property = 'ionic_conductivity' ) print ( f \"Li { dopant } PO4: E_g= { E_g : .2f } eV, \u03c3= { conductivity : .2e } S/cm\" ) Catalysis Applications: # Surface models for catalysis def model_surface ( bulk_structure , miller_indices = ( 1 , 1 , 1 )): # Create surface slab slab = bulk_structure . get_surface ( miller_indices , thickness = 4 ) # Add adsorbate adsorbate = Molecule ([ 'H' , 'H' ], [[ 0 , 0 , 0 ], [ 0 , 0 , 0.74 ]]) slab_with_ads = add_adsorbate ( slab , adsorbate , site = 'ontop' ) # Predict adsorption energy E_slab = model . predict_energy ( slab ) E_slab_ads = model . predict_energy ( slab_with_ads ) E_H2 = model . predict_energy ( adsorbate ) E_ads = E_slab_ads - E_slab - 0.5 * E_H2 return E_ads","title":"Applications"},{"location":"day3/#challenges__and__future__directions","text":"Current Limitations: Size limitations : Most models use small unit cells Typical: 10-50 atoms Real systems: Can have 100-1000 atoms (supercells, defects) Accuracy vs speed trade-off : GNNs: Fast but ~10% error DFT: Slow but ~1% error Need: Fast AND accurate Out-of-distribution generalization : Models trained on known materials May fail on truly novel chemistries Active learning can help Emerging Directions: Foundation Models : Pre-train on ALL known structures (~200K) Transfer to specific tasks Few-shot learning for new properties Inverse Design : Input: Desired properties (E_g=1.3 eV, stable, earth-abundant) Output: Candidate structures Challenge: Generative models for crystals Multi-fidelity Learning : Low-fidelity: GNN predictions (fast, many) High-fidelity: DFT calculations (slow, few) Combine: Bayesian optimization, active learning Uncertainty Quantification : prediction , uncertainty = model . predict_with_uncertainty ( structure ) if uncertainty < threshold : # High confidence, trust prediction use_prediction () else : # Low confidence, run DFT run_dft_calculation ()","title":"Challenges and Future Directions"},{"location":"day3/#6__practical__building__a__gat__for__qm9__dataset","text":"In this hands-on session, we\u2019ll build a Graph Attention Network to predict molecular properties on the QM9 dataset. This practical will cover the complete workflow from data loading to model evaluation and interpretation.","title":"6. Practical: Building a GAT for QM9 Dataset"},{"location":"day3/#dataset__overview","text":"QM9 Dataset: The QM9 dataset is a quantum chemistry benchmark containing: - 134,000 small organic molecules (subset of GDB-17 database) - Up to 9 heavy atoms (C, O, N, F) - excludes hydrogen in counting - 19 regression targets computed with Density Functional Theory (DFT) - 3D molecular geometries with optimized coordinates Target Properties: Index Property Units Typical Range 0 Dipole moment D 0-5 1 Isotropic polarizability a\u2080\u00b3 10-100 2 HOMO energy eV -12 to -5 3 LUMO energy eV -5 to 5 4 HOMO-LUMO gap eV 2-15 5 Electronic spatial extent a\u2080\u00b2 200-1500 6 Zero-point vibrational energy eV 0.5-3.0 7 Internal energy (0K) eV -100 to 0 8 Internal energy (298K) eV -100 to 0 9 Enthalpy (298K) eV -100 to 0 10 Free energy (298K) eV -100 to 0 11 Heat capacity (298K) cal/mol/K 10-40 12 Atomization energy (0K) eV -100 to 0 13 Atomization energy (298K) eV -100 to 0 14-18 Rotational constants A,B,C GHz Various Why QM9 is Important: Benchmark standard : Most papers report QM9 results Diverse chemistry : Various functional groups and structures Ground truth quality : High-level DFT calculations (B3LYP/6-31G(2df,p)) Moderate size : Large enough to train deep models, small enough to iterate quickly Multi-task learning : 19 targets allow testing generalization Data Format: # Each molecule has: molecule = { 'num_atoms' : 18 , 'atomic_numbers' : [ 6 , 6 , 6 , 1 , 1 , ... ], # Element types 'positions' : [[ x1 , y1 , z1 ], [ x2 , y2 , z2 ], ... ], # 3D coordinates (\u00c5) 'edge_index' : [[ 0 , 1 ], [ 1 , 2 ], ... ], # Bond connectivity 'edge_attr' : [[ 1 ], [ 2 ], ... ], # Bond types (1=single, 2=double, 3=triple) 'y' : [ 2.7 , 48.2 , - 7.8 , ... ], # 19 target properties }","title":"Dataset Overview"},{"location":"day3/#implementation__steps","text":"Step 1: Data Loading and Preprocessing import torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.datasets import QM9 from torch_geometric.loader import DataLoader from torch_geometric.nn import GATConv , global_mean_pool , global_add_pool import numpy as np from sklearn.metrics import mean_absolute_error , r2_score import matplotlib.pyplot as plt # Set random seeds for reproducibility torch . manual_seed ( 42 ) np . random . seed ( 42 ) # Load QM9 dataset # This will download ~200MB on first run print ( \"Loading QM9 dataset...\" ) dataset = QM9 ( root = './data/QM9' ) print ( f \"Total molecules: { len ( dataset ) } \" ) print ( f \"Number of features: { dataset . num_features } \" ) print ( f \"Number of targets: { dataset . num_tasks } \" ) # Select target property to predict # Let's predict HOMO energy (index 2) target_idx = 2 target_name = 'HOMO_energy' print ( f \" \\n Target: { target_name } (index { target_idx } )\" ) print ( f \"Mean: { dataset . data . y [:, target_idx ] . mean () : .4f } \" ) print ( f \"Std: { dataset . data . y [:, target_idx ] . std () : .4f } \" ) # Examine a sample molecule sample = dataset [ 0 ] print ( f \" \\n Sample molecule:\" ) print ( f \" Number of atoms: { sample . num_nodes } \" ) print ( f \" Number of bonds: { sample . num_edges } \" ) print ( f \" Node features shape: { sample . x . shape } \" ) print ( f \" Edge indices shape: { sample . edge_index . shape } \" ) print ( f \" Target values: { sample . y . shape } \" ) # Split dataset (80% train, 10% validation, 10% test) train_size = int ( 0.8 * len ( dataset )) val_size = int ( 0.1 * len ( dataset )) test_size = len ( dataset ) - train_size - val_size # Use specific indices to ensure consistent splits train_dataset = dataset [: train_size ] val_dataset = dataset [ train_size : train_size + val_size ] test_dataset = dataset [ train_size + val_size :] print ( f \" \\n Dataset splits:\" ) print ( f \" Train: { len ( train_dataset ) } molecules\" ) print ( f \" Val: { len ( val_dataset ) } molecules\" ) print ( f \" Test: { len ( test_dataset ) } molecules\" ) # Compute normalization statistics from training set only # This prevents data leakage y_train = torch . stack ([ data . y [ target_idx ] for data in train_dataset ]) target_mean = y_train . mean () target_std = y_train . std () print ( f \" \\n Normalization (from train set):\" ) print ( f \" Mean: { target_mean : .4f } \" ) print ( f \" Std: { target_std : .4f } \" ) # Create data loaders batch_size = 32 train_loader = DataLoader ( train_dataset , batch_size = batch_size , shuffle = True , # Shuffle training data num_workers = 4 # Parallel data loading ) val_loader = DataLoader ( val_dataset , batch_size = batch_size , shuffle = False , num_workers = 4 ) test_loader = DataLoader ( test_dataset , batch_size = batch_size , shuffle = False , num_workers = 4 ) print ( f \" \\n Batch information:\" ) print ( f \" Batch size: { batch_size } \" ) print ( f \" Train batches: { len ( train_loader ) } \" ) print ( f \" Val batches: { len ( val_loader ) } \" ) print ( f \" Test batches: { len ( test_loader ) } \" ) Understanding the Data: # Visualize feature distributions def analyze_dataset ( dataset , name = 'Dataset' ): \"\"\"Analyze and visualize dataset statistics\"\"\" # Collect statistics num_atoms = [ data . num_nodes for data in dataset ] num_bonds = [ data . num_edges // 2 for data in dataset ] # Divide by 2 (undirected) print ( f \" \\n { name } Statistics:\" ) print ( f \" Molecules with 5 atoms: { sum ( n == 5 for n in num_atoms ) } \" ) print ( f \" Molecules with 9 atoms: { sum ( n == 9 for n in num_atoms ) } \" ) print ( f \" Average atoms: { np . mean ( num_atoms ) : .2f } \" ) print ( f \" Average bonds: { np . mean ( num_bonds ) : .2f } \" ) # Plot distributions fig , axes = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) axes [ 0 ] . hist ( num_atoms , bins = range ( 5 , 11 ), edgecolor = 'black' , alpha = 0.7 ) axes [ 0 ] . set_xlabel ( 'Number of Atoms' ) axes [ 0 ] . set_ylabel ( 'Frequency' ) axes [ 0 ] . set_title ( 'Molecule Size Distribution' ) axes [ 0 ] . grid ( alpha = 0.3 ) # Get target values targets = [ data . y [ target_idx ] . item () for data in dataset ] axes [ 1 ] . hist ( targets , bins = 50 , edgecolor = 'black' , alpha = 0.7 , color = 'green' ) axes [ 1 ] . set_xlabel ( f ' { target_name } (eV)' ) axes [ 1 ] . set_ylabel ( 'Frequency' ) axes [ 1 ] . set_title ( 'Target Distribution' ) axes [ 1 ] . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( f ' { name . lower () } _analysis.png' , dpi = 150 ) print ( f \" Saved: { name . lower () } _analysis.png\" ) analyze_dataset ( train_dataset , 'Training Set' ) Step 2: Define GAT Model class GATForQM9 ( nn . Module ): \"\"\" Graph Attention Network for molecular property prediction Architecture: - Initial embedding layer - Multiple GAT layers with multi-head attention - Batch normalization and residual connections - Global pooling - MLP prediction head \"\"\" def __init__ ( self , num_node_features , hidden_dim = 64 , num_heads = 4 , num_layers = 3 , dropout = 0.2 , target_mean = 0.0 , target_std = 1.0 ): super ( GATForQM9 , self ) . __init__ () # Store normalization parameters self . register_buffer ( 'target_mean' , torch . tensor ( target_mean )) self . register_buffer ( 'target_std' , torch . tensor ( target_std )) # Initial node embedding # Maps input features to hidden dimension self . embedding = nn . Linear ( num_node_features , hidden_dim ) self . embedding_bn = nn . BatchNorm1d ( hidden_dim ) # GAT layers with multi-head attention self . gat_layers = nn . ModuleList () self . batch_norms = nn . ModuleList () for i in range ( num_layers ): # Input dimension changes after concatenation if i == 0 : in_channels = hidden_dim else : in_channels = hidden_dim * num_heads # Last layer averages heads instead of concatenation if i == num_layers - 1 : self . gat_layers . append ( GATConv ( in_channels = in_channels , out_channels = hidden_dim , heads = num_heads , concat = False , # Average instead of concatenate dropout = dropout , add_self_loops = True , # Include self-attention bias = True ) ) self . batch_norms . append ( nn . BatchNorm1d ( hidden_dim )) else : self . gat_layers . append ( GATConv ( in_channels = in_channels , out_channels = hidden_dim , heads = num_heads , concat = True , # Concatenate attention heads dropout = dropout , add_self_loops = True , bias = True ) ) self . batch_norms . append ( nn . BatchNorm1d ( hidden_dim * num_heads )) # Prediction head # Two-layer MLP with ReLU activation self . fc1 = nn . Linear ( hidden_dim , hidden_dim // 2 ) self . bn_fc1 = nn . BatchNorm1d ( hidden_dim // 2 ) self . fc2 = nn . Linear ( hidden_dim // 2 , 1 ) self . dropout = nn . Dropout ( dropout ) self . relu = nn . ReLU () # Initialize weights self . _initialize_weights () def _initialize_weights ( self ): \"\"\"Kaiming initialization for better training\"\"\" for m in self . modules (): if isinstance ( m , nn . Linear ): nn . init . kaiming_normal_ ( m . weight , mode = 'fan_out' , nonlinearity = 'relu' ) if m . bias is not None : nn . init . constant_ ( m . bias , 0 ) def forward ( self , data ): \"\"\" Forward pass Args: data: PyG Batch object with x, edge_index, batch Returns: predictions: Tensor of shape [batch_size] \"\"\" x , edge_index , batch = data . x , data . edge_index , data . batch # Initial embedding x = self . embedding ( x ) x = self . embedding_bn ( x ) x = self . relu ( x ) x = self . dropout ( x ) # GAT layers with residual connections for i , ( gat , bn ) in enumerate ( zip ( self . gat_layers , self . batch_norms )): # Store input for residual connection x_residual = x # GAT convolution x = gat ( x , edge_index ) x = bn ( x ) x = self . relu ( x ) x = self . dropout ( x ) # Residual connection (with projection if dimensions don't match) if x_residual . size ( 1 ) == x . size ( 1 ): x = x + x_residual elif i > 0 : # Skip for first layer # Project residual to match dimensions x_residual_proj = self . _project_residual ( x_residual , x . size ( 1 )) x = x + x_residual_proj # Global pooling: aggregate node features to graph-level # Mean pooling: intensive property (per-atom average) x = global_mean_pool ( x , batch ) # Prediction head x = self . fc1 ( x ) x = self . bn_fc1 ( x ) x = self . relu ( x ) x = self . dropout ( x ) x = self . fc2 ( x ) # Denormalize predictions x = x * self . target_std + self . target_mean return x . squeeze ( - 1 ) # Remove last dimension [batch, 1] -> [batch] def _project_residual ( self , x_residual , target_dim ): \"\"\"Helper to project residual connection\"\"\" if not hasattr ( self , 'residual_projections' ): self . residual_projections = {} key = ( x_residual . size ( 1 ), target_dim ) if key not in self . residual_projections : self . residual_projections [ key ] = nn . Linear ( x_residual . size ( 1 ), target_dim , bias = False ) . to ( x_residual . device ) return self . residual_projections [ key ]( x_residual ) # Instantiate model device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) print ( f \" \\n Using device: { device } \" ) model = GATForQM9 ( num_node_features = dataset . num_features , hidden_dim = 128 , num_heads = 4 , num_layers = 4 , dropout = 0.2 , target_mean = target_mean . item (), target_std = target_std . item () ) . to ( device ) # Count parameters num_params = sum ( p . numel () for p in model . parameters () if p . requires_grad ) print ( f \"Number of trainable parameters: { num_params : , } \" ) # Print model architecture print ( \" \\n Model Architecture:\" ) print ( model ) Step 3: Training Loop with Best Practices def train_epoch ( model , loader , optimizer , criterion , device ): \"\"\"Train for one epoch\"\"\" model . train () total_loss = 0 num_samples = 0 for data in loader : data = data . to ( device ) # Forward pass pred = model ( data ) target = data . y [:, target_idx ] # Compute loss loss = criterion ( pred , target ) # Backward pass optimizer . zero_grad () loss . backward () # Gradient clipping to prevent explosion torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm = 1.0 ) optimizer . step () total_loss += loss . item () * data . num_graphs num_samples += data . num_graphs return total_loss / num_samples def evaluate ( model , loader , criterion , device ): \"\"\"Evaluate model\"\"\" model . eval () total_loss = 0 predictions = [] targets = [] with torch . no_grad (): for data in loader : data = data . to ( device ) pred = model ( data ) target = data . y [:, target_idx ] loss = criterion ( pred , target ) total_loss += loss . item () * data . num_graphs predictions . append ( pred . cpu ()) targets . append ( target . cpu ()) predictions = torch . cat ( predictions ) targets = torch . cat ( targets ) # Compute metrics mae = torch . mean ( torch . abs ( predictions - targets )) . item () rmse = torch . sqrt ( torch . mean (( predictions - targets ) ** 2 )) . item () r2 = r2_score ( targets . numpy (), predictions . numpy ()) return { 'loss' : total_loss / len ( loader . dataset ), 'mae' : mae , 'rmse' : rmse , 'r2' : r2 , 'predictions' : predictions , 'targets' : targets } # Setup training optimizer = torch . optim . Adam ( model . parameters (), lr = 0.001 , weight_decay = 1e-5 ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = 'min' , factor = 0.5 , # Reduce LR by half patience = 10 , # Wait 10 epochs before reducing verbose = True , min_lr = 1e-6 # Don't go below this ) criterion = nn . L1Loss () # MAE loss # Training loop num_epochs = 200 best_val_loss = float ( 'inf' ) patience_counter = 0 early_stop_patience = 30 # Track history history = { 'train_loss' : [], 'val_loss' : [], 'val_mae' : [], 'val_r2' : [], 'learning_rate' : [] } print ( f \" \\n Starting training for { num_epochs } epochs...\" ) print ( \"=\" * 70 ) for epoch in range ( num_epochs ): # Train train_loss = train_epoch ( model , train_loader , optimizer , criterion , device ) # Evaluate val_metrics = evaluate ( model , val_loader , criterion , device ) val_loss = val_metrics [ 'loss' ] val_mae = val_metrics [ 'mae' ] val_r2 = val_metrics [ 'r2' ] # Learning rate scheduling scheduler . step ( val_loss ) current_lr = optimizer . param_groups [ 0 ][ 'lr' ] # Store history history [ 'train_loss' ] . append ( train_loss ) history [ 'val_loss' ] . append ( val_loss ) history [ 'val_mae' ] . append ( val_mae ) history [ 'val_r2' ] . append ( val_r2 ) history [ 'learning_rate' ] . append ( current_lr ) # Save best model if val_loss < best_val_loss : best_val_loss = val_loss torch . save ({ 'epoch' : epoch , 'model_state_dict' : model . state_dict (), 'optimizer_state_dict' : optimizer . state_dict (), 'val_loss' : val_loss , 'val_mae' : val_mae , }, 'best_gat_qm9.pt' ) patience_counter = 0 else : patience_counter += 1 # Print progress if ( epoch + 1 ) % 10 == 0 or epoch == 0 : print ( f 'Epoch { epoch + 1 : 3d } / { num_epochs } :' ) print ( f ' Train Loss: { train_loss : .4f } ' ) print ( f ' Val Loss: { val_loss : .4f } | MAE: { val_mae : .4f } | R\u00b2: { val_r2 : .4f } ' ) print ( f ' LR: { current_lr : .2e } | Best Val: { best_val_loss : .4f } | Patience: { patience_counter } / { early_stop_patience } ' ) print ( \"-\" * 70 ) # Early stopping if patience_counter >= early_stop_patience : print ( f \" \\n Early stopping triggered at epoch { epoch + 1 } \" ) break print ( \"=\" * 70 ) print ( \"Training completed!\" ) Step 4: Model Evaluation and Analysis # Load best model print ( \" \\n Loading best model...\" ) checkpoint = torch . load ( 'best_gat_qm9.pt' ) model . load_state_dict ( checkpoint [ 'model_state_dict' ]) print ( f \"Best model from epoch { checkpoint [ 'epoch' ] + 1 } \" ) print ( f \"Best validation MAE: { checkpoint [ 'val_mae' ] : .4f } eV\" ) # Evaluate on test set print ( \" \\n Evaluating on test set...\" ) test_metrics = evaluate ( model , test_loader , criterion , device ) print ( \" \\n Test Set Results:\" ) print ( f \" Loss: { test_metrics [ 'loss' ] : .4f } \" ) print ( f \" MAE: { test_metrics [ 'mae' ] : .4f } eV\" ) print ( f \" RMSE: { test_metrics [ 'rmse' ] : .4f } eV\" ) print ( f \" R\u00b2: { test_metrics [ 'r2' ] : .4f } \" ) # Plot training curves fig , axes = plt . subplots ( 2 , 2 , figsize = ( 14 , 10 )) # Loss curves axes [ 0 , 0 ] . plot ( history [ 'train_loss' ], label = 'Train' , linewidth = 2 ) axes [ 0 , 0 ] . plot ( history [ 'val_loss' ], label = 'Validation' , linewidth = 2 ) axes [ 0 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 0 ] . set_ylabel ( 'Loss (MAE)' ) axes [ 0 , 0 ] . set_title ( 'Training and Validation Loss' ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( alpha = 0.3 ) # MAE over epochs axes [ 0 , 1 ] . plot ( history [ 'val_mae' ], color = 'green' , linewidth = 2 ) axes [ 0 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 0 , 1 ] . set_ylabel ( 'MAE (eV)' ) axes [ 0 , 1 ] . set_title ( 'Validation MAE' ) axes [ 0 , 1 ] . grid ( alpha = 0.3 ) # R\u00b2 over epochs axes [ 1 , 0 ] . plot ( history [ 'val_r2' ], color = 'purple' , linewidth = 2 ) axes [ 1 , 0 ] . set_xlabel ( 'Epoch' ) axes [ 1 , 0 ] . set_ylabel ( 'R\u00b2 Score' ) axes [ 1 , 0 ] . set_title ( 'Validation R\u00b2' ) axes [ 1 , 0 ] . grid ( alpha = 0.3 ) # Learning rate schedule axes [ 1 , 1 ] . plot ( history [ 'learning_rate' ], color = 'red' , linewidth = 2 ) axes [ 1 , 1 ] . set_xlabel ( 'Epoch' ) axes [ 1 , 1 ] . set_ylabel ( 'Learning Rate' ) axes [ 1 , 1 ] . set_title ( 'Learning Rate Schedule' ) axes [ 1 , 1 ] . set_yscale ( 'log' ) axes [ 1 , 1 ] . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( 'training_curves.png' , dpi = 150 ) print ( \" \\n Saved: training_curves.png\" ) Step 5: Visualization and Interpretation def plot_predictions ( predictions , targets , dataset_name = 'Test' ): \"\"\"Create comprehensive prediction plots\"\"\" predictions = predictions . numpy () targets = targets . numpy () fig , axes = plt . subplots ( 2 , 2 , figsize = ( 14 , 12 )) # Scatter plot: Predicted vs Actual axes [ 0 , 0 ] . scatter ( targets , predictions , alpha = 0.3 , s = 20 ) # Perfect prediction line min_val , max_val = min ( targets . min (), predictions . min ()), max ( targets . max (), predictions . max ()) axes [ 0 , 0 ] . plot ([ min_val , max_val ], [ min_val , max_val ], 'r--' , lw = 2 , label = 'Perfect Prediction' ) # Add statistics text mae = mean_absolute_error ( targets , predictions ) r2 = r2_score ( targets , predictions ) axes [ 0 , 0 ] . text ( 0.05 , 0.95 , f 'MAE = { mae : .4f } eV \\n R\u00b2 = { r2 : .4f } ' , transform = axes [ 0 , 0 ] . transAxes , fontsize = 12 , verticalalignment = 'top' , bbox = dict ( boxstyle = 'round' , facecolor = 'wheat' , alpha = 0.5 ) ) axes [ 0 , 0 ] . set_xlabel ( f 'Actual { target_name } (eV)' , fontsize = 12 ) axes [ 0 , 0 ] . set_ylabel ( f 'Predicted { target_name } (eV)' , fontsize = 12 ) axes [ 0 , 0 ] . set_title ( f ' { dataset_name } Set: Predictions vs Actual' , fontsize = 14 ) axes [ 0 , 0 ] . legend () axes [ 0 , 0 ] . grid ( alpha = 0.3 ) axes [ 0 , 0 ] . set_aspect ( 'equal' ) # Error distribution errors = predictions - targets axes [ 0 , 1 ] . hist ( errors , bins = 50 , edgecolor = 'black' , alpha = 0.7 , color = 'coral' ) axes [ 0 , 1 ] . axvline ( 0 , color = 'r' , linestyle = '--' , linewidth = 2 , label = 'Zero Error' ) axes [ 0 , 1 ] . set_xlabel ( 'Prediction Error (eV)' , fontsize = 12 ) axes [ 0 , 1 ] . set_ylabel ( 'Frequency' , fontsize = 12 ) axes [ 0 , 1 ] . set_title ( 'Error Distribution' , fontsize = 14 ) axes [ 0 , 1 ] . legend () axes [ 0 , 1 ] . grid ( alpha = 0.3 ) # Error vs actual value axes [ 1 , 0 ] . scatter ( targets , np . abs ( errors ), alpha = 0.3 , s = 20 , color = 'green' ) axes [ 1 , 0 ] . axhline ( mae , color = 'r' , linestyle = '--' , linewidth = 2 , label = f 'Mean MAE = { mae : .4f } ' ) axes [ 1 , 0 ] . set_xlabel ( f 'Actual { target_name } (eV)' , fontsize = 12 ) axes [ 1 , 0 ] . set_ylabel ( 'Absolute Error (eV)' , fontsize = 12 ) axes [ 1 , 0 ] . set_title ( 'Absolute Error vs Actual Value' , fontsize = 14 ) axes [ 1 , 0 ] . legend () axes [ 1 , 0 ] . grid ( alpha = 0.3 ) # Q-Q plot (Quantile-Quantile) from scipy import stats stats . probplot ( errors , dist = \"norm\" , plot = axes [ 1 , 1 ]) axes [ 1 , 1 ] . set_title ( 'Q-Q Plot (Error Normality)' , fontsize = 14 ) axes [ 1 , 1 ] . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( f ' { dataset_name . lower () } _predictions.png' , dpi = 150 ) print ( f \"Saved: { dataset_name . lower () } _predictions.png\" ) # Plot test set predictions plot_predictions ( test_metrics [ 'predictions' ], test_metrics [ 'targets' ], dataset_name = 'Test' ) # Analyze errors by molecule size def analyze_errors_by_size ( model , loader , device ): \"\"\"Analyze how prediction error varies with molecule size\"\"\" model . eval () errors_by_size = { i : [] for i in range ( 5 , 10 )} with torch . no_grad (): for data in loader : data = data . to ( device ) pred = model ( data ) target = data . y [:, target_idx ] errors = torch . abs ( pred - target ) . cpu () # Group by number of atoms for i , error in enumerate ( errors ): num_atoms = ( data . batch == i ) . sum () . item () if num_atoms in errors_by_size : errors_by_size [ num_atoms ] . append ( error . item ()) # Plot plt . figure ( figsize = ( 10 , 6 )) sizes = sorted ( errors_by_size . keys ()) mean_errors = [ np . mean ( errors_by_size [ size ]) if errors_by_size [ size ] else 0 for size in sizes ] std_errors = [ np . std ( errors_by_size [ size ]) if errors_by_size [ size ] else 0 for size in sizes ] plt . errorbar ( sizes , mean_errors , yerr = std_errors , marker = 'o' , linewidth = 2 , markersize = 8 , capsize = 5 ) plt . xlabel ( 'Number of Heavy Atoms' , fontsize = 12 ) plt . ylabel ( 'Mean Absolute Error (eV)' , fontsize = 12 ) plt . title ( 'Prediction Error vs Molecule Size' , fontsize = 14 ) plt . grid ( alpha = 0.3 ) plt . tight_layout () plt . savefig ( 'error_by_size.png' , dpi = 150 ) print ( \"Saved: error_by_size.png\" ) return errors_by_size errors_by_size = analyze_errors_by_size ( model , test_loader , device )","title":"Implementation Steps"},{"location":"day3/#expected__results","text":"Performance Benchmarks: For HOMO energy prediction (in meV): Method MAE RMSE R\u00b2 Parameters Baseline 300 420 0.00 - Linear 180 250 0.45 10K MLP 120 170 0.72 50K GCN 45 65 0.94 500K GAT (ours) 35-40 50-55 0.96 800K DimeNet++ 25-30 40-45 0.98 2M Note: Results vary by hyperparameters and random seed Training Time: Hardware: Single GPU (V100) Batch size: 32 Epochs: ~100-150 to convergence Time per epoch: ~30 seconds Total training: ~60-90 minutes Inference: ~1000 molecules/second","title":"Expected Results"},{"location":"day3/#extension__ideas","text":"1. Multi-Task Learning class MultiTaskGAT ( nn . Module ): \"\"\"Predict multiple properties simultaneously\"\"\" def __init__ ( self , num_tasks = 19 ): super () . __init__ () # Shared encoder self . encoder = GATEncoder ( ... ) # Separate heads for each task self . task_heads = nn . ModuleList ([ nn . Linear ( hidden_dim , 1 ) for _ in range ( num_tasks ) ]) def forward ( self , data ): h = self . encoder ( data ) return torch . cat ([ head ( h ) for head in self . task_heads ], dim =- 1 ) # Train with multi-task loss loss = sum ( F . mse_loss ( pred [:, i ], target [:, i ]) for i in range ( num_tasks )) 2. Add 3D Distance Information # Add edge features based on 3D distances edge_attr = [] for edge in data . edge_index . t (): i , j = edge dist = torch . norm ( data . pos [ i ] - data . pos [ j ]) # Gaussian RBF encoding rbf = torch . exp ( - ( dist - centers ) ** 2 / gamma ) edge_attr . append ( rbf ) # Use in GAT model = GATConv ( edge_dim = num_rbf ) # Enable edge features 3. Attention Visualization def visualize_attention ( model , molecule_idx ): \"\"\"Visualize attention weights for a molecule\"\"\" data = dataset [ molecule_idx ] . to ( device ) model . eval () # Hook to capture attention attention_weights = [] def hook_fn ( module , input , output ): # GATConv returns (output, attention_weights) if isinstance ( output , tuple ) and len ( output ) == 2 : attention_weights . append ( output [ 1 ] . detach () . cpu ()) # Register hooks on GAT layers hooks = [] for layer in model . gat_layers : hooks . append ( layer . register_forward_hook ( hook_fn )) # Forward pass with torch . no_grad (): _ = model ( data ) # Remove hooks for hook in hooks : hook . remove () # Visualize (requires RDKit or py3Dmol) from rdkit import Chem from rdkit.Chem import Draw # Highlight high-attention bonds mol = Chem . MolFromSmiles ( data . smiles ) # If SMILES available highlight_bonds = [] for i , ( src , dst ) in enumerate ( data . edge_index . t ()): if attention_weights [ 0 ][ i ] > threshold : highlight_bonds . append ( mol . GetBondBetweenAtoms ( int ( src ), int ( dst )) . GetIdx ()) img = Draw . MolToImage ( mol , highlightBonds = highlight_bonds ) plt . imshow ( img ) plt . axis ( 'off' ) plt . savefig ( f 'attention_mol_ { molecule_idx } .png' ) 4. Hyperparameter Tuning import optuna def objective ( trial ): # Suggest hyperparameters hidden_dim = trial . suggest_int ( 'hidden_dim' , 64 , 256 ) num_heads = trial . suggest_int ( 'num_heads' , 2 , 8 ) num_layers = trial . suggest_int ( 'num_layers' , 2 , 6 ) dropout = trial . suggest_float ( 'dropout' , 0.1 , 0.5 ) lr = trial . suggest_loguniform ( 'lr' , 1e-4 , 1e-2 ) # Train model model = GATForQM9 ( hidden_dim = hidden_dim , num_heads = num_heads , ... ) # ... training loop ... return val_mae # Run optimization study = optuna . create_study ( direction = 'minimize' ) study . optimize ( objective , n_trials = 100 ) print ( f \"Best hyperparameters: { study . best_params } \" ) 5. Ensemble Methods # Train multiple models with different seeds models = [] for seed in range ( 5 ): torch . manual_seed ( seed ) model = GATForQM9 ( ... ) # ... train model ... models . append ( model ) # Ensemble prediction def ensemble_predict ( data ): predictions = [ model ( data ) for model in models ] return torch . stack ( predictions ) . mean ( dim = 0 ) # Usually 2-5% better than single model This practical provides a complete, production-ready implementation of GAT for molecular property prediction with extensive analysis and interpretation tools.","title":"Extension Ideas"},{"location":"day3/#summary","text":"Today we covered the fundamentals of graph neural networks and their applications in computational chemistry and materials science: Message Passing Neural Networks provide a flexible framework for learning on graphs Graph Attention Networks learn adaptive edge weights through attention mechanisms Equivariant Networks (SchNet, DimeNet, PaiNN) respect 3D geometric symmetries Protein-Ligand Modeling enables AI-driven drug discovery Crystal Structure Prediction accelerates materials discovery Practical Implementation demonstrated how to build and train GATs on real molecular data","title":"Summary"},{"location":"day3/#additional__resources","text":"Papers: - \u201cNeural Message Passing for Quantum Chemistry\u201d (Gilmer et al., 2017) - \u201cGraph Attention Networks\u201d (Veli\u010dkovi\u0107 et al., 2018) - \u201cSchNet: A continuous-filter convolutional neural network\u201d (Sch\u00fctt et al., 2017) - \u201cDirectional Message Passing for Molecular Graphs\u201d (Klicpera et al., 2020) - \u201cEquivariant message passing for the prediction of tensorial properties\u201d (Sch\u00fctt et al., 2021) Tutorials: - PyTorch Geometric documentation and tutorials - Open Catalyst Project tutorials - Deep Graph Library (DGL) examples Datasets: - QM9: Small organic molecules - PCQM4M: Large-scale quantum chemistry dataset - Materials Project: Inorganic crystals - PDBbind: Protein-ligand binding affinities - Open Catalyst: Catalytic materials","title":"Additional Resources"},{"location":"day4/","text":"Day 4: Generative Models for Molecular Design \u00b6 Overview \u00b6 Generative models have revolutionized computational drug discovery and molecular design by enabling the creation of novel molecular structures with desired properties. This session explores state-of-the-art generative approaches and their application to molecular generation. Learning Objectives \u00b6 By the end of this session, you will be able to: - Understand the principles behind various generative model architectures - Apply VAEs and GANs to molecular generation tasks - Implement autoregressive models for sequential molecular design - Utilize diffusion models for high-quality molecule generation - Integrate reinforcement learning for property optimization - Develop conditional generation systems for targeted molecular design - Build and train a conditional VAE for molecular generation 1. VAEs and GANs for Molecular Generation \u00b6 Variational Autoencoders (VAEs) \u00b6 Variational Autoencoders learn a continuous latent representation of molecules, enabling smooth interpolation and sampling of chemical space. Architecture \u00b6 The VAE consists of two main components: Encoder : Maps molecules to a probabilistic latent space - Input: SMILES string or molecular graph - Output: Mean (\u03bc) and log-variance (log \u03c3\u00b2) of latent distribution - Typically uses RNN/GRU/LSTM or Graph Neural Networks Decoder : Reconstructs molecules from latent representations - Input: Sampled latent vector z ~ N(\u03bc, \u03c3\u00b2) - Output: Reconstructed molecular representation - Generates SMILES character-by-character or reconstructs graph Loss Function \u00b6 The VAE optimizes two objectives: L_VAE = L_reconstruction + \u03b2 \u00d7 L_KL Reconstruction Loss : Ensures accurate molecule reconstruction (e.g., cross-entropy for SMILES) KL Divergence : Regularizes latent space to follow standard normal distribution N(0, I) \u03b2 parameter : Controls the trade-off between reconstruction quality and latent space regularity Molecular Representations \u00b6 Common representations for VAE-based molecular generation: SMILES-based : Character-level sequence modeling Graph-based : Node and edge features with graph autoencoders Fingerprint-based : Fixed-length binary or count vectors 3D conformations : Atomic coordinates and features Advantages \u00b6 Continuous latent space enables interpolation between molecules Sampling generates diverse molecular structures Can be trained unsupervised on large molecular databases Challenges \u00b6 Reconstruction of valid molecules can be difficult Mode collapse in complex chemical spaces Balancing reconstruction quality and latent space organization Generative Adversarial Networks (GANs) \u00b6 GANs consist of two competing neural networks that learn to generate realistic molecular structures. Architecture \u00b6 Generator (G) : Creates fake molecules from random noise - Input: Random noise vector z ~ N(0, I) - Output: Generated molecular representation (SMILES, graph, etc.) - Learns to fool the discriminator Discriminator (D) : Distinguishes real molecules from generated ones - Input: Molecular representation (real or generated) - Output: Probability that input is real - Learns to identify fake molecules Training Process \u00b6 The networks play a minimax game: min_G max_D V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))] Training alternates between: 1. Update discriminator to better distinguish real from fake 2. Update generator to better fool discriminator Molecular GANs Variants \u00b6 MolGAN : Graph-based GAN for molecular generation - Generates molecular graphs directly - Uses permutation-invariant discriminator - Includes reward network for property optimization Objective-Reinforced GAN (ORGAN) : Combines GAN with RL - Adds policy gradient for optimizing molecular properties - Generator receives rewards for desired properties - Balances diversity with property optimization Advantages \u00b6 Can generate highly realistic molecular distributions No explicit likelihood computation needed Flexible in incorporating domain knowledge Challenges \u00b6 Training instability and mode collapse Difficulty ensuring chemical validity Requires careful hyperparameter tuning 2. Autoregressive Models (RNNs, Transformers) \u00b6 Autoregressive models generate molecules sequentially, one token at a time, based on previously generated tokens. Recurrent Neural Networks (RNNs) \u00b6 Architecture \u00b6 RNNs process SMILES strings character-by-character: h_t = f(h_{t-1}, x_t) p(x_t | x_1, ..., x_{t-1}) = softmax(W_h h_t + b) LSTM/GRU variants address vanishing gradient problems and capture long-range dependencies in molecular structures. Training \u00b6 Models are trained on large databases of molecules using teacher forcing: - Input: SMILES prefixes (e.g., \u201cCC(=O)\u201d) - Target: Next character (e.g., \u201cO\u201d) - Loss: Cross-entropy between predicted and actual next character Generation \u00b6 Sample molecules by iteratively predicting the next character: 1. Start with begin token 2. Sample next character from probability distribution 3. Append to sequence and repeat 4. Stop at end token or max length Transfer Learning \u00b6 Pre-trained RNN language models can be fine-tuned for specific molecular design tasks: - Pre-train on large molecular databases (ChEMBL, ZINC) - Fine-tune on smaller target datasets with desired properties - Enables generation of molecules similar to training distribution Transformers for Molecular Generation \u00b6 Transformers leverage self-attention mechanisms for improved sequence modeling. Architecture \u00b6 Self-Attention : Allows model to attend to all positions simultaneously Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V Multi-Head Attention : Parallel attention mechanisms capture different relationships Positional Encoding : Injects sequence order information Advantages Over RNNs \u00b6 Parallel processing enables faster training Better capture of long-range dependencies More effective at learning complex molecular patterns Molecular Transformer Applications \u00b6 SMILES Generation : Character-level transformer language models - Pre-trained on millions of molecules - Fine-tuned for specific chemical spaces Reaction Prediction : Transform reactants to products - Trained on reaction datasets (USPTO) - Can predict reaction outcomes and suggest synthesis routes Molecular Translation : Convert between representations - SMILES to IUPAC names - 2D to 3D structure prediction - Retrosynthesis planning Notable Models \u00b6 ChemBERTa : Transformer pre-trained on SMILES - Masked language modeling on molecular data - Transfer learning for downstream tasks MolGPT : GPT-based architecture for molecules - Autoregressive generation of SMILES - Can be conditioned on desired properties 3. Diffusion Models \u00b6 Diffusion models represent a recent paradigm shift in generative modeling, achieving state-of-the-art results across multiple domains including molecular generation. Principles \u00b6 Diffusion models learn to reverse a gradual noising process: Forward Process : Progressively add Gaussian noise to data q(x_t | x_{t-1}) = N(x_t; \u221a(1-\u03b2_t)x_{t-1}, \u03b2_t I) Reverse Process : Learn to denoise and recover original data p_\u03b8(x_{t-1} | x_t) = N(x_{t-1}; \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t)) Training \u00b6 Train a neural network to predict the noise at each timestep: 1. Sample molecule x\u2080 from dataset 2. Sample timestep t and noise \u03b5 ~ N(0, I) 3. Create noisy version x_t 4. Train network to predict \u03b5 from x_t and t 5. Loss: MSE between predicted and actual noise Generation \u00b6 Generate molecules by iterative denoising: 1. Start with pure noise x_T ~ N(0, I) 2. Iteratively denoise: x_{t-1} = denoise(x_t, t) 3. Final sample x\u2080 is generated molecule Molecular Diffusion Models \u00b6 Continuous Representations : Operate in latent space or coordinate space - E(n) Equivariant Diffusion : Generates 3D molecular conformations - Respects rotational and translational symmetries - Directly outputs atomic positions Discrete Representations : Adapt diffusion to categorical data - Discrete Diffusion : For SMILES or graph generation - Multinomial diffusion over molecular tokens - Transition matrices for adding/removing atoms and bonds Conditional Diffusion : Guide generation towards desired properties - Add property information at each denoising step - Classifier guidance or classifier-free guidance - Control molecular properties during generation Advantages \u00b6 High-quality, diverse samples Stable training compared to GANs Flexible conditioning mechanisms Can generate both 2D and 3D molecular structures Challenges \u00b6 Slower generation compared to one-shot methods Computational cost of iterative sampling Ensuring chemical validity in discrete spaces 4. Reinforcement Learning for Optimization \u00b6 Reinforcement learning enables goal-directed molecular generation by optimizing molecules for specific properties. Framework \u00b6 Treat molecular generation as a sequential decision-making problem: Agent : Generative model (policy) Environment : Chemical space State : Current partial molecule Action : Add/modify atom or bond Reward : Molecular property score (e.g., drug-likeness, binding affinity) Policy Optimization \u00b6 REINFORCE Algorithm \u00b6 Update policy to maximize expected reward: \u2207_\u03b8 J(\u03b8) = E[\u2211_t R_t \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_t)] Baseline subtraction reduces variance: \u2207_\u03b8 J(\u03b8) = E[\u2211_t (R_t - b) \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_t)] Proximal Policy Optimization (PPO) \u00b6 Constrains policy updates for stable training: L(\u03b8) = E[min(r_t(\u03b8)\u00c2_t, clip(r_t(\u03b8), 1-\u03b5, 1+\u03b5)\u00c2_t)] Where r_t(\u03b8) = \u03c0_\u03b8(a_t|s_t) / \u03c0_\u03b8_old(a_t|s_t) Reward Functions \u00b6 Design reward functions to capture desired properties: Single Objective : R = f(molecule) where f might be: - Binding affinity prediction - Synthetic accessibility score - QED (quantitative estimate of drug-likeness) - LogP (lipophilicity) Multi-Objective : R = w\u2081f\u2081(mol) + w\u2082f\u2082(mol) + ... + w\u2099f\u2099(mol) Reward Shaping : - Penalize invalid molecules - Reward intermediate steps - Balance exploration and exploitation Transfer Learning \u00b6 Pre-train generative model on molecular databases, then fine-tune with RL: Pre-training : Learn general molecular distribution RL Fine-tuning : Optimize for specific properties Constrained Optimization : Maintain molecular validity while optimizing Applications \u00b6 De Novo Drug Design : Generate molecules with: - High predicted binding affinity to target protein - Good ADMET properties - Synthetic accessibility Lead Optimization : Modify existing molecules to: - Improve potency - Reduce toxicity - Optimize pharmacokinetics Multi-Parameter Optimization (MPO) : Balance multiple objectives - Efficacy vs. safety - Potency vs. selectivity - Activity vs. synthetic accessibility Challenges \u00b6 Reward function design and balancing Sparse and delayed rewards Maintaining molecular diversity Mode collapse to few high-reward molecules 5. Conditional Generation \u00b6 Conditional generation enables control over generated molecules by incorporating desired properties or constraints into the generation process. Conditioning Strategies \u00b6 Explicit Conditioning \u00b6 Concatenate property information with latent vector: z_cond = [z, c] where c represents condition (e.g., property value, class label) Cross-Attention Conditioning \u00b6 Use attention mechanisms to integrate conditions: - Query: Latent molecular representation - Key/Value: Condition embeddings - Allows flexible, learned integration of conditions Classifier Guidance \u00b6 Steer generation using property prediction model: \u2207_x log p(x|c) \u2248 \u2207_x log p(x) + \u03bb\u2207_x log p(c|x) Guide generation towards higher predicted property values. Types of Conditions \u00b6 Scalar Properties : - Molecular weight - LogP - TPSA (topological polar surface area) - QED score Categorical Properties : - Compound class (kinase inhibitor, GPCR ligand, etc.) - Scaffold type - Functional groups present Structural Constraints : - Required substructures (pharmacophores) - Forbidden substructures (toxic moieties) - Scaffold constraints Target-Based : - Protein target (e.g., \u201cgenerate EGFR inhibitor\u201d) - Binding site information - Target protein sequence/structure Conditional VAE Architecture \u00b6 Modify standard VAE to accept conditions: Encoder : q_\u03c6(z | x, c) = N(\u03bc_\u03c6(x, c), \u03c3_\u03c6(x, c)) Decoder : p_\u03b8(x | z, c) Prior : p(z | c) = N(\u03bc_prior(c), \u03c3_prior(c)) Multi-Conditional Generation \u00b6 Generate molecules satisfying multiple constraints simultaneously: - Combine multiple property conditions - Use hierarchical conditioning - Balance competing objectives Controllable Generation Workflow \u00b6 Train conditional model on labeled molecular dataset Specify desired properties for new molecule Generate candidates with specified properties Validate and filter generated molecules Iterate by adjusting conditions 6. Practical: Building a Conditional VAE \u00b6 This practical exercise guides you through implementing a conditional VAE for molecular generation. Dataset Preparation \u00b6 We\u2019ll use a subset of molecules with associated properties: import pandas as pd import numpy as np from rdkit import Chem from rdkit.Chem import Descriptors , QED import torch from torch.utils.data import Dataset , DataLoader # Load molecular data def prepare_dataset ( smiles_file , max_length = 120 ): \"\"\"Prepare SMILES dataset with properties\"\"\" df = pd . read_csv ( smiles_file ) # Calculate properties properties = [] valid_smiles = [] for smiles in df [ 'smiles' ]: mol = Chem . MolFromSmiles ( smiles ) if mol is not None : props = { 'logp' : Descriptors . MolLogP ( mol ), 'mw' : Descriptors . MolWt ( mol ), 'qed' : QED . qed ( mol ) } properties . append ( props ) valid_smiles . append ( smiles ) # Normalize properties props_df = pd . DataFrame ( properties ) props_normalized = ( props_df - props_df . mean ()) / props_df . std () return valid_smiles , props_normalized # Create vocabulary def create_vocabulary ( smiles_list ): \"\"\"Create character vocabulary from SMILES\"\"\" chars = set () for smiles in smiles_list : chars . update ( smiles ) chars = sorted ( list ( chars )) char_to_idx = { c : i + 1 for i , c in enumerate ( chars )} char_to_idx [ '<PAD>' ] = 0 char_to_idx [ '<START>' ] = len ( char_to_idx ) char_to_idx [ '<END>' ] = len ( char_to_idx ) idx_to_char = { v : k for k , v in char_to_idx . items ()} return char_to_idx , idx_to_char class MolecularDataset ( Dataset ): \"\"\"PyTorch dataset for molecular SMILES\"\"\" def __init__ ( self , smiles_list , properties , char_to_idx , max_length = 120 ): self . smiles_list = smiles_list self . properties = properties self . char_to_idx = char_to_idx self . max_length = max_length def __len__ ( self ): return len ( self . smiles_list ) def __getitem__ ( self , idx ): smiles = self . smiles_list [ idx ] props = self . properties . iloc [ idx ] . values # Encode SMILES encoded = [ self . char_to_idx [ '<START>' ]] encoded += [ self . char_to_idx [ c ] for c in smiles ] encoded += [ self . char_to_idx [ '<END>' ]] # Pad sequence if len ( encoded ) < self . max_length : encoded += [ self . char_to_idx [ '<PAD>' ]] * ( self . max_length - len ( encoded )) else : encoded = encoded [: self . max_length ] return torch . tensor ( encoded , dtype = torch . long ), torch . tensor ( props , dtype = torch . float32 ) Model Architecture \u00b6 Implement the conditional VAE components: import torch.nn as nn import torch.nn.functional as F class ConditionalEncoder ( nn . Module ): \"\"\"Encoder: SMILES + properties -> latent distribution\"\"\" def __init__ ( self , vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers = 2 ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embed_dim , padding_idx = 0 ) self . lstm = nn . LSTM ( embed_dim + num_properties , hidden_dim , num_layers = num_layers , batch_first = True , bidirectional = True ) # Latent parameters self . fc_mu = nn . Linear ( hidden_dim * 2 , latent_dim ) self . fc_logvar = nn . Linear ( hidden_dim * 2 , latent_dim ) def forward ( self , x , properties ): # Embed SMILES embedded = self . embedding ( x ) # [batch, seq_len, embed_dim] # Concatenate properties to each timestep batch_size , seq_len = x . size () props_expanded = properties . unsqueeze ( 1 ) . expand ( - 1 , seq_len , - 1 ) lstm_input = torch . cat ([ embedded , props_expanded ], dim =- 1 ) # LSTM encoding _ , ( hidden , _ ) = self . lstm ( lstm_input ) # Combine forward and backward hidden states hidden = torch . cat ([ hidden [ - 2 ], hidden [ - 1 ]], dim = 1 ) # Latent distribution parameters mu = self . fc_mu ( hidden ) logvar = self . fc_logvar ( hidden ) return mu , logvar class ConditionalDecoder ( nn . Module ): \"\"\"Decoder: latent + properties -> SMILES\"\"\" def __init__ ( self , vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers = 2 ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embed_dim , padding_idx = 0 ) self . lstm = nn . LSTM ( embed_dim + num_properties , hidden_dim , num_layers = num_layers , batch_first = True ) # Project latent to initial hidden state self . latent_to_hidden = nn . Linear ( latent_dim + num_properties , hidden_dim * num_layers ) self . latent_to_cell = nn . Linear ( latent_dim + num_properties , hidden_dim * num_layers ) self . output = nn . Linear ( hidden_dim , vocab_size ) self . num_layers = num_layers def forward ( self , z , properties , target_seq ): batch_size = z . size ( 0 ) # Combine latent and properties z_cond = torch . cat ([ z , properties ], dim = 1 ) # Initialize hidden and cell states h0 = self . latent_to_hidden ( z_cond ) . view ( batch_size , self . num_layers , - 1 ) . transpose ( 0 , 1 ) . contiguous () c0 = self . latent_to_cell ( z_cond ) . view ( batch_size , self . num_layers , - 1 ) . transpose ( 0 , 1 ) . contiguous () # Embed target sequence embedded = self . embedding ( target_seq ) # Add properties to each timestep seq_len = target_seq . size ( 1 ) props_expanded = properties . unsqueeze ( 1 ) . expand ( - 1 , seq_len , - 1 ) lstm_input = torch . cat ([ embedded , props_expanded ], dim =- 1 ) # LSTM decoding output , _ = self . lstm ( lstm_input , ( h0 , c0 )) # Project to vocabulary logits = self . output ( output ) return logits class ConditionalVAE ( nn . Module ): \"\"\"Complete Conditional VAE for molecular generation\"\"\" def __init__ ( self , vocab_size , embed_dim = 128 , hidden_dim = 256 , latent_dim = 64 , num_properties = 3 , num_layers = 2 ): super () . __init__ () self . encoder = ConditionalEncoder ( vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers ) self . decoder = ConditionalDecoder ( vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers ) self . latent_dim = latent_dim def reparameterize ( self , mu , logvar ): \"\"\"Reparameterization trick\"\"\" std = torch . exp ( 0.5 * logvar ) eps = torch . randn_like ( std ) return mu + eps * std def forward ( self , x , properties ): # Encode mu , logvar = self . encoder ( x , properties ) # Reparameterize z = self . reparameterize ( mu , logvar ) # Decode (teacher forcing) # Input: all tokens except last, Target: all tokens except first decoder_input = x [:, : - 1 ] logits = self . decoder ( z , properties , decoder_input ) return logits , mu , logvar def generate ( self , properties , char_to_idx , idx_to_char , max_length = 120 , temperature = 1.0 ): \"\"\"Generate SMILES from properties\"\"\" self . eval () with torch . no_grad (): batch_size = properties . size ( 0 ) # Sample from prior z = torch . randn ( batch_size , self . latent_dim ) . to ( properties . device ) # Start with <START> token generated = torch . tensor ([[ char_to_idx [ '<START>' ]]] * batch_size ) . to ( properties . device ) # Generate sequence for _ in range ( max_length - 1 ): logits = self . decoder ( z , properties , generated ) # Get last token logits next_token_logits = logits [:, - 1 , :] / temperature probs = F . softmax ( next_token_logits , dim =- 1 ) # Sample next token next_token = torch . multinomial ( probs , 1 ) # Append to sequence generated = torch . cat ([ generated , next_token ], dim = 1 ) # Stop if all sequences generated <END> if ( next_token == char_to_idx [ '<END>' ]) . all (): break # Convert to SMILES smiles_list = [] for seq in generated : chars = [ idx_to_char [ idx . item ()] for idx in seq ] # Stop at <END> token if '<END>' in chars : chars = chars [: chars . index ( '<END>' )] smiles = '' . join ([ c for c in chars if c not in [ '<START>' , '<PAD>' ]]) smiles_list . append ( smiles ) return smiles_list Training Loop \u00b6 Train the conditional VAE with reconstruction and KL divergence losses: def vae_loss ( logits , target , mu , logvar , beta = 0.1 ): \"\"\" VAE loss = Reconstruction loss + \u03b2 * KL divergence \"\"\" # Reconstruction loss (cross-entropy) recon_loss = F . cross_entropy ( logits . reshape ( - 1 , logits . size ( - 1 )), target . reshape ( - 1 ), ignore_index = 0 ) # Ignore padding # KL divergence kl_loss = - 0.5 * torch . sum ( 1 + logvar - mu . pow ( 2 ) - logvar . exp ()) / mu . size ( 0 ) return recon_loss + beta * kl_loss , recon_loss , kl_loss def train_cvae ( model , train_loader , num_epochs = 50 , lr = 1e-3 , beta = 0.1 , device = 'cuda' ): \"\"\"Train conditional VAE\"\"\" model = model . to ( device ) optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) history = { 'total_loss' : [], 'recon_loss' : [], 'kl_loss' : []} for epoch in range ( num_epochs ): model . train () epoch_losses = { 'total' : 0 , 'recon' : 0 , 'kl' : 0 } for batch_idx , ( smiles_encoded , properties ) in enumerate ( train_loader ): smiles_encoded = smiles_encoded . to ( device ) properties = properties . to ( device ) # Forward pass logits , mu , logvar = model ( smiles_encoded , properties ) # Compute loss target = smiles_encoded [:, 1 :] # Shift by 1 for target total_loss , recon_loss , kl_loss = vae_loss ( logits , target , mu , logvar , beta ) # Backward pass optimizer . zero_grad () total_loss . backward () torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) optimizer . step () # Track losses epoch_losses [ 'total' ] += total_loss . item () epoch_losses [ 'recon' ] += recon_loss . item () epoch_losses [ 'kl' ] += kl_loss . item () # Average losses num_batches = len ( train_loader ) avg_total = epoch_losses [ 'total' ] / num_batches avg_recon = epoch_losses [ 'recon' ] / num_batches avg_kl = epoch_losses [ 'kl' ] / num_batches history [ 'total_loss' ] . append ( avg_total ) history [ 'recon_loss' ] . append ( avg_recon ) history [ 'kl_loss' ] . append ( avg_kl ) print ( f \"Epoch { epoch + 1 } / { num_epochs } - \" f \"Loss: { avg_total : .4f } (Recon: { avg_recon : .4f } , KL: { avg_kl : .4f } )\" ) return history # Example usage def main (): # Prepare data smiles_list , properties = prepare_dataset ( 'molecules.csv' ) char_to_idx , idx_to_char = create_vocabulary ( smiles_list ) dataset = MolecularDataset ( smiles_list , properties , char_to_idx ) train_loader = DataLoader ( dataset , batch_size = 64 , shuffle = True ) # Initialize model vocab_size = len ( char_to_idx ) model = ConditionalVAE ( vocab_size , num_properties = 3 ) # Train history = train_cvae ( model , train_loader , num_epochs = 50 , beta = 0.1 ) # Generate molecules with desired properties # Example: Generate molecules with specific LogP, MW, QED target_props = torch . tensor ([[ 0.5 , 0.0 , 0.8 ]]) # Normalized values generated_smiles = model . generate ( target_props , char_to_idx , idx_to_char ) print ( \" \\n Generated SMILES:\" ) for smiles in generated_smiles : mol = Chem . MolFromSmiles ( smiles ) if mol is not None : print ( f \" { smiles } - Valid molecule\" ) else : print ( f \" { smiles } - Invalid\" ) Evaluation and Analysis \u00b6 Evaluate the conditional VAE\u2019s performance: def evaluate_generation_quality ( model , test_loader , char_to_idx , idx_to_char , num_samples = 1000 ): \"\"\"Evaluate generation quality metrics\"\"\" model . eval () metrics = { 'validity' : 0 , 'uniqueness' : set (), 'reconstruction_accuracy' : 0 , 'property_correlation' : [] } with torch . no_grad (): # Test reconstruction total_correct = 0 total_tokens = 0 for smiles_encoded , properties in test_loader : logits , mu , logvar = model ( smiles_encoded . to ( model . device ), properties . to ( model . device )) # Calculate accuracy predictions = logits . argmax ( dim =- 1 ) target = smiles_encoded [:, 1 :] . to ( model . device ) correct = ( predictions == target ) & ( target != 0 ) # Exclude padding total_correct += correct . sum () . item () total_tokens += ( target != 0 ) . sum () . item () metrics [ 'reconstruction_accuracy' ] = total_correct / total_tokens # Test generation property_values = torch . randn ( num_samples , 3 ) # Sample random properties generated_smiles = model . generate ( property_values , char_to_idx , idx_to_char ) for smiles in generated_smiles : mol = Chem . MolFromSmiles ( smiles ) if mol is not None : metrics [ 'validity' ] += 1 metrics [ 'uniqueness' ] . add ( smiles ) metrics [ 'validity' ] /= num_samples metrics [ 'uniqueness' ] = len ( metrics [ 'uniqueness' ]) / num_samples return metrics def visualize_latent_space ( model , data_loader , properties_df ): \"\"\"Visualize latent space with t-SNE\"\"\" from sklearn.manifold import TSNE import matplotlib.pyplot as plt model . eval () latent_vectors = [] property_values = [] with torch . no_grad (): for smiles_encoded , properties in data_loader : mu , _ = model . encoder ( smiles_encoded . to ( model . device ), properties . to ( model . device )) latent_vectors . append ( mu . cpu () . numpy ()) property_values . append ( properties . cpu () . numpy ()) latent_vectors = np . vstack ( latent_vectors ) property_values = np . vstack ( property_values ) # Apply t-SNE tsne = TSNE ( n_components = 2 , random_state = 42 ) latent_2d = tsne . fit_transform ( latent_vectors ) # Plot colored by property (e.g., LogP) plt . figure ( figsize = ( 10 , 8 )) scatter = plt . scatter ( latent_2d [:, 0 ], latent_2d [:, 1 ], c = property_values [:, 0 ], cmap = 'viridis' , alpha = 0.6 ) plt . colorbar ( scatter , label = 'LogP' ) plt . xlabel ( 't-SNE 1' ) plt . ylabel ( 't-SNE 2' ) plt . title ( 'Latent Space Visualization' ) plt . show () def interpolate_molecules ( model , smiles1 , smiles2 , properties , char_to_idx , idx_to_char , steps = 5 ): \"\"\"Interpolate between two molecules in latent space\"\"\" model . eval () # Encode both molecules with torch . no_grad (): encoded1 = torch . tensor ([[ char_to_idx . get ( c , 0 ) for c in smiles1 ]]) . to ( model . device ) encoded2 = torch . tensor ([[ char_to_idx . get ( c , 0 ) for c in smiles2 ]]) . to ( model . device ) props = properties . to ( model . device ) mu1 , _ = model . encoder ( encoded1 , props ) mu2 , _ = model . encoder ( encoded2 , props ) # Interpolate interpolated = [] for alpha in np . linspace ( 0 , 1 , steps ): z_interp = alpha * mu1 + ( 1 - alpha ) * mu2 smiles = model . decoder . generate_from_latent ( z_interp , props , char_to_idx , idx_to_char )[ 0 ] interpolated . append ( smiles ) return interpolated Hyperparameter Tuning \u00b6 Key hyperparameters to tune: hyperparameters = { 'embed_dim' : [ 64 , 128 , 256 ], 'hidden_dim' : [ 128 , 256 , 512 ], 'latent_dim' : [ 32 , 64 , 128 ], 'num_layers' : [ 1 , 2 , 3 ], 'beta' : [ 0.01 , 0.1 , 1.0 ], # KL weight 'learning_rate' : [ 1e-4 , 1e-3 , 1e-2 ], 'batch_size' : [ 32 , 64 , 128 ], 'temperature' : [ 0.7 , 1.0 , 1.5 ] # For generation } Suggested \u03b2 annealing schedule for better training: def get_beta ( epoch , warmup_epochs = 10 , max_beta = 1.0 ): \"\"\"Gradually increase KL weight\"\"\" if epoch < warmup_epochs : return max_beta * ( epoch / warmup_epochs ) return max_beta Summary \u00b6 In this session, we explored various generative modeling approaches for molecular design: VAEs and GANs provide complementary approaches to learning molecular distributions, with VAEs offering interpretable latent spaces and GANs enabling high-quality generation through adversarial training. Autoregressive models (RNNs and Transformers) generate molecules sequentially, leveraging powerful language modeling techniques adapted for chemical structures. Diffusion models represent the state-of-the-art in many generation tasks, offering stable training and high-quality, diverse molecular samples. Reinforcement learning enables goal-directed optimization, steering generation towards molecules with desired properties through reward-based training. Conditional generation provides fine-grained control over molecular properties, enabling targeted design for specific applications. The practical implementation demonstrates how these concepts come together in a working system, from data preparation through model training to generation and evaluation. Additional Resources \u00b6 Papers \u00b6 VAEs : \u201cAutomatic Chemical Design Using a Data-Driven Continuous Representation of Molecules\u201d (G\u00f3mez-Bombarelli et al., 2018) GANs : \u201cMolGAN: An implicit generative model for small molecular graphs\u201d (De Cao & Kipf, 2018) Transformers : \u201cMolecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\u201d (Schwaller et al., 2019) Diffusion : \u201cEquivariant Diffusion for Molecule Generation in 3D\u201d (Hoogeboom et al., 2022) RL : \u201cOptimization of Molecules via Deep Reinforcement Learning\u201d (Zhou et al., 2019) Libraries and Tools \u00b6 RDKit : Cheminformatics toolkit for molecular manipulation PyTorch Geometric : Graph neural network library Transformers (Hugging Face) : Pre-trained transformer models GuacaMol : Benchmarking platform for generative models MOSES : Molecular sets for evaluation Datasets \u00b6 ZINC : 230M commercially available compounds ChEMBL : Bioactive molecules with drug-like properties QM9 : 134k molecules with quantum properties USPTO : Chemical reaction dataset for synthesis planning Exercises \u00b6 Extend the conditional VAE to include additional properties (e.g., number of rotatable bonds, number of aromatic rings) Implement sampling strategies : Compare greedy decoding, beam search, and nucleus sampling for molecule generation Add validity constraints : Modify the decoder to enforce SMILES grammar rules and improve validity rate Multi-objective conditioning : Train a model to generate molecules satisfying multiple property constraints simultaneously Latent space arithmetic : Explore property modification by manipulating latent representations (e.g., z_new = z_base + \u03b4z_property) Benchmark your model : Evaluate on standard metrics (validity, uniqueness, novelty, FCD, KL divergence) and compare with baselines Incorporate 3D structure : Extend the model to condition on or generate 3D molecular conformations Active learning loop : Implement an iterative generation and selection process where predicted properties guide the next generation round","title":"Generative Models"},{"location":"day4/#day__4__generative__models__for__molecular__design","text":"","title":"Day 4: Generative Models for Molecular Design"},{"location":"day4/#overview","text":"Generative models have revolutionized computational drug discovery and molecular design by enabling the creation of novel molecular structures with desired properties. This session explores state-of-the-art generative approaches and their application to molecular generation.","title":"Overview"},{"location":"day4/#learning__objectives","text":"By the end of this session, you will be able to: - Understand the principles behind various generative model architectures - Apply VAEs and GANs to molecular generation tasks - Implement autoregressive models for sequential molecular design - Utilize diffusion models for high-quality molecule generation - Integrate reinforcement learning for property optimization - Develop conditional generation systems for targeted molecular design - Build and train a conditional VAE for molecular generation","title":"Learning Objectives"},{"location":"day4/#1__vaes__and__gans__for__molecular__generation","text":"","title":"1. VAEs and GANs for Molecular Generation"},{"location":"day4/#variational__autoencoders__vaes","text":"Variational Autoencoders learn a continuous latent representation of molecules, enabling smooth interpolation and sampling of chemical space.","title":"Variational Autoencoders (VAEs)"},{"location":"day4/#architecture","text":"The VAE consists of two main components: Encoder : Maps molecules to a probabilistic latent space - Input: SMILES string or molecular graph - Output: Mean (\u03bc) and log-variance (log \u03c3\u00b2) of latent distribution - Typically uses RNN/GRU/LSTM or Graph Neural Networks Decoder : Reconstructs molecules from latent representations - Input: Sampled latent vector z ~ N(\u03bc, \u03c3\u00b2) - Output: Reconstructed molecular representation - Generates SMILES character-by-character or reconstructs graph","title":"Architecture"},{"location":"day4/#loss__function","text":"The VAE optimizes two objectives: L_VAE = L_reconstruction + \u03b2 \u00d7 L_KL Reconstruction Loss : Ensures accurate molecule reconstruction (e.g., cross-entropy for SMILES) KL Divergence : Regularizes latent space to follow standard normal distribution N(0, I) \u03b2 parameter : Controls the trade-off between reconstruction quality and latent space regularity","title":"Loss Function"},{"location":"day4/#molecular__representations","text":"Common representations for VAE-based molecular generation: SMILES-based : Character-level sequence modeling Graph-based : Node and edge features with graph autoencoders Fingerprint-based : Fixed-length binary or count vectors 3D conformations : Atomic coordinates and features","title":"Molecular Representations"},{"location":"day4/#advantages","text":"Continuous latent space enables interpolation between molecules Sampling generates diverse molecular structures Can be trained unsupervised on large molecular databases","title":"Advantages"},{"location":"day4/#challenges","text":"Reconstruction of valid molecules can be difficult Mode collapse in complex chemical spaces Balancing reconstruction quality and latent space organization","title":"Challenges"},{"location":"day4/#generative__adversarial__networks__gans","text":"GANs consist of two competing neural networks that learn to generate realistic molecular structures.","title":"Generative Adversarial Networks (GANs)"},{"location":"day4/#architecture_1","text":"Generator (G) : Creates fake molecules from random noise - Input: Random noise vector z ~ N(0, I) - Output: Generated molecular representation (SMILES, graph, etc.) - Learns to fool the discriminator Discriminator (D) : Distinguishes real molecules from generated ones - Input: Molecular representation (real or generated) - Output: Probability that input is real - Learns to identify fake molecules","title":"Architecture"},{"location":"day4/#training__process","text":"The networks play a minimax game: min_G max_D V(D, G) = E[log D(x)] + E[log(1 - D(G(z)))] Training alternates between: 1. Update discriminator to better distinguish real from fake 2. Update generator to better fool discriminator","title":"Training Process"},{"location":"day4/#molecular__gans__variants","text":"MolGAN : Graph-based GAN for molecular generation - Generates molecular graphs directly - Uses permutation-invariant discriminator - Includes reward network for property optimization Objective-Reinforced GAN (ORGAN) : Combines GAN with RL - Adds policy gradient for optimizing molecular properties - Generator receives rewards for desired properties - Balances diversity with property optimization","title":"Molecular GANs Variants"},{"location":"day4/#advantages_1","text":"Can generate highly realistic molecular distributions No explicit likelihood computation needed Flexible in incorporating domain knowledge","title":"Advantages"},{"location":"day4/#challenges_1","text":"Training instability and mode collapse Difficulty ensuring chemical validity Requires careful hyperparameter tuning","title":"Challenges"},{"location":"day4/#2__autoregressive__models__rnns__transformers","text":"Autoregressive models generate molecules sequentially, one token at a time, based on previously generated tokens.","title":"2. Autoregressive Models (RNNs, Transformers)"},{"location":"day4/#recurrent__neural__networks__rnns","text":"","title":"Recurrent Neural Networks (RNNs)"},{"location":"day4/#architecture_2","text":"RNNs process SMILES strings character-by-character: h_t = f(h_{t-1}, x_t) p(x_t | x_1, ..., x_{t-1}) = softmax(W_h h_t + b) LSTM/GRU variants address vanishing gradient problems and capture long-range dependencies in molecular structures.","title":"Architecture"},{"location":"day4/#training","text":"Models are trained on large databases of molecules using teacher forcing: - Input: SMILES prefixes (e.g., \u201cCC(=O)\u201d) - Target: Next character (e.g., \u201cO\u201d) - Loss: Cross-entropy between predicted and actual next character","title":"Training"},{"location":"day4/#generation","text":"Sample molecules by iteratively predicting the next character: 1. Start with begin token 2. Sample next character from probability distribution 3. Append to sequence and repeat 4. Stop at end token or max length","title":"Generation"},{"location":"day4/#transfer__learning","text":"Pre-trained RNN language models can be fine-tuned for specific molecular design tasks: - Pre-train on large molecular databases (ChEMBL, ZINC) - Fine-tune on smaller target datasets with desired properties - Enables generation of molecules similar to training distribution","title":"Transfer Learning"},{"location":"day4/#transformers__for__molecular__generation","text":"Transformers leverage self-attention mechanisms for improved sequence modeling.","title":"Transformers for Molecular Generation"},{"location":"day4/#architecture_3","text":"Self-Attention : Allows model to attend to all positions simultaneously Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V Multi-Head Attention : Parallel attention mechanisms capture different relationships Positional Encoding : Injects sequence order information","title":"Architecture"},{"location":"day4/#advantages__over__rnns","text":"Parallel processing enables faster training Better capture of long-range dependencies More effective at learning complex molecular patterns","title":"Advantages Over RNNs"},{"location":"day4/#molecular__transformer__applications","text":"SMILES Generation : Character-level transformer language models - Pre-trained on millions of molecules - Fine-tuned for specific chemical spaces Reaction Prediction : Transform reactants to products - Trained on reaction datasets (USPTO) - Can predict reaction outcomes and suggest synthesis routes Molecular Translation : Convert between representations - SMILES to IUPAC names - 2D to 3D structure prediction - Retrosynthesis planning","title":"Molecular Transformer Applications"},{"location":"day4/#notable__models","text":"ChemBERTa : Transformer pre-trained on SMILES - Masked language modeling on molecular data - Transfer learning for downstream tasks MolGPT : GPT-based architecture for molecules - Autoregressive generation of SMILES - Can be conditioned on desired properties","title":"Notable Models"},{"location":"day4/#3__diffusion__models","text":"Diffusion models represent a recent paradigm shift in generative modeling, achieving state-of-the-art results across multiple domains including molecular generation.","title":"3. Diffusion Models"},{"location":"day4/#principles","text":"Diffusion models learn to reverse a gradual noising process: Forward Process : Progressively add Gaussian noise to data q(x_t | x_{t-1}) = N(x_t; \u221a(1-\u03b2_t)x_{t-1}, \u03b2_t I) Reverse Process : Learn to denoise and recover original data p_\u03b8(x_{t-1} | x_t) = N(x_{t-1}; \u03bc_\u03b8(x_t, t), \u03a3_\u03b8(x_t, t))","title":"Principles"},{"location":"day4/#training_1","text":"Train a neural network to predict the noise at each timestep: 1. Sample molecule x\u2080 from dataset 2. Sample timestep t and noise \u03b5 ~ N(0, I) 3. Create noisy version x_t 4. Train network to predict \u03b5 from x_t and t 5. Loss: MSE between predicted and actual noise","title":"Training"},{"location":"day4/#generation_1","text":"Generate molecules by iterative denoising: 1. Start with pure noise x_T ~ N(0, I) 2. Iteratively denoise: x_{t-1} = denoise(x_t, t) 3. Final sample x\u2080 is generated molecule","title":"Generation"},{"location":"day4/#molecular__diffusion__models","text":"Continuous Representations : Operate in latent space or coordinate space - E(n) Equivariant Diffusion : Generates 3D molecular conformations - Respects rotational and translational symmetries - Directly outputs atomic positions Discrete Representations : Adapt diffusion to categorical data - Discrete Diffusion : For SMILES or graph generation - Multinomial diffusion over molecular tokens - Transition matrices for adding/removing atoms and bonds Conditional Diffusion : Guide generation towards desired properties - Add property information at each denoising step - Classifier guidance or classifier-free guidance - Control molecular properties during generation","title":"Molecular Diffusion Models"},{"location":"day4/#advantages_2","text":"High-quality, diverse samples Stable training compared to GANs Flexible conditioning mechanisms Can generate both 2D and 3D molecular structures","title":"Advantages"},{"location":"day4/#challenges_2","text":"Slower generation compared to one-shot methods Computational cost of iterative sampling Ensuring chemical validity in discrete spaces","title":"Challenges"},{"location":"day4/#4__reinforcement__learning__for__optimization","text":"Reinforcement learning enables goal-directed molecular generation by optimizing molecules for specific properties.","title":"4. Reinforcement Learning for Optimization"},{"location":"day4/#framework","text":"Treat molecular generation as a sequential decision-making problem: Agent : Generative model (policy) Environment : Chemical space State : Current partial molecule Action : Add/modify atom or bond Reward : Molecular property score (e.g., drug-likeness, binding affinity)","title":"Framework"},{"location":"day4/#policy__optimization","text":"","title":"Policy Optimization"},{"location":"day4/#reinforce__algorithm","text":"Update policy to maximize expected reward: \u2207_\u03b8 J(\u03b8) = E[\u2211_t R_t \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_t)] Baseline subtraction reduces variance: \u2207_\u03b8 J(\u03b8) = E[\u2211_t (R_t - b) \u2207_\u03b8 log \u03c0_\u03b8(a_t | s_t)]","title":"REINFORCE Algorithm"},{"location":"day4/#proximal__policy__optimization__ppo","text":"Constrains policy updates for stable training: L(\u03b8) = E[min(r_t(\u03b8)\u00c2_t, clip(r_t(\u03b8), 1-\u03b5, 1+\u03b5)\u00c2_t)] Where r_t(\u03b8) = \u03c0_\u03b8(a_t|s_t) / \u03c0_\u03b8_old(a_t|s_t)","title":"Proximal Policy Optimization (PPO)"},{"location":"day4/#reward__functions","text":"Design reward functions to capture desired properties: Single Objective : R = f(molecule) where f might be: - Binding affinity prediction - Synthetic accessibility score - QED (quantitative estimate of drug-likeness) - LogP (lipophilicity) Multi-Objective : R = w\u2081f\u2081(mol) + w\u2082f\u2082(mol) + ... + w\u2099f\u2099(mol) Reward Shaping : - Penalize invalid molecules - Reward intermediate steps - Balance exploration and exploitation","title":"Reward Functions"},{"location":"day4/#transfer__learning_1","text":"Pre-train generative model on molecular databases, then fine-tune with RL: Pre-training : Learn general molecular distribution RL Fine-tuning : Optimize for specific properties Constrained Optimization : Maintain molecular validity while optimizing","title":"Transfer Learning"},{"location":"day4/#applications","text":"De Novo Drug Design : Generate molecules with: - High predicted binding affinity to target protein - Good ADMET properties - Synthetic accessibility Lead Optimization : Modify existing molecules to: - Improve potency - Reduce toxicity - Optimize pharmacokinetics Multi-Parameter Optimization (MPO) : Balance multiple objectives - Efficacy vs. safety - Potency vs. selectivity - Activity vs. synthetic accessibility","title":"Applications"},{"location":"day4/#challenges_3","text":"Reward function design and balancing Sparse and delayed rewards Maintaining molecular diversity Mode collapse to few high-reward molecules","title":"Challenges"},{"location":"day4/#5__conditional__generation","text":"Conditional generation enables control over generated molecules by incorporating desired properties or constraints into the generation process.","title":"5. Conditional Generation"},{"location":"day4/#conditioning__strategies","text":"","title":"Conditioning Strategies"},{"location":"day4/#explicit__conditioning","text":"Concatenate property information with latent vector: z_cond = [z, c] where c represents condition (e.g., property value, class label)","title":"Explicit Conditioning"},{"location":"day4/#cross-attention__conditioning","text":"Use attention mechanisms to integrate conditions: - Query: Latent molecular representation - Key/Value: Condition embeddings - Allows flexible, learned integration of conditions","title":"Cross-Attention Conditioning"},{"location":"day4/#classifier__guidance","text":"Steer generation using property prediction model: \u2207_x log p(x|c) \u2248 \u2207_x log p(x) + \u03bb\u2207_x log p(c|x) Guide generation towards higher predicted property values.","title":"Classifier Guidance"},{"location":"day4/#types__of__conditions","text":"Scalar Properties : - Molecular weight - LogP - TPSA (topological polar surface area) - QED score Categorical Properties : - Compound class (kinase inhibitor, GPCR ligand, etc.) - Scaffold type - Functional groups present Structural Constraints : - Required substructures (pharmacophores) - Forbidden substructures (toxic moieties) - Scaffold constraints Target-Based : - Protein target (e.g., \u201cgenerate EGFR inhibitor\u201d) - Binding site information - Target protein sequence/structure","title":"Types of Conditions"},{"location":"day4/#conditional__vae__architecture","text":"Modify standard VAE to accept conditions: Encoder : q_\u03c6(z | x, c) = N(\u03bc_\u03c6(x, c), \u03c3_\u03c6(x, c)) Decoder : p_\u03b8(x | z, c) Prior : p(z | c) = N(\u03bc_prior(c), \u03c3_prior(c))","title":"Conditional VAE Architecture"},{"location":"day4/#multi-conditional__generation","text":"Generate molecules satisfying multiple constraints simultaneously: - Combine multiple property conditions - Use hierarchical conditioning - Balance competing objectives","title":"Multi-Conditional Generation"},{"location":"day4/#controllable__generation__workflow","text":"Train conditional model on labeled molecular dataset Specify desired properties for new molecule Generate candidates with specified properties Validate and filter generated molecules Iterate by adjusting conditions","title":"Controllable Generation Workflow"},{"location":"day4/#6__practical__building__a__conditional__vae","text":"This practical exercise guides you through implementing a conditional VAE for molecular generation.","title":"6. Practical: Building a Conditional VAE"},{"location":"day4/#dataset__preparation","text":"We\u2019ll use a subset of molecules with associated properties: import pandas as pd import numpy as np from rdkit import Chem from rdkit.Chem import Descriptors , QED import torch from torch.utils.data import Dataset , DataLoader # Load molecular data def prepare_dataset ( smiles_file , max_length = 120 ): \"\"\"Prepare SMILES dataset with properties\"\"\" df = pd . read_csv ( smiles_file ) # Calculate properties properties = [] valid_smiles = [] for smiles in df [ 'smiles' ]: mol = Chem . MolFromSmiles ( smiles ) if mol is not None : props = { 'logp' : Descriptors . MolLogP ( mol ), 'mw' : Descriptors . MolWt ( mol ), 'qed' : QED . qed ( mol ) } properties . append ( props ) valid_smiles . append ( smiles ) # Normalize properties props_df = pd . DataFrame ( properties ) props_normalized = ( props_df - props_df . mean ()) / props_df . std () return valid_smiles , props_normalized # Create vocabulary def create_vocabulary ( smiles_list ): \"\"\"Create character vocabulary from SMILES\"\"\" chars = set () for smiles in smiles_list : chars . update ( smiles ) chars = sorted ( list ( chars )) char_to_idx = { c : i + 1 for i , c in enumerate ( chars )} char_to_idx [ '<PAD>' ] = 0 char_to_idx [ '<START>' ] = len ( char_to_idx ) char_to_idx [ '<END>' ] = len ( char_to_idx ) idx_to_char = { v : k for k , v in char_to_idx . items ()} return char_to_idx , idx_to_char class MolecularDataset ( Dataset ): \"\"\"PyTorch dataset for molecular SMILES\"\"\" def __init__ ( self , smiles_list , properties , char_to_idx , max_length = 120 ): self . smiles_list = smiles_list self . properties = properties self . char_to_idx = char_to_idx self . max_length = max_length def __len__ ( self ): return len ( self . smiles_list ) def __getitem__ ( self , idx ): smiles = self . smiles_list [ idx ] props = self . properties . iloc [ idx ] . values # Encode SMILES encoded = [ self . char_to_idx [ '<START>' ]] encoded += [ self . char_to_idx [ c ] for c in smiles ] encoded += [ self . char_to_idx [ '<END>' ]] # Pad sequence if len ( encoded ) < self . max_length : encoded += [ self . char_to_idx [ '<PAD>' ]] * ( self . max_length - len ( encoded )) else : encoded = encoded [: self . max_length ] return torch . tensor ( encoded , dtype = torch . long ), torch . tensor ( props , dtype = torch . float32 )","title":"Dataset Preparation"},{"location":"day4/#model__architecture","text":"Implement the conditional VAE components: import torch.nn as nn import torch.nn.functional as F class ConditionalEncoder ( nn . Module ): \"\"\"Encoder: SMILES + properties -> latent distribution\"\"\" def __init__ ( self , vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers = 2 ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embed_dim , padding_idx = 0 ) self . lstm = nn . LSTM ( embed_dim + num_properties , hidden_dim , num_layers = num_layers , batch_first = True , bidirectional = True ) # Latent parameters self . fc_mu = nn . Linear ( hidden_dim * 2 , latent_dim ) self . fc_logvar = nn . Linear ( hidden_dim * 2 , latent_dim ) def forward ( self , x , properties ): # Embed SMILES embedded = self . embedding ( x ) # [batch, seq_len, embed_dim] # Concatenate properties to each timestep batch_size , seq_len = x . size () props_expanded = properties . unsqueeze ( 1 ) . expand ( - 1 , seq_len , - 1 ) lstm_input = torch . cat ([ embedded , props_expanded ], dim =- 1 ) # LSTM encoding _ , ( hidden , _ ) = self . lstm ( lstm_input ) # Combine forward and backward hidden states hidden = torch . cat ([ hidden [ - 2 ], hidden [ - 1 ]], dim = 1 ) # Latent distribution parameters mu = self . fc_mu ( hidden ) logvar = self . fc_logvar ( hidden ) return mu , logvar class ConditionalDecoder ( nn . Module ): \"\"\"Decoder: latent + properties -> SMILES\"\"\" def __init__ ( self , vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers = 2 ): super () . __init__ () self . embedding = nn . Embedding ( vocab_size , embed_dim , padding_idx = 0 ) self . lstm = nn . LSTM ( embed_dim + num_properties , hidden_dim , num_layers = num_layers , batch_first = True ) # Project latent to initial hidden state self . latent_to_hidden = nn . Linear ( latent_dim + num_properties , hidden_dim * num_layers ) self . latent_to_cell = nn . Linear ( latent_dim + num_properties , hidden_dim * num_layers ) self . output = nn . Linear ( hidden_dim , vocab_size ) self . num_layers = num_layers def forward ( self , z , properties , target_seq ): batch_size = z . size ( 0 ) # Combine latent and properties z_cond = torch . cat ([ z , properties ], dim = 1 ) # Initialize hidden and cell states h0 = self . latent_to_hidden ( z_cond ) . view ( batch_size , self . num_layers , - 1 ) . transpose ( 0 , 1 ) . contiguous () c0 = self . latent_to_cell ( z_cond ) . view ( batch_size , self . num_layers , - 1 ) . transpose ( 0 , 1 ) . contiguous () # Embed target sequence embedded = self . embedding ( target_seq ) # Add properties to each timestep seq_len = target_seq . size ( 1 ) props_expanded = properties . unsqueeze ( 1 ) . expand ( - 1 , seq_len , - 1 ) lstm_input = torch . cat ([ embedded , props_expanded ], dim =- 1 ) # LSTM decoding output , _ = self . lstm ( lstm_input , ( h0 , c0 )) # Project to vocabulary logits = self . output ( output ) return logits class ConditionalVAE ( nn . Module ): \"\"\"Complete Conditional VAE for molecular generation\"\"\" def __init__ ( self , vocab_size , embed_dim = 128 , hidden_dim = 256 , latent_dim = 64 , num_properties = 3 , num_layers = 2 ): super () . __init__ () self . encoder = ConditionalEncoder ( vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers ) self . decoder = ConditionalDecoder ( vocab_size , embed_dim , hidden_dim , latent_dim , num_properties , num_layers ) self . latent_dim = latent_dim def reparameterize ( self , mu , logvar ): \"\"\"Reparameterization trick\"\"\" std = torch . exp ( 0.5 * logvar ) eps = torch . randn_like ( std ) return mu + eps * std def forward ( self , x , properties ): # Encode mu , logvar = self . encoder ( x , properties ) # Reparameterize z = self . reparameterize ( mu , logvar ) # Decode (teacher forcing) # Input: all tokens except last, Target: all tokens except first decoder_input = x [:, : - 1 ] logits = self . decoder ( z , properties , decoder_input ) return logits , mu , logvar def generate ( self , properties , char_to_idx , idx_to_char , max_length = 120 , temperature = 1.0 ): \"\"\"Generate SMILES from properties\"\"\" self . eval () with torch . no_grad (): batch_size = properties . size ( 0 ) # Sample from prior z = torch . randn ( batch_size , self . latent_dim ) . to ( properties . device ) # Start with <START> token generated = torch . tensor ([[ char_to_idx [ '<START>' ]]] * batch_size ) . to ( properties . device ) # Generate sequence for _ in range ( max_length - 1 ): logits = self . decoder ( z , properties , generated ) # Get last token logits next_token_logits = logits [:, - 1 , :] / temperature probs = F . softmax ( next_token_logits , dim =- 1 ) # Sample next token next_token = torch . multinomial ( probs , 1 ) # Append to sequence generated = torch . cat ([ generated , next_token ], dim = 1 ) # Stop if all sequences generated <END> if ( next_token == char_to_idx [ '<END>' ]) . all (): break # Convert to SMILES smiles_list = [] for seq in generated : chars = [ idx_to_char [ idx . item ()] for idx in seq ] # Stop at <END> token if '<END>' in chars : chars = chars [: chars . index ( '<END>' )] smiles = '' . join ([ c for c in chars if c not in [ '<START>' , '<PAD>' ]]) smiles_list . append ( smiles ) return smiles_list","title":"Model Architecture"},{"location":"day4/#training__loop","text":"Train the conditional VAE with reconstruction and KL divergence losses: def vae_loss ( logits , target , mu , logvar , beta = 0.1 ): \"\"\" VAE loss = Reconstruction loss + \u03b2 * KL divergence \"\"\" # Reconstruction loss (cross-entropy) recon_loss = F . cross_entropy ( logits . reshape ( - 1 , logits . size ( - 1 )), target . reshape ( - 1 ), ignore_index = 0 ) # Ignore padding # KL divergence kl_loss = - 0.5 * torch . sum ( 1 + logvar - mu . pow ( 2 ) - logvar . exp ()) / mu . size ( 0 ) return recon_loss + beta * kl_loss , recon_loss , kl_loss def train_cvae ( model , train_loader , num_epochs = 50 , lr = 1e-3 , beta = 0.1 , device = 'cuda' ): \"\"\"Train conditional VAE\"\"\" model = model . to ( device ) optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) history = { 'total_loss' : [], 'recon_loss' : [], 'kl_loss' : []} for epoch in range ( num_epochs ): model . train () epoch_losses = { 'total' : 0 , 'recon' : 0 , 'kl' : 0 } for batch_idx , ( smiles_encoded , properties ) in enumerate ( train_loader ): smiles_encoded = smiles_encoded . to ( device ) properties = properties . to ( device ) # Forward pass logits , mu , logvar = model ( smiles_encoded , properties ) # Compute loss target = smiles_encoded [:, 1 :] # Shift by 1 for target total_loss , recon_loss , kl_loss = vae_loss ( logits , target , mu , logvar , beta ) # Backward pass optimizer . zero_grad () total_loss . backward () torch . nn . utils . clip_grad_norm_ ( model . parameters (), 1.0 ) optimizer . step () # Track losses epoch_losses [ 'total' ] += total_loss . item () epoch_losses [ 'recon' ] += recon_loss . item () epoch_losses [ 'kl' ] += kl_loss . item () # Average losses num_batches = len ( train_loader ) avg_total = epoch_losses [ 'total' ] / num_batches avg_recon = epoch_losses [ 'recon' ] / num_batches avg_kl = epoch_losses [ 'kl' ] / num_batches history [ 'total_loss' ] . append ( avg_total ) history [ 'recon_loss' ] . append ( avg_recon ) history [ 'kl_loss' ] . append ( avg_kl ) print ( f \"Epoch { epoch + 1 } / { num_epochs } - \" f \"Loss: { avg_total : .4f } (Recon: { avg_recon : .4f } , KL: { avg_kl : .4f } )\" ) return history # Example usage def main (): # Prepare data smiles_list , properties = prepare_dataset ( 'molecules.csv' ) char_to_idx , idx_to_char = create_vocabulary ( smiles_list ) dataset = MolecularDataset ( smiles_list , properties , char_to_idx ) train_loader = DataLoader ( dataset , batch_size = 64 , shuffle = True ) # Initialize model vocab_size = len ( char_to_idx ) model = ConditionalVAE ( vocab_size , num_properties = 3 ) # Train history = train_cvae ( model , train_loader , num_epochs = 50 , beta = 0.1 ) # Generate molecules with desired properties # Example: Generate molecules with specific LogP, MW, QED target_props = torch . tensor ([[ 0.5 , 0.0 , 0.8 ]]) # Normalized values generated_smiles = model . generate ( target_props , char_to_idx , idx_to_char ) print ( \" \\n Generated SMILES:\" ) for smiles in generated_smiles : mol = Chem . MolFromSmiles ( smiles ) if mol is not None : print ( f \" { smiles } - Valid molecule\" ) else : print ( f \" { smiles } - Invalid\" )","title":"Training Loop"},{"location":"day4/#evaluation__and__analysis","text":"Evaluate the conditional VAE\u2019s performance: def evaluate_generation_quality ( model , test_loader , char_to_idx , idx_to_char , num_samples = 1000 ): \"\"\"Evaluate generation quality metrics\"\"\" model . eval () metrics = { 'validity' : 0 , 'uniqueness' : set (), 'reconstruction_accuracy' : 0 , 'property_correlation' : [] } with torch . no_grad (): # Test reconstruction total_correct = 0 total_tokens = 0 for smiles_encoded , properties in test_loader : logits , mu , logvar = model ( smiles_encoded . to ( model . device ), properties . to ( model . device )) # Calculate accuracy predictions = logits . argmax ( dim =- 1 ) target = smiles_encoded [:, 1 :] . to ( model . device ) correct = ( predictions == target ) & ( target != 0 ) # Exclude padding total_correct += correct . sum () . item () total_tokens += ( target != 0 ) . sum () . item () metrics [ 'reconstruction_accuracy' ] = total_correct / total_tokens # Test generation property_values = torch . randn ( num_samples , 3 ) # Sample random properties generated_smiles = model . generate ( property_values , char_to_idx , idx_to_char ) for smiles in generated_smiles : mol = Chem . MolFromSmiles ( smiles ) if mol is not None : metrics [ 'validity' ] += 1 metrics [ 'uniqueness' ] . add ( smiles ) metrics [ 'validity' ] /= num_samples metrics [ 'uniqueness' ] = len ( metrics [ 'uniqueness' ]) / num_samples return metrics def visualize_latent_space ( model , data_loader , properties_df ): \"\"\"Visualize latent space with t-SNE\"\"\" from sklearn.manifold import TSNE import matplotlib.pyplot as plt model . eval () latent_vectors = [] property_values = [] with torch . no_grad (): for smiles_encoded , properties in data_loader : mu , _ = model . encoder ( smiles_encoded . to ( model . device ), properties . to ( model . device )) latent_vectors . append ( mu . cpu () . numpy ()) property_values . append ( properties . cpu () . numpy ()) latent_vectors = np . vstack ( latent_vectors ) property_values = np . vstack ( property_values ) # Apply t-SNE tsne = TSNE ( n_components = 2 , random_state = 42 ) latent_2d = tsne . fit_transform ( latent_vectors ) # Plot colored by property (e.g., LogP) plt . figure ( figsize = ( 10 , 8 )) scatter = plt . scatter ( latent_2d [:, 0 ], latent_2d [:, 1 ], c = property_values [:, 0 ], cmap = 'viridis' , alpha = 0.6 ) plt . colorbar ( scatter , label = 'LogP' ) plt . xlabel ( 't-SNE 1' ) plt . ylabel ( 't-SNE 2' ) plt . title ( 'Latent Space Visualization' ) plt . show () def interpolate_molecules ( model , smiles1 , smiles2 , properties , char_to_idx , idx_to_char , steps = 5 ): \"\"\"Interpolate between two molecules in latent space\"\"\" model . eval () # Encode both molecules with torch . no_grad (): encoded1 = torch . tensor ([[ char_to_idx . get ( c , 0 ) for c in smiles1 ]]) . to ( model . device ) encoded2 = torch . tensor ([[ char_to_idx . get ( c , 0 ) for c in smiles2 ]]) . to ( model . device ) props = properties . to ( model . device ) mu1 , _ = model . encoder ( encoded1 , props ) mu2 , _ = model . encoder ( encoded2 , props ) # Interpolate interpolated = [] for alpha in np . linspace ( 0 , 1 , steps ): z_interp = alpha * mu1 + ( 1 - alpha ) * mu2 smiles = model . decoder . generate_from_latent ( z_interp , props , char_to_idx , idx_to_char )[ 0 ] interpolated . append ( smiles ) return interpolated","title":"Evaluation and Analysis"},{"location":"day4/#hyperparameter__tuning","text":"Key hyperparameters to tune: hyperparameters = { 'embed_dim' : [ 64 , 128 , 256 ], 'hidden_dim' : [ 128 , 256 , 512 ], 'latent_dim' : [ 32 , 64 , 128 ], 'num_layers' : [ 1 , 2 , 3 ], 'beta' : [ 0.01 , 0.1 , 1.0 ], # KL weight 'learning_rate' : [ 1e-4 , 1e-3 , 1e-2 ], 'batch_size' : [ 32 , 64 , 128 ], 'temperature' : [ 0.7 , 1.0 , 1.5 ] # For generation } Suggested \u03b2 annealing schedule for better training: def get_beta ( epoch , warmup_epochs = 10 , max_beta = 1.0 ): \"\"\"Gradually increase KL weight\"\"\" if epoch < warmup_epochs : return max_beta * ( epoch / warmup_epochs ) return max_beta","title":"Hyperparameter Tuning"},{"location":"day4/#summary","text":"In this session, we explored various generative modeling approaches for molecular design: VAEs and GANs provide complementary approaches to learning molecular distributions, with VAEs offering interpretable latent spaces and GANs enabling high-quality generation through adversarial training. Autoregressive models (RNNs and Transformers) generate molecules sequentially, leveraging powerful language modeling techniques adapted for chemical structures. Diffusion models represent the state-of-the-art in many generation tasks, offering stable training and high-quality, diverse molecular samples. Reinforcement learning enables goal-directed optimization, steering generation towards molecules with desired properties through reward-based training. Conditional generation provides fine-grained control over molecular properties, enabling targeted design for specific applications. The practical implementation demonstrates how these concepts come together in a working system, from data preparation through model training to generation and evaluation.","title":"Summary"},{"location":"day4/#additional__resources","text":"","title":"Additional Resources"},{"location":"day4/#papers","text":"VAEs : \u201cAutomatic Chemical Design Using a Data-Driven Continuous Representation of Molecules\u201d (G\u00f3mez-Bombarelli et al., 2018) GANs : \u201cMolGAN: An implicit generative model for small molecular graphs\u201d (De Cao & Kipf, 2018) Transformers : \u201cMolecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction\u201d (Schwaller et al., 2019) Diffusion : \u201cEquivariant Diffusion for Molecule Generation in 3D\u201d (Hoogeboom et al., 2022) RL : \u201cOptimization of Molecules via Deep Reinforcement Learning\u201d (Zhou et al., 2019)","title":"Papers"},{"location":"day4/#libraries__and__tools","text":"RDKit : Cheminformatics toolkit for molecular manipulation PyTorch Geometric : Graph neural network library Transformers (Hugging Face) : Pre-trained transformer models GuacaMol : Benchmarking platform for generative models MOSES : Molecular sets for evaluation","title":"Libraries and Tools"},{"location":"day4/#datasets","text":"ZINC : 230M commercially available compounds ChEMBL : Bioactive molecules with drug-like properties QM9 : 134k molecules with quantum properties USPTO : Chemical reaction dataset for synthesis planning","title":"Datasets"},{"location":"day4/#exercises","text":"Extend the conditional VAE to include additional properties (e.g., number of rotatable bonds, number of aromatic rings) Implement sampling strategies : Compare greedy decoding, beam search, and nucleus sampling for molecule generation Add validity constraints : Modify the decoder to enforce SMILES grammar rules and improve validity rate Multi-objective conditioning : Train a model to generate molecules satisfying multiple property constraints simultaneously Latent space arithmetic : Explore property modification by manipulating latent representations (e.g., z_new = z_base + \u03b4z_property) Benchmark your model : Evaluate on standard metrics (validity, uniqueness, novelty, FCD, KL divergence) and compare with baselines Incorporate 3D structure : Extend the model to condition on or generate 3D molecular conformations Active learning loop : Implement an iterative generation and selection process where predicted properties guide the next generation round","title":"Exercises"},{"location":"day5/","text":"Day 5: Advanced Applications and Practical Integration \u00b6 Overview \u00b6 Day 5 focuses on applying machine learning to real-world molecular and drug discovery challenges. This session bridges the gap between theoretical knowledge and practical implementation, covering state-of-the-art applications in computational chemistry, drug discovery, and emerging technologies. Learning Objectives: - Apply ML models to reaction prediction and retrosynthetic planning - Implement protein-ligand docking workflows with ML enhancement - Integrate molecular dynamics simulations with machine learning - Accelerate quantum chemistry calculations using ML surrogates - Design end-to-end drug discovery pipelines - Deploy ML models for production use - Explore cutting-edge developments in molecular AI 1. Reaction Prediction and Retrosynthesis \u00b6 1.1 Forward Reaction Prediction \u00b6 Forward reaction prediction involves predicting the products of a chemical reaction given reactants and conditions. Key Concepts: - Reaction SMILES : Encoding reactions as reactants>>products - Template-based methods : Using reaction templates extracted from databases - Template-free methods : Sequence-to-sequence models treating reactions as translation tasks - Graph-based methods : Operating directly on molecular graphs Popular Models: - Molecular Transformer : Attention-based sequence models for reaction prediction - Graph2SMILES : Graph neural networks with SMILES generation - LocalRetro : Template-free retrosynthesis using local reaction patterns Implementation Example: from rxnfp.transformer_fingerprints import RXNBERTFingerprintGenerator from rdkit import Chem # Initialize reaction fingerprint generator rxnfp_generator = RXNBERTFingerprintGenerator () # Define a reaction reaction_smiles = \"CC(=O)O.CCO>>CC(=O)OCC.O\" # Generate reaction fingerprint fingerprint = rxnfp_generator . convert ( reaction_smiles ) # Use for similarity search or downstream prediction 1.2 Retrosynthetic Planning \u00b6 Retrosynthesis involves working backwards from a target molecule to identify synthetic routes using available starting materials. Approaches: - Single-step retrosynthesis : Predicting immediate precursors - Multi-step planning : Building full synthetic trees - Search algorithms : Monte Carlo Tree Search (MCTS), best-first search - Cost optimization : Balancing route complexity, yield, and availability Tools and Frameworks: - AiZynthFinder : Open-source retrosynthesis planning with MCTS - IBM RXN for Chemistry : Cloud-based reaction prediction platform - Molecule.one : Commercial retrosynthesis software Practical Considerations: - Starting material availability - Reaction condition feasibility - Stereochemistry preservation - Scalability to industrial synthesis 2. Protein-Ligand Docking \u00b6 2.1 Traditional Docking Methods \u00b6 Protein-ligand docking predicts the binding pose and affinity of small molecules to protein targets. Classical Approaches: - AutoDock Vina : Scoring function-based docking - Glide : Precision docking with MM-GBSA rescoring - GOLD : Genetic algorithm-based pose prediction Limitations: - Computational expense for large libraries - Rigid protein approximation - Scoring function accuracy 2.2 ML-Enhanced Docking \u00b6 Machine learning improves docking through better scoring functions, pose prediction, and virtual screening. ML Applications: - Scoring function refinement : Neural networks trained on binding affinity data - Pose ranking : Graph neural networks for pose selection - Virtual screening : Rapid filtering before expensive calculations - Protein flexibility : ML-based conformer generation Modern Tools: - DeepDock : Deep learning for binding pose prediction - Gnina : CNN-based scoring for AutoDock Vina - EquiBind : SE(3)-equivariant network for direct pose prediction - DiffDock : Diffusion models for blind docking Implementation Workflow: from gnina import Gnina from rdkit import Chem # Initialize Gnina docker = Gnina () # Prepare inputs protein_file = \"protein.pdb\" ligand_file = \"ligand.sdf\" # Run docking with CNN scoring results = docker . dock ( protein = protein_file , ligand = ligand_file , center = [ x , y , z ], size = [ 20 , 20 , 20 ], num_modes = 10 ) # Extract top poses and scores for pose , score in results : print ( f \"Score: { score : .2f } \" ) 2.3 Binding Affinity Prediction \u00b6 Beyond docking, ML models predict binding affinity (pKd, pIC50) directly from structure. Approaches: - Structure-based : Using 3D protein-ligand complexes - Ligand-based : QSAR models using molecular descriptors - Hybrid : Combining structural and chemical features Popular Models: - DeepDTA : Deep learning for drug-target affinity - GraphDTA : Graph neural networks for affinity prediction - KDEEP : Kernelized deep learning approach 3. Molecular Dynamics with ML \u00b6 3.1 ML Force Fields \u00b6 Machine learning force fields (MLFFs) provide quantum mechanical accuracy at classical MD speed. Key Models: - ANI : Accurate neural network potentials for organic molecules - SchNet : Continuous-filter convolutional networks - PaiNN : Polarizable atom interaction neural networks - MACE : Multi-atomic cluster expansion with higher-order interactions Advantages: - 100-1000x faster than ab initio MD - Quantum accuracy for energies and forces - Transferable across chemical space Implementation Example: import torch from torchani import ANI2x # Load ANI-2x model model = ANI2x () # Prepare molecular coordinates species = torch . tensor ([[ 1 , 6 , 6 , 1 , 1 , 1 , 1 ]]) # H-C-C-H... coordinates = torch . tensor ([[[ ... ], [ ... ], ... ]]) # Angstroms # Compute energy and forces energy = model (( species , coordinates )) . energies forces = - torch . autograd . grad ( energy . sum (), coordinates )[ 0 ] print ( f \"Energy: { energy . item () : .4f } Hartree\" ) 3.2 Enhanced Sampling with ML \u00b6 ML accelerates exploration of conformational space in MD simulations. Techniques: - Collective variable identification : Autoencoders for reaction coordinates - Biasing potentials : Neural network-based metadynamics - Trajectory reweighting : Correcting for ML force field errors - Rare event sampling : Reinforcement learning for transition path sampling Applications: - Protein folding pathways - Ligand binding/unbinding kinetics - Free energy calculations - Conformational transitions 3.3 Coarse-Grained Modeling \u00b6 ML enables learned coarse-grained (CG) representations for larger spatiotemporal scales. Approaches: - Bottom-up : Deriving CG models from atomistic simulations - Top-down : Learning directly from experimental data - Backmapping : Reconstructing atomistic details from CG 4. Quantum Chemistry Acceleration \u00b6 4.1 ML as QM Surrogates \u00b6 Machine learning models replace expensive quantum chemistry calculations. Applications: - Energy prediction : DFT energy at GNN speed - Property prediction : HOMO-LUMO gaps, dipole moments, polarizabilities - Wavefunction approximation : Neural network wavefunctions - Density functional approximation : Learning exchange-correlation functionals Popular Datasets: - QM9 : 134k small organic molecules with DFT properties - OE62 : Organic reactions with transition state energies - MD17 : Molecular dynamics trajectories for benzene, aspirin, etc. - GEOM : 37M conformers with GFN2-xTB energies 4.2 Delta Learning \u00b6 Delta learning combines ML with lower-level quantum methods to achieve higher accuracy. Concept: E_high \u2248 E_low + \u0394E_ML Where: - E_high : Target high-level method (e.g., CCSD(T)) - E_low : Fast low-level method (e.g., DFT) - \u0394E_ML : ML-predicted correction Benefits: - Reduces data requirements - Improves extrapolation - Physically motivated architecture 4.3 Active Learning for QM \u00b6 Active learning selects the most informative molecules for expensive QM calculations. Workflow: 1. Train initial ML model on small dataset 2. Use uncertainty quantification to identify uncertain predictions 3. Run QM calculations on uncertain molecules 4. Retrain model with augmented data 5. Repeat until convergence Uncertainty Methods: - Ensemble disagreement - Bayesian neural networks - MC dropout - Gaussian process regression 5. End-to-End Drug Discovery Workflows \u00b6 5.1 Pipeline Architecture \u00b6 Modern ML-driven drug discovery integrates multiple components into cohesive workflows. Typical Pipeline Stages: Target Identification \u2192 Hit Discovery \u2192 Lead Optimization \u2192 Preclinical \u2192 Clinical \u2193 \u2193 \u2193 \u2193 \u2193 ML: Target ML: Virtual ML: Property ML: ADMET ML: Clinical prediction screening optimization prediction trial design 5.2 Multi-Objective Optimization \u00b6 Drug candidates must satisfy multiple constraints simultaneously. Objectives: - Potency : High binding affinity to target - Selectivity : Low off-target binding - ADMET : Good absorption, distribution, metabolism, excretion, toxicity - Synthesizability : Feasible synthetic routes - Physicochemical properties : Drug-likeness (Lipinski\u2019s rules) Optimization Approaches: - Pareto optimization : Finding non-dominated solutions - Weighted scalarization : Combining objectives into single score - Constraint satisfaction : Hard constraints + objective optimization - Multi-task learning : Joint prediction of all properties Tools: - GuacaMol : Benchmarking for molecular design - MOSES : Molecular sets for generative models - Therapeutics Data Commons (TDC) : Unified ML tasks for drug discovery 5.3 Integration with Laboratory Automation \u00b6 Closing the loop between computational prediction and experimental validation. Design-Make-Test-Analyze (DMTA) Cycles: - Design : ML generates candidate molecules - Make : Automated synthesis or compound ordering - Test : High-throughput screening (HTS) - Analyze : ML learns from results, iterates design Infrastructure: - Robotic synthesis platforms - Automated assay systems - LIMS (Laboratory Information Management Systems) - Real-time data feedback 6. Model Deployment and Production \u00b6 6.1 Deployment Strategies \u00b6 Moving ML models from research to production environments. Deployment Options: - REST APIs : Flask, FastAPI for web services - Containerization : Docker for reproducible environments - Cloud platforms : AWS SageMaker, Google Cloud AI Platform - Edge deployment : ONNX for mobile/embedded devices Example FastAPI Deployment: from fastapi import FastAPI from pydantic import BaseModel import torch app = FastAPI () # Load pre-trained model model = torch . load ( \"model.pt\" ) model . eval () class MoleculeInput ( BaseModel ): smiles : str @app . post ( \"/predict\" ) def predict_property ( mol : MoleculeInput ): # Convert SMILES to features features = smiles_to_features ( mol . smiles ) # Make prediction with torch . no_grad (): prediction = model ( features ) return { \"prediction\" : prediction . item ()} 6.2 Model Monitoring and Maintenance \u00b6 Ensuring model performance in production. Key Considerations: - Data drift : Detecting distribution shifts in input data - Concept drift : Changes in input-output relationships - Performance monitoring : Tracking prediction accuracy over time - Retraining triggers : Automated model updates - A/B testing : Comparing model versions Monitoring Tools: - Evidently AI - Fiddler - Arize AI - Weights & Biases 6.3 Reproducibility and Version Control \u00b6 Best practices for ML in production. Essential Components: - Code version control : Git, GitHub - Data versioning : DVC, LakeFS - Model versioning : MLflow, Weights & Biases - Environment management : Conda, Docker - Experiment tracking : MLflow, Neptune.ai MLflow Example: import mlflow from mlflow.models import infer_signature # Start MLflow run with mlflow . start_run (): # Train model model = train_model ( train_data ) # Log parameters mlflow . log_params ({ \"learning_rate\" : 0.001 , \"epochs\" : 100 }) # Log metrics mlflow . log_metrics ({ \"mae\" : 0.5 , \"r2\" : 0.85 }) # Log model signature = infer_signature ( X_test , predictions ) mlflow . sklearn . log_model ( model , \"model\" , signature = signature ) 6.4 Regulatory Considerations \u00b6 ML models for drug discovery face regulatory scrutiny. FDA Guidance: - Model transparency and interpretability - Validation on diverse datasets - Handling of edge cases and uncertainty - Audit trails and documentation - Bias detection and mitigation Good Machine Learning Practice (GMLP): - Data quality assurance - Model validation and testing - Risk management - Continuous monitoring - Stakeholder engagement 7. Future Directions \u00b6 7.1 Foundation Models for Molecules \u00b6 Large-scale pre-trained models that can be fine-tuned for various downstream tasks. Current Developments: - MolBERT : BERT-like pre-training on SMILES - ChemBERTa : RoBERTa for chemical language - MolFormer : Transformer with 1B parameters trained on 1.1B molecules - UniMol : 3D molecular pre-training with geometric information - Galactica : General-purpose scientific language model including chemistry Advantages: - Transfer learning across tasks - Few-shot learning capabilities - Emergent understanding of chemical principles - Democratization of ML in chemistry Challenges: - Computational cost of training - Data curation at scale - Evaluation benchmarks - Interpretability of large models 7.2 Autonomous Laboratories \u00b6 Self-driving labs combine ML, robotics, and automation for autonomous experimentation. Components: - Robotic synthesis : Automated chemical synthesis platforms - High-throughput characterization : Rapid property measurement - ML planning : Bayesian optimization, active learning - Closed-loop control : Real-time experiment adaptation Examples: - Chemspeed : Automated synthesis and screening - Emerald Cloud Lab : Remote-access automated lab - IBM RoboRXN : Autonomous synthesis planning and execution - Material acceleration platforms : Rapid materials discovery Impact: - 10-100x acceleration of discovery - 24/7 operation - Exploration of unconventional chemistry - Reduced human bias 7.3 Quantum Machine Learning \u00b6 Intersection of quantum computing and ML for molecular science. Quantum Advantages: - Quantum simulation : Efficient simulation of quantum systems - Variational algorithms : VQE for ground state energies - Quantum kernels : Enhanced feature spaces for ML - Quantum neural networks : Parameterized quantum circuits Near-Term Applications: - Hybrid classical-quantum algorithms : Quantum hardware accelerates specific steps - Quantum-enhanced sampling : Improved exploration of molecular space - Error-mitigated chemistry : Quantum chemistry despite hardware noise Frameworks: - Qiskit : IBM\u2019s quantum computing SDK - Cirq : Google\u2019s quantum programming framework - PennyLane : Quantum ML library with autodifferentiation - TensorFlow Quantum : Integration of quantum computing with TensorFlow Example - Variational Quantum Eigensolver: from qiskit.algorithms import VQE from qiskit.primitives import Estimator from qiskit.circuit.library import RealAmplitudes from qiskit_nature.second_q.drivers import PySCFDriver # Define molecular system driver = PySCFDriver ( atom = \"H 0 0 0; H 0 0 0.735\" ) problem = driver . run () # Set up VQE ansatz = RealAmplitudes ( num_qubits = 4 , reps = 2 ) vqe = VQE ( Estimator (), ansatz , optimizer = SLSQP ()) # Compute ground state energy result = vqe . compute_minimum_eigenvalue ( problem . hamiltonian ) print ( f \"Ground state energy: { result . eigenvalue : .6f } Hartree\" ) 7.4 Multimodal Learning \u00b6 Integrating diverse data types for comprehensive molecular understanding. Data Modalities: - Chemical structure : SMILES, graphs, 3D conformers - Spectroscopy : NMR, IR, MS, UV-Vis - Images : Microscopy, crystallography - Text : Literature, patents, experimental protocols - Bioactivity : Screening data, clinical outcomes Approaches: - Cross-modal pre-training : Learning shared representations - Late fusion : Combining predictions from separate models - Attention mechanisms : Learning to weight different modalities - Contrastive learning : Aligning representations across modalities Applications: - Structure elucidation from spectra - Literature-guided molecule generation - Predicting experimental conditions from desired outcomes - Hypothesis generation from multimodal data 7.5 Explainable AI for Chemistry \u00b6 Making ML predictions interpretable for chemists. Techniques: - Attention visualization : Highlighting important molecular substructures - SHAP values : Quantifying feature importance - Counterfactual explanations : \u201cWhat if\u201d molecular modifications - Causal inference : Distinguishing correlation from causation - Mechanistic modeling : Hybrid physics-ML models Benefits: - Building trust with domain experts - Accelerating hypothesis generation - Regulatory compliance - Identifying model failures - Scientific discovery beyond prediction Practical Exercises \u00b6 Exercise 1: Retrosynthesis Planning \u00b6 Use AiZynthFinder to plan a synthesis route for ibuprofen starting from commercially available materials. Exercise 2: ML-Enhanced Docking \u00b6 Compare traditional AutoDock Vina with Gnina on a set of protein-ligand complexes and evaluate scoring improvements. Exercise 3: MD with ML Force Fields \u00b6 Run a short MD simulation of alanine dipeptide using both classical AMBER and ANI-2x force fields. Compare trajectories and computational cost. Exercise 4: Drug Discovery Pipeline \u00b6 Build an end-to-end pipeline that: (1) generates molecules with desired properties, (2) predicts ADMET, (3) performs virtual screening, and (4) suggests top candidates. Exercise 5: Model Deployment \u00b6 Deploy a solubility prediction model as a REST API using FastAPI and test it with various SMILES inputs. Additional Resources \u00b6 Research Papers \u00b6 \u201cMachine learning for molecular and materials science\u201d - Nature (2018) \u201cRetrosynthesis prediction with conditional graph logic network\u201d - NeurIPS (2019) \u201cEquiBind: Geometric deep learning for drug binding structure prediction\u201d - ICML (2022) \u201cMachine learning force fields\u201d - Chemical Reviews (2021) \u201cThe rise of diffusion models in drug discovery\u201d - Nature Communications (2024) Software Tools \u00b6 RDKit : Cheminformatics toolkit DeepChem : Deep learning for chemistry OpenMM : Molecular dynamics simulation TorchDrug : PyTorch library for drug discovery Therapeutics Data Commons : ML benchmarks for drug discovery Databases \u00b6 ChEMBL : Bioactive molecules with drug-like properties PubChem : Chemical information database PDB : Protein Data Bank ZINC : Commercial compounds for virtual screening BindingDB : Binding affinities for protein-ligand complexes Online Courses & Tutorials \u00b6 DeepChem tutorials : Practical ML for drug discovery RDKit tutorials : Molecular manipulation and analysis Papers with Code : Implementations of recent papers MIT Deep Learning for Molecules : Course materials Summary \u00b6 Day 5 demonstrated how machine learning transforms molecular science from theoretical concepts to practical applications. Key takeaways include: Reaction prediction and retrosynthesis enable automated synthesis planning ML-enhanced docking improves virtual screening efficiency and accuracy ML force fields bring quantum accuracy to molecular dynamics at reduced cost Quantum chemistry acceleration makes high-level calculations accessible End-to-end workflows integrate ML throughout drug discovery pipelines Production deployment requires careful attention to monitoring, versioning, and reproducibility Future directions including foundation models, autonomous labs, and quantum ML promise revolutionary advances The integration of these techniques creates powerful tools for accelerating scientific discovery while maintaining rigor and interpretability. Next Steps \u00b6 Explore hands-on implementations using the provided code examples Join online communities (RDKit discussions, DeepChem forums) Stay current with literature (preprints on arXiv, ChemRxiv) Contribute to open-source molecular ML projects Apply these techniques to your own research problems","title":"Advanced Models"},{"location":"day5/#day__5__advanced__applications__and__practical__integration","text":"","title":"Day 5: Advanced Applications and Practical Integration"},{"location":"day5/#overview","text":"Day 5 focuses on applying machine learning to real-world molecular and drug discovery challenges. This session bridges the gap between theoretical knowledge and practical implementation, covering state-of-the-art applications in computational chemistry, drug discovery, and emerging technologies. Learning Objectives: - Apply ML models to reaction prediction and retrosynthetic planning - Implement protein-ligand docking workflows with ML enhancement - Integrate molecular dynamics simulations with machine learning - Accelerate quantum chemistry calculations using ML surrogates - Design end-to-end drug discovery pipelines - Deploy ML models for production use - Explore cutting-edge developments in molecular AI","title":"Overview"},{"location":"day5/#1__reaction__prediction__and__retrosynthesis","text":"","title":"1. Reaction Prediction and Retrosynthesis"},{"location":"day5/#11__forward__reaction__prediction","text":"Forward reaction prediction involves predicting the products of a chemical reaction given reactants and conditions. Key Concepts: - Reaction SMILES : Encoding reactions as reactants>>products - Template-based methods : Using reaction templates extracted from databases - Template-free methods : Sequence-to-sequence models treating reactions as translation tasks - Graph-based methods : Operating directly on molecular graphs Popular Models: - Molecular Transformer : Attention-based sequence models for reaction prediction - Graph2SMILES : Graph neural networks with SMILES generation - LocalRetro : Template-free retrosynthesis using local reaction patterns Implementation Example: from rxnfp.transformer_fingerprints import RXNBERTFingerprintGenerator from rdkit import Chem # Initialize reaction fingerprint generator rxnfp_generator = RXNBERTFingerprintGenerator () # Define a reaction reaction_smiles = \"CC(=O)O.CCO>>CC(=O)OCC.O\" # Generate reaction fingerprint fingerprint = rxnfp_generator . convert ( reaction_smiles ) # Use for similarity search or downstream prediction","title":"1.1 Forward Reaction Prediction"},{"location":"day5/#12__retrosynthetic__planning","text":"Retrosynthesis involves working backwards from a target molecule to identify synthetic routes using available starting materials. Approaches: - Single-step retrosynthesis : Predicting immediate precursors - Multi-step planning : Building full synthetic trees - Search algorithms : Monte Carlo Tree Search (MCTS), best-first search - Cost optimization : Balancing route complexity, yield, and availability Tools and Frameworks: - AiZynthFinder : Open-source retrosynthesis planning with MCTS - IBM RXN for Chemistry : Cloud-based reaction prediction platform - Molecule.one : Commercial retrosynthesis software Practical Considerations: - Starting material availability - Reaction condition feasibility - Stereochemistry preservation - Scalability to industrial synthesis","title":"1.2 Retrosynthetic Planning"},{"location":"day5/#2__protein-ligand__docking","text":"","title":"2. Protein-Ligand Docking"},{"location":"day5/#21__traditional__docking__methods","text":"Protein-ligand docking predicts the binding pose and affinity of small molecules to protein targets. Classical Approaches: - AutoDock Vina : Scoring function-based docking - Glide : Precision docking with MM-GBSA rescoring - GOLD : Genetic algorithm-based pose prediction Limitations: - Computational expense for large libraries - Rigid protein approximation - Scoring function accuracy","title":"2.1 Traditional Docking Methods"},{"location":"day5/#22__ml-enhanced__docking","text":"Machine learning improves docking through better scoring functions, pose prediction, and virtual screening. ML Applications: - Scoring function refinement : Neural networks trained on binding affinity data - Pose ranking : Graph neural networks for pose selection - Virtual screening : Rapid filtering before expensive calculations - Protein flexibility : ML-based conformer generation Modern Tools: - DeepDock : Deep learning for binding pose prediction - Gnina : CNN-based scoring for AutoDock Vina - EquiBind : SE(3)-equivariant network for direct pose prediction - DiffDock : Diffusion models for blind docking Implementation Workflow: from gnina import Gnina from rdkit import Chem # Initialize Gnina docker = Gnina () # Prepare inputs protein_file = \"protein.pdb\" ligand_file = \"ligand.sdf\" # Run docking with CNN scoring results = docker . dock ( protein = protein_file , ligand = ligand_file , center = [ x , y , z ], size = [ 20 , 20 , 20 ], num_modes = 10 ) # Extract top poses and scores for pose , score in results : print ( f \"Score: { score : .2f } \" )","title":"2.2 ML-Enhanced Docking"},{"location":"day5/#23__binding__affinity__prediction","text":"Beyond docking, ML models predict binding affinity (pKd, pIC50) directly from structure. Approaches: - Structure-based : Using 3D protein-ligand complexes - Ligand-based : QSAR models using molecular descriptors - Hybrid : Combining structural and chemical features Popular Models: - DeepDTA : Deep learning for drug-target affinity - GraphDTA : Graph neural networks for affinity prediction - KDEEP : Kernelized deep learning approach","title":"2.3 Binding Affinity Prediction"},{"location":"day5/#3__molecular__dynamics__with__ml","text":"","title":"3. Molecular Dynamics with ML"},{"location":"day5/#31__ml__force__fields","text":"Machine learning force fields (MLFFs) provide quantum mechanical accuracy at classical MD speed. Key Models: - ANI : Accurate neural network potentials for organic molecules - SchNet : Continuous-filter convolutional networks - PaiNN : Polarizable atom interaction neural networks - MACE : Multi-atomic cluster expansion with higher-order interactions Advantages: - 100-1000x faster than ab initio MD - Quantum accuracy for energies and forces - Transferable across chemical space Implementation Example: import torch from torchani import ANI2x # Load ANI-2x model model = ANI2x () # Prepare molecular coordinates species = torch . tensor ([[ 1 , 6 , 6 , 1 , 1 , 1 , 1 ]]) # H-C-C-H... coordinates = torch . tensor ([[[ ... ], [ ... ], ... ]]) # Angstroms # Compute energy and forces energy = model (( species , coordinates )) . energies forces = - torch . autograd . grad ( energy . sum (), coordinates )[ 0 ] print ( f \"Energy: { energy . item () : .4f } Hartree\" )","title":"3.1 ML Force Fields"},{"location":"day5/#32__enhanced__sampling__with__ml","text":"ML accelerates exploration of conformational space in MD simulations. Techniques: - Collective variable identification : Autoencoders for reaction coordinates - Biasing potentials : Neural network-based metadynamics - Trajectory reweighting : Correcting for ML force field errors - Rare event sampling : Reinforcement learning for transition path sampling Applications: - Protein folding pathways - Ligand binding/unbinding kinetics - Free energy calculations - Conformational transitions","title":"3.2 Enhanced Sampling with ML"},{"location":"day5/#33__coarse-grained__modeling","text":"ML enables learned coarse-grained (CG) representations for larger spatiotemporal scales. Approaches: - Bottom-up : Deriving CG models from atomistic simulations - Top-down : Learning directly from experimental data - Backmapping : Reconstructing atomistic details from CG","title":"3.3 Coarse-Grained Modeling"},{"location":"day5/#4__quantum__chemistry__acceleration","text":"","title":"4. Quantum Chemistry Acceleration"},{"location":"day5/#41__ml__as__qm__surrogates","text":"Machine learning models replace expensive quantum chemistry calculations. Applications: - Energy prediction : DFT energy at GNN speed - Property prediction : HOMO-LUMO gaps, dipole moments, polarizabilities - Wavefunction approximation : Neural network wavefunctions - Density functional approximation : Learning exchange-correlation functionals Popular Datasets: - QM9 : 134k small organic molecules with DFT properties - OE62 : Organic reactions with transition state energies - MD17 : Molecular dynamics trajectories for benzene, aspirin, etc. - GEOM : 37M conformers with GFN2-xTB energies","title":"4.1 ML as QM Surrogates"},{"location":"day5/#42__delta__learning","text":"Delta learning combines ML with lower-level quantum methods to achieve higher accuracy. Concept: E_high \u2248 E_low + \u0394E_ML Where: - E_high : Target high-level method (e.g., CCSD(T)) - E_low : Fast low-level method (e.g., DFT) - \u0394E_ML : ML-predicted correction Benefits: - Reduces data requirements - Improves extrapolation - Physically motivated architecture","title":"4.2 Delta Learning"},{"location":"day5/#43__active__learning__for__qm","text":"Active learning selects the most informative molecules for expensive QM calculations. Workflow: 1. Train initial ML model on small dataset 2. Use uncertainty quantification to identify uncertain predictions 3. Run QM calculations on uncertain molecules 4. Retrain model with augmented data 5. Repeat until convergence Uncertainty Methods: - Ensemble disagreement - Bayesian neural networks - MC dropout - Gaussian process regression","title":"4.3 Active Learning for QM"},{"location":"day5/#5__end-to-end__drug__discovery__workflows","text":"","title":"5. End-to-End Drug Discovery Workflows"},{"location":"day5/#51__pipeline__architecture","text":"Modern ML-driven drug discovery integrates multiple components into cohesive workflows. Typical Pipeline Stages: Target Identification \u2192 Hit Discovery \u2192 Lead Optimization \u2192 Preclinical \u2192 Clinical \u2193 \u2193 \u2193 \u2193 \u2193 ML: Target ML: Virtual ML: Property ML: ADMET ML: Clinical prediction screening optimization prediction trial design","title":"5.1 Pipeline Architecture"},{"location":"day5/#52__multi-objective__optimization","text":"Drug candidates must satisfy multiple constraints simultaneously. Objectives: - Potency : High binding affinity to target - Selectivity : Low off-target binding - ADMET : Good absorption, distribution, metabolism, excretion, toxicity - Synthesizability : Feasible synthetic routes - Physicochemical properties : Drug-likeness (Lipinski\u2019s rules) Optimization Approaches: - Pareto optimization : Finding non-dominated solutions - Weighted scalarization : Combining objectives into single score - Constraint satisfaction : Hard constraints + objective optimization - Multi-task learning : Joint prediction of all properties Tools: - GuacaMol : Benchmarking for molecular design - MOSES : Molecular sets for generative models - Therapeutics Data Commons (TDC) : Unified ML tasks for drug discovery","title":"5.2 Multi-Objective Optimization"},{"location":"day5/#53__integration__with__laboratory__automation","text":"Closing the loop between computational prediction and experimental validation. Design-Make-Test-Analyze (DMTA) Cycles: - Design : ML generates candidate molecules - Make : Automated synthesis or compound ordering - Test : High-throughput screening (HTS) - Analyze : ML learns from results, iterates design Infrastructure: - Robotic synthesis platforms - Automated assay systems - LIMS (Laboratory Information Management Systems) - Real-time data feedback","title":"5.3 Integration with Laboratory Automation"},{"location":"day5/#6__model__deployment__and__production","text":"","title":"6. Model Deployment and Production"},{"location":"day5/#61__deployment__strategies","text":"Moving ML models from research to production environments. Deployment Options: - REST APIs : Flask, FastAPI for web services - Containerization : Docker for reproducible environments - Cloud platforms : AWS SageMaker, Google Cloud AI Platform - Edge deployment : ONNX for mobile/embedded devices Example FastAPI Deployment: from fastapi import FastAPI from pydantic import BaseModel import torch app = FastAPI () # Load pre-trained model model = torch . load ( \"model.pt\" ) model . eval () class MoleculeInput ( BaseModel ): smiles : str @app . post ( \"/predict\" ) def predict_property ( mol : MoleculeInput ): # Convert SMILES to features features = smiles_to_features ( mol . smiles ) # Make prediction with torch . no_grad (): prediction = model ( features ) return { \"prediction\" : prediction . item ()}","title":"6.1 Deployment Strategies"},{"location":"day5/#62__model__monitoring__and__maintenance","text":"Ensuring model performance in production. Key Considerations: - Data drift : Detecting distribution shifts in input data - Concept drift : Changes in input-output relationships - Performance monitoring : Tracking prediction accuracy over time - Retraining triggers : Automated model updates - A/B testing : Comparing model versions Monitoring Tools: - Evidently AI - Fiddler - Arize AI - Weights & Biases","title":"6.2 Model Monitoring and Maintenance"},{"location":"day5/#63__reproducibility__and__version__control","text":"Best practices for ML in production. Essential Components: - Code version control : Git, GitHub - Data versioning : DVC, LakeFS - Model versioning : MLflow, Weights & Biases - Environment management : Conda, Docker - Experiment tracking : MLflow, Neptune.ai MLflow Example: import mlflow from mlflow.models import infer_signature # Start MLflow run with mlflow . start_run (): # Train model model = train_model ( train_data ) # Log parameters mlflow . log_params ({ \"learning_rate\" : 0.001 , \"epochs\" : 100 }) # Log metrics mlflow . log_metrics ({ \"mae\" : 0.5 , \"r2\" : 0.85 }) # Log model signature = infer_signature ( X_test , predictions ) mlflow . sklearn . log_model ( model , \"model\" , signature = signature )","title":"6.3 Reproducibility and Version Control"},{"location":"day5/#64__regulatory__considerations","text":"ML models for drug discovery face regulatory scrutiny. FDA Guidance: - Model transparency and interpretability - Validation on diverse datasets - Handling of edge cases and uncertainty - Audit trails and documentation - Bias detection and mitigation Good Machine Learning Practice (GMLP): - Data quality assurance - Model validation and testing - Risk management - Continuous monitoring - Stakeholder engagement","title":"6.4 Regulatory Considerations"},{"location":"day5/#7__future__directions","text":"","title":"7. Future Directions"},{"location":"day5/#71__foundation__models__for__molecules","text":"Large-scale pre-trained models that can be fine-tuned for various downstream tasks. Current Developments: - MolBERT : BERT-like pre-training on SMILES - ChemBERTa : RoBERTa for chemical language - MolFormer : Transformer with 1B parameters trained on 1.1B molecules - UniMol : 3D molecular pre-training with geometric information - Galactica : General-purpose scientific language model including chemistry Advantages: - Transfer learning across tasks - Few-shot learning capabilities - Emergent understanding of chemical principles - Democratization of ML in chemistry Challenges: - Computational cost of training - Data curation at scale - Evaluation benchmarks - Interpretability of large models","title":"7.1 Foundation Models for Molecules"},{"location":"day5/#72__autonomous__laboratories","text":"Self-driving labs combine ML, robotics, and automation for autonomous experimentation. Components: - Robotic synthesis : Automated chemical synthesis platforms - High-throughput characterization : Rapid property measurement - ML planning : Bayesian optimization, active learning - Closed-loop control : Real-time experiment adaptation Examples: - Chemspeed : Automated synthesis and screening - Emerald Cloud Lab : Remote-access automated lab - IBM RoboRXN : Autonomous synthesis planning and execution - Material acceleration platforms : Rapid materials discovery Impact: - 10-100x acceleration of discovery - 24/7 operation - Exploration of unconventional chemistry - Reduced human bias","title":"7.2 Autonomous Laboratories"},{"location":"day5/#73__quantum__machine__learning","text":"Intersection of quantum computing and ML for molecular science. Quantum Advantages: - Quantum simulation : Efficient simulation of quantum systems - Variational algorithms : VQE for ground state energies - Quantum kernels : Enhanced feature spaces for ML - Quantum neural networks : Parameterized quantum circuits Near-Term Applications: - Hybrid classical-quantum algorithms : Quantum hardware accelerates specific steps - Quantum-enhanced sampling : Improved exploration of molecular space - Error-mitigated chemistry : Quantum chemistry despite hardware noise Frameworks: - Qiskit : IBM\u2019s quantum computing SDK - Cirq : Google\u2019s quantum programming framework - PennyLane : Quantum ML library with autodifferentiation - TensorFlow Quantum : Integration of quantum computing with TensorFlow Example - Variational Quantum Eigensolver: from qiskit.algorithms import VQE from qiskit.primitives import Estimator from qiskit.circuit.library import RealAmplitudes from qiskit_nature.second_q.drivers import PySCFDriver # Define molecular system driver = PySCFDriver ( atom = \"H 0 0 0; H 0 0 0.735\" ) problem = driver . run () # Set up VQE ansatz = RealAmplitudes ( num_qubits = 4 , reps = 2 ) vqe = VQE ( Estimator (), ansatz , optimizer = SLSQP ()) # Compute ground state energy result = vqe . compute_minimum_eigenvalue ( problem . hamiltonian ) print ( f \"Ground state energy: { result . eigenvalue : .6f } Hartree\" )","title":"7.3 Quantum Machine Learning"},{"location":"day5/#74__multimodal__learning","text":"Integrating diverse data types for comprehensive molecular understanding. Data Modalities: - Chemical structure : SMILES, graphs, 3D conformers - Spectroscopy : NMR, IR, MS, UV-Vis - Images : Microscopy, crystallography - Text : Literature, patents, experimental protocols - Bioactivity : Screening data, clinical outcomes Approaches: - Cross-modal pre-training : Learning shared representations - Late fusion : Combining predictions from separate models - Attention mechanisms : Learning to weight different modalities - Contrastive learning : Aligning representations across modalities Applications: - Structure elucidation from spectra - Literature-guided molecule generation - Predicting experimental conditions from desired outcomes - Hypothesis generation from multimodal data","title":"7.4 Multimodal Learning"},{"location":"day5/#75__explainable__ai__for__chemistry","text":"Making ML predictions interpretable for chemists. Techniques: - Attention visualization : Highlighting important molecular substructures - SHAP values : Quantifying feature importance - Counterfactual explanations : \u201cWhat if\u201d molecular modifications - Causal inference : Distinguishing correlation from causation - Mechanistic modeling : Hybrid physics-ML models Benefits: - Building trust with domain experts - Accelerating hypothesis generation - Regulatory compliance - Identifying model failures - Scientific discovery beyond prediction","title":"7.5 Explainable AI for Chemistry"},{"location":"day5/#practical__exercises","text":"","title":"Practical Exercises"},{"location":"day5/#exercise__1__retrosynthesis__planning","text":"Use AiZynthFinder to plan a synthesis route for ibuprofen starting from commercially available materials.","title":"Exercise 1: Retrosynthesis Planning"},{"location":"day5/#exercise__2__ml-enhanced__docking","text":"Compare traditional AutoDock Vina with Gnina on a set of protein-ligand complexes and evaluate scoring improvements.","title":"Exercise 2: ML-Enhanced Docking"},{"location":"day5/#exercise__3__md__with__ml__force__fields","text":"Run a short MD simulation of alanine dipeptide using both classical AMBER and ANI-2x force fields. Compare trajectories and computational cost.","title":"Exercise 3: MD with ML Force Fields"},{"location":"day5/#exercise__4__drug__discovery__pipeline","text":"Build an end-to-end pipeline that: (1) generates molecules with desired properties, (2) predicts ADMET, (3) performs virtual screening, and (4) suggests top candidates.","title":"Exercise 4: Drug Discovery Pipeline"},{"location":"day5/#exercise__5__model__deployment","text":"Deploy a solubility prediction model as a REST API using FastAPI and test it with various SMILES inputs.","title":"Exercise 5: Model Deployment"},{"location":"day5/#additional__resources","text":"","title":"Additional Resources"},{"location":"day5/#research__papers","text":"\u201cMachine learning for molecular and materials science\u201d - Nature (2018) \u201cRetrosynthesis prediction with conditional graph logic network\u201d - NeurIPS (2019) \u201cEquiBind: Geometric deep learning for drug binding structure prediction\u201d - ICML (2022) \u201cMachine learning force fields\u201d - Chemical Reviews (2021) \u201cThe rise of diffusion models in drug discovery\u201d - Nature Communications (2024)","title":"Research Papers"},{"location":"day5/#software__tools","text":"RDKit : Cheminformatics toolkit DeepChem : Deep learning for chemistry OpenMM : Molecular dynamics simulation TorchDrug : PyTorch library for drug discovery Therapeutics Data Commons : ML benchmarks for drug discovery","title":"Software Tools"},{"location":"day5/#databases","text":"ChEMBL : Bioactive molecules with drug-like properties PubChem : Chemical information database PDB : Protein Data Bank ZINC : Commercial compounds for virtual screening BindingDB : Binding affinities for protein-ligand complexes","title":"Databases"},{"location":"day5/#online__courses__tutorials","text":"DeepChem tutorials : Practical ML for drug discovery RDKit tutorials : Molecular manipulation and analysis Papers with Code : Implementations of recent papers MIT Deep Learning for Molecules : Course materials","title":"Online Courses &amp; Tutorials"},{"location":"day5/#summary","text":"Day 5 demonstrated how machine learning transforms molecular science from theoretical concepts to practical applications. Key takeaways include: Reaction prediction and retrosynthesis enable automated synthesis planning ML-enhanced docking improves virtual screening efficiency and accuracy ML force fields bring quantum accuracy to molecular dynamics at reduced cost Quantum chemistry acceleration makes high-level calculations accessible End-to-end workflows integrate ML throughout drug discovery pipelines Production deployment requires careful attention to monitoring, versioning, and reproducibility Future directions including foundation models, autonomous labs, and quantum ML promise revolutionary advances The integration of these techniques creates powerful tools for accelerating scientific discovery while maintaining rigor and interpretability.","title":"Summary"},{"location":"day5/#next__steps","text":"Explore hands-on implementations using the provided code examples Join online communities (RDKit discussions, DeepChem forums) Stay current with literature (preprints on arXiv, ChemRxiv) Contribute to open-source molecular ML projects Apply these techniques to your own research problems","title":"Next Steps"}]}